# Zero-Redundancy LLM Arbitration System Using Probabilistic Early Rejection and Reward-Guided Selection
Summary Statement Example for Your Patent or Paper
â€œOur Zero-Redundancy LLM Arbitration System is a novel, highly efficient approach to selecting the best Large Language Model for classification tasks. By combining early model dropout based on easy examples with a reward-driven multi-armed bandit for dynamic input assignment, our system optimally balances accuracy and computational cost. The inclusion of an optional task-specific routing mechanism further enhances assignment precision. This architecture uniquely minimizes redundant evaluations, adapts in real time, and scales gracefully, establishing a new state-of-the-art for efficient LLM model selection.â€


## Abstract

We introduce a novel arbitration system that efficiently selects the best-performing Large Language Models (LLMs) for classification, minimizing redundancy by:

- Using a fast early-rejection step on â€œeasyâ€ examples
- Employing a reward-driven multi-armed bandit algorithm to assign future examples to strong-performing models
- Optionally routing examples to specialized models using learned input features

This design significantly reduces computation from \( \mathcal{O}(N \times D) \) to approximately \( \mathcal{O}(N) + \mathcal{O}(D) \), making it viable for deployment in real-time, compute-constrained environments.

---

## 1. Background

Running all \( N \) LLMs on \( D \) examples incurs a cost of:

\[
\mathcal{O}(N \times D)
\]

Ensemble methods provide robustness but are computationally expensive and often redundant. Our approach reduces redundancy by early-dropping weak models and assigning examples intelligently using adaptive reward mechanisms.

---

## 2. Architecture Overview

The system proceeds in three distinct stages:

### Stage 1: Probabilistic Early Rejection (PER)

Let:

- \( \mathcal{E} = \{x_1, x_2, \ldots, x_k\} \) be a set of easy examples  
- \( M_i \) be the \( i \)-th model out of \( N \)  
- \( y_j \) be the true label for input \( x_j \)

We compute the accuracy of model \( M_i \) on the easy set:

\[
A_i = \frac{1}{k} \sum_{j=1}^{k} \mathbf{1}[M_i(x_j) = y_j]
\]

If:

\[
A_i < \tau
\]

where \( \tau \in (0, 1) \), model \( M_i \) is dropped from future consideration.

This filters out models that fail to classify trivial or low-entropy examples correctly.

---

### Stage 2: Reward-Guided Bandit Allocation (RGBA)

Remaining models (say \( M' \ll N \)) are modeled as arms in a contextual multi-armed bandit. At each time step \( t \), an input \( x_t \) is assigned to a model \( M_i \) based on a policy \( \pi(t) \).

Each model receives a reward defined as:

\[
r_i(t) = \alpha \cdot \mathbf{1}[M_i(x_t) = y_t] + \beta \cdot \mathrm{conf}(M_i(x_t))
\]

Where:

- \( \alpha, \beta \) are hyperparameters  
- \( \mathrm{conf}(M_i(x_t)) \in [0, 1] \) is model confidence  

The cumulative reward is:

\[
R_T = \sum_{t=1}^{T} r_{\pi(t)}(t)
\]

We update the value estimate of each model using:

\[
Q_i(t+1) = Q_i(t) + \eta \cdot \left(r_i(t) - Q_i(t)\right)
\]

Where:

- \( Q_i \) is the estimated value of model \( M_i \)  
- \( \eta \) is the learning rate  

---

### Stage 3 (Optional): Task-Specific Router (TSR)

Train a lightweight classifier \( R(x) \) using features \( \phi(x) \) (e.g., sentence embeddings) to route inputs to the most suitable model:

\[
R(x) = \arg\max_i \; P(M_i \mid \phi(x))
\]

Router models may include logistic regression, SVMs, or shallow neural networks.

---

## 3. Component Overview

| Component         | Description                                                                                   |
|-------------------|-----------------------------------------------------------------------------------------------|
| EasySetSelector   | Selects low-entropy examples \( H(p(y|x)) < H_0 \)                                            |
| FailureDetector   | Removes models \( M_i \) with \( A_i < \tau \)                                               |
| BanditAllocator   | Selects models via reward-maximizing bandit policy                                           |
| RewardEngine      | Computes: \( r_i(t) = \alpha \cdot \text{accuracy} + \beta \cdot \text{confidence} \)       |
| ModelRouter (TSR) | Learns to route inputs via \( R(x) = \arg\max_i P(M_i \mid \phi(x)) \)                        |

---

## 4. Complexity Comparison

| Method           | Complexity                   |
|------------------|------------------------------|
| Traditional      | \( \mathcal{O}(N \times D) \) |
| This Method      | \( \mathcal{O}(k \times N) + \mathcal{O}(M' \times D') \), where \( k \ll D \), \( M' \ll N \) |

Net effective cost:  

\[
\mathcal{O}(N) + \mathcal{O}(D)
\]

---

## 5. Novelty & Inventive Step

- **Probabilistic Early Rejection:** dynamically filters weak models early based on easy samples.  
- **Bandit-Driven Selection:** incorporates model confidence and reward signals for efficient selection.  
- **Task-Specific Routing:** optional downstream router personalizes model assignment.

No prior art combines:

- Early rejection on easy samples,  
- Bandit-based selection using model confidence,  
- Task-aware dynamic routing.

---


Uniqueness and Novelty
1. Probabilistic Early Rejection Based on â€œEasyâ€ Examples
Unlike existing ensemble or multi-model selection techniques that treat all data points equally, this system introduces a data-driven early rejection step using a carefully curated subset of "easy" examples.

This ensures models that fail on trivial or low-entropy cases are immediately discarded, saving computational resources upfront.

The dynamic threshold 
ğœ
Ï„ is adaptive and learned, enabling flexible model pruning customized per deployment environment.

This pre-filtering concept based on example difficulty combined with model performance is a novel use in LLM arbitration.

2. Reward-Guided Multi-Armed Bandit Model Selection
While multi-armed bandits have been widely used in model selection and online learning, our system integrates:

Model confidence scores into the reward signal, not just accuracy, reflecting the certainty of predictions, which improves stability and decision granularity.

An adaptive learning rate mechanism that balances exploration and exploitation in selecting models per incoming example stream.

This fine-grained reward design with confidence-augmented feedback for model arbitration is unique in the LLM domain.

3. Dynamic Model Dropout Driven by Easy Example Performance
The novel integration of early rejection with bandit-based allocation enables dynamic dropout of underperforming models after the initial easy example evaluation, which contrasts with static ensembling or manual selection in prior art.

The approach reduces the model pool without multiple passes over the entire dataset.

This efficiency gain is crucial for large-scale LLM systems, where each model evaluation is costly.

4. Optional Task-Specific Router for Input-Conditional Model Assignment
The introduction of a lightweight input router trained on learned input features to map examples to the most competent model introduces:

Fine-grained, input-aware specialization beyond simple global model ranking.

A modular architecture allowing plug-and-play of any classification routing mechanism, including logistic regression, SVM, or shallow neural nets.

This hybrid combination of bandit-based arbitration with routing adds flexibility and further efficiency.

5. Computational Efficiency and Scalability
The system achieves a near-linear computational cost in dataset size and model count compared to quadratic cost in naÃ¯ve multi-model evaluation.

This significant reduction in inference cost while maintaining accuracy creates practical viability for real-time, large-scale deployments.

6. Patent-Worthy Combinatorial Approach
While individual components like multi-armed bandits and confidence scoring exist in the literature, the specific combination and sequencing of early rejection, confidence-augmented reward, and input-aware routing applied to LLM arbitration is novel and non-obvious.

The architecture also incorporates dynamic model dropout based on easy example performance in a single/few-pass approach, which is not previously known.



Rethought Workflow for Multi-LLM Model Selection Using Multi-Armed Bandit with Early Dropout
Input
ğ‘
N candidate LLMs: 
ğ‘€
1
,
ğ‘€
2
,
.
.
.
,
ğ‘€
ğ‘
M 
1
â€‹
 ,M 
2
â€‹
 ,...,M 
N
â€‹
 

Dataset with 
ğ‘€
M rows/examples: 
ğ·
=
{
(
ğ‘¥
ğ‘–
,
ğ‘¦
ğ‘–
)
}
ğ‘–
=
1
ğ‘€
D={(x 
i
â€‹
 ,y 
i
â€‹
 )} 
i=1
M
â€‹
 

Step 1: Initial Dataset Split and Early Performance Monitoring
Split dataset 
ğ·
D into two parts:

Phase 1: First 20% of data, 
ğ·
ğ‘–
ğ‘›
ğ‘–
ğ‘¡
D 
init
â€‹
  (e.g., first 
0.2
Ã—
ğ‘€
0.2Ã—M rows)

Phase 2: Remaining 80%, 
ğ·
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘¡
D 
rest
â€‹
 

Step 2: Parallel Model Evaluation on 
ğ·
ğ‘–
ğ‘›
ğ‘–
ğ‘¡
D 
init
â€‹
 
Run all 
ğ‘
N models on 
ğ·
ğ‘–
ğ‘›
ğ‘–
ğ‘¡
D 
init
â€‹
  in parallel (or batched to minimize runtime).

Track model performance accuracy 
ğ´
ğ‘–
A 
i
â€‹
  over 
ğ·
ğ‘–
ğ‘›
ğ‘–
ğ‘¡
D 
init
â€‹
  incrementally, i.e., update accuracy after each example or small batch.

Step 3: Early Dropout with Patience
Define a patience parameter 
ğ‘
=
3
p=3 (number of consecutive failures allowed).

For each model 
ğ‘€
ğ‘–
M 
i
â€‹
 , monitor performance in sliding windows or per example:

If 
ğ‘€
ğ‘–
M 
i
â€‹
  fails to predict correctly on 
ğ‘
p consecutive examples while at least one other model performs correctly on those examples, mark 
ğ‘€
ğ‘–
M 
i
â€‹
  as a candidate for dropout.

Drop 
ğ‘€
ğ‘–
M 
i
â€‹
  only if this failure pattern persists (no recovery) within the first 20% data.

This eliminates clearly underperforming models early, relying on relative performance, not absolute thresholds.

Step 4: Narrowed Down Model Set
After Phase 1, retain models that survived dropout.

If more than 5 models remain, keep top 5 based on accuracy or aggregate performance metrics on 
ğ·
ğ‘–
ğ‘›
ğ‘–
ğ‘¡
D 
init
â€‹
 .

Step 5: Multi-Armed Bandit on Remaining Data 
ğ·
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘¡
D 
rest
â€‹
 
Initialize bandit values 
ğ‘„
ğ‘–
Q 
i
â€‹
  for each retained model 
ğ‘€
ğ‘–
M 
i
â€‹
 .

For each incoming example 
ğ‘¥
ğ‘¡
x 
t
â€‹
  in 
ğ·
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘¡
D 
rest
â€‹
 :

Select a model 
ğ‘€
ğ‘–
M 
i
â€‹
  using the multi-armed bandit policy (e.g., UCB, Thompson Sampling) balancing exploration-exploitation.

Model 
ğ‘€
ğ‘–
M 
i
â€‹
  predicts 
ğ‘¦
^
ğ‘¡
y
^
â€‹
  
t
â€‹
 .

Reward 
ğ‘Ÿ
ğ‘–
=
1
r 
i
â€‹
 =1 if 
ğ‘¦
^
ğ‘¡
=
ğ‘¦
ğ‘¡
y
^
â€‹
  
t
â€‹
 =y 
t
â€‹
 , else 0.

Update 
ğ‘„
ğ‘–
Q 
i
â€‹
  based on reward to improve model selection policy.

Step 6: Optional Router Training (Post Bandit Phase)
Use data from bandit assignments and outcomes to train a router 
ğ‘…
R that predicts best model per input features.

This router can accelerate or replace bandit decisions on new/unseen data.

Step 7: Output
Final best subset of models selected by performance and bandit optimization.

Model value estimates 
ğ‘„
ğ‘–
Q 
i
â€‹
  reflecting per-model utility.

Optional trained router 
ğ‘…
R for input-driven model assignment.

Summary:
Early phase dropout uses relative performance with patience on 20% data.

Patience ensures transient failures donâ€™t unfairly drop models, but persistent lagging models are pruned.

Final selection uses multi-armed bandit on reduced model pool and remaining data, dynamically optimizing model assignment.

--------
Algorithm: Multi-LLM Selection with Early Dropout and Bandit Optimization
Input:

Models 
ğ‘€
=
{
ğ‘€
1
,
ğ‘€
2
,
.
.
.
,
ğ‘€
ğ‘
}
M={M 
1
â€‹
 ,M 
2
â€‹
 ,...,M 
N
â€‹
 }

Dataset 
ğ·
=
{
(
ğ‘¥
ğ‘–
,
ğ‘¦
ğ‘–
)
}
ğ‘–
=
1
ğ‘€
D={(x 
i
â€‹
 ,y 
i
â€‹
 )} 
i=1
M
â€‹
 

Patience parameter 
ğ‘
p (e.g., 3)

Max retained models 
ğ‘˜
k (e.g., 5)

Output:

Selected subset of models 
ğ‘€
ğ‘“
ğ‘–
ğ‘›
ğ‘
ğ‘™
M 
final
â€‹
 

Model quality estimates 
ğ‘„
Q

Optional router 
ğ‘…
R

Procedure:

Split dataset:

ğ·
ğ‘–
ğ‘›
ğ‘–
ğ‘¡
â†
D 
init
â€‹
 â† first 20% of 
ğ·
D

ğ·
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘¡
â†
D 
rest
â€‹
 â† remaining 80% of 
ğ·
D

Initialize:

For each model 
ğ‘€
ğ‘–
M 
i
â€‹
 , set:

ğ‘
ğ‘œ
ğ‘›
ğ‘ 
ğ‘’
ğ‘
ğ‘¢
ğ‘¡
ğ‘–
ğ‘£
ğ‘’
_
ğ‘“
ğ‘
ğ‘–
ğ‘™
ğ‘¢
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘–
â†
0
consecutive_failures 
i
â€‹
 â†0

ğ‘‘
ğ‘Ÿ
ğ‘œ
ğ‘
ğ‘
ğ‘’
ğ‘‘
ğ‘–
â†
ğ¹
ğ‘
ğ‘™
ğ‘ 
ğ‘’
dropped 
i
â€‹
 â†False

ğ‘
ğ‘œ
ğ‘Ÿ
ğ‘Ÿ
ğ‘’
ğ‘
ğ‘¡
ğ‘–
â†
0
correct 
i
â€‹
 â†0

ğ‘¡
ğ‘œ
ğ‘¡
ğ‘
ğ‘™
ğ‘–
â†
0
total 
i
â€‹
 â†0

Early evaluation & dropout:
For each example 
(
ğ‘¥
ğ‘¡
,
ğ‘¦
ğ‘¡
)
âˆˆ
ğ·
ğ‘–
ğ‘›
ğ‘–
ğ‘¡
(x 
t
â€‹
 ,y 
t
â€‹
 )âˆˆD 
init
â€‹
 :
Â Â a. Collect set 
ğ¶
=
{
}
C={} of models predicting correctly on 
ğ‘¥
ğ‘¡
x 
t
â€‹
 .
Â Â b. For each 
ğ‘€
ğ‘–
M 
i
â€‹
  not dropped:
Â Â Â Â i. Predict 
ğ‘¦
^
ğ‘¡
=
ğ‘€
ğ‘–
(
ğ‘¥
ğ‘¡
)
y
^
â€‹
  
t
â€‹
 =M 
i
â€‹
 (x 
t
â€‹
 )
Â Â Â Â ii. Update 
ğ‘¡
ğ‘œ
ğ‘¡
ğ‘
ğ‘™
ğ‘–
â†
ğ‘¡
ğ‘œ
ğ‘¡
ğ‘
ğ‘™
ğ‘–
+
1
total 
i
â€‹
 â†total 
i
â€‹
 +1
Â Â Â Â iii. If 
ğ‘¦
^
ğ‘¡
=
ğ‘¦
ğ‘¡
y
^
â€‹
  
t
â€‹
 =y 
t
â€‹
 , then
Â Â Â Â Â Â - 
ğ‘
ğ‘œ
ğ‘Ÿ
ğ‘Ÿ
ğ‘’
ğ‘
ğ‘¡
ğ‘–
â†
ğ‘
ğ‘œ
ğ‘Ÿ
ğ‘Ÿ
ğ‘’
ğ‘
ğ‘¡
ğ‘–
+
1
correct 
i
â€‹
 â†correct 
i
â€‹
 +1
Â Â Â Â Â Â - 
ğ‘
ğ‘œ
ğ‘›
ğ‘ 
ğ‘’
ğ‘
ğ‘¢
ğ‘¡
ğ‘–
ğ‘£
ğ‘’
_
ğ‘“
ğ‘
ğ‘–
ğ‘™
ğ‘¢
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘–
â†
0
consecutive_failures 
i
â€‹
 â†0
Â Â Â Â Â Â - Add 
ğ‘€
ğ‘–
M 
i
â€‹
  to 
ğ¶
C
Â Â Â Â Else
Â Â Â Â Â Â - 
ğ‘
ğ‘œ
ğ‘›
ğ‘ 
ğ‘’
ğ‘
ğ‘¢
ğ‘¡
ğ‘–
ğ‘£
ğ‘’
_
ğ‘“
ğ‘
ğ‘–
ğ‘™
ğ‘¢
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘–
â†
ğ‘
ğ‘œ
ğ‘›
ğ‘ 
ğ‘’
ğ‘
ğ‘¢
ğ‘¡
ğ‘–
ğ‘£
ğ‘’
_
ğ‘“
ğ‘
ğ‘–
ğ‘™
ğ‘¢
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘–
+
1
consecutive_failures 
i
â€‹
 â†consecutive_failures 
i
â€‹
 +1
Â Â c. For each 
ğ‘€
ğ‘–
M 
i
â€‹
  not dropped:
Â Â Â Â i. If 
ğ‘
ğ‘œ
ğ‘›
ğ‘ 
ğ‘’
ğ‘
ğ‘¢
ğ‘¡
ğ‘–
ğ‘£
ğ‘’
_
ğ‘“
ğ‘
ğ‘–
ğ‘™
ğ‘¢
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘–
â‰¥
ğ‘
consecutive_failures 
i
â€‹
 â‰¥p AND 
ğ‘€
ğ‘–
âˆ‰
ğ¶
M 
i
â€‹
 âˆˆ
/
C AND 
âˆ£
ğ¶
âˆ£
>
0
âˆ£Câˆ£>0 then
Â Â Â Â Â Â Set 
ğ‘‘
ğ‘Ÿ
ğ‘œ
ğ‘
ğ‘
ğ‘’
ğ‘‘
ğ‘–
â†
ğ‘‡
ğ‘Ÿ
ğ‘¢
ğ‘’
dropped 
i
â€‹
 â†True

Select survivors:

Let 
ğ‘†
=
{
ğ‘€
ğ‘–
âˆ£
ğ‘‘
ğ‘Ÿ
ğ‘œ
ğ‘
ğ‘
ğ‘’
ğ‘‘
ğ‘–
=
ğ¹
ğ‘
ğ‘™
ğ‘ 
ğ‘’
}
S={M 
i
â€‹
 âˆ£dropped 
i
â€‹
 =False}

If 
âˆ£
ğ‘†
âˆ£
>
ğ‘˜
âˆ£Sâˆ£>k, keep top 
ğ‘˜
k models with highest 
ğ‘
ğ‘œ
ğ‘Ÿ
ğ‘Ÿ
ğ‘’
ğ‘
ğ‘¡
ğ‘–
ğ‘¡
ğ‘œ
ğ‘¡
ğ‘
ğ‘™
ğ‘–
total 
i
â€‹
 
correct 
i
â€‹
 
â€‹
 

Initialize bandit:

For each 
ğ‘€
ğ‘–
âˆˆ
ğ‘†
M 
i
â€‹
 âˆˆS, set 
ğ‘„
ğ‘–
â†
0
Q 
i
â€‹
 â†0, 
ğ‘
ğ‘–
â†
0
N 
i
â€‹
 â†0

Bandit-based selection on 
ğ·
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘¡
D 
rest
â€‹
 :
For each 
(
ğ‘¥
ğ‘¡
,
ğ‘¦
ğ‘¡
)
âˆˆ
ğ·
ğ‘Ÿ
ğ‘’
ğ‘ 
ğ‘¡
(x 
t
â€‹
 ,y 
t
â€‹
 )âˆˆD 
rest
â€‹
 :
Â Â a. Select 
ğ‘€
ğ‘—
âˆˆ
ğ‘†
M 
j
â€‹
 âˆˆS via bandit policy (e.g., UCB)
Â Â b. Predict 
ğ‘¦
^
ğ‘¡
=
ğ‘€
ğ‘—
(
ğ‘¥
ğ‘¡
)
y
^
â€‹
  
t
â€‹
 =M 
j
â€‹
 (x 
t
â€‹
 )
Â Â c. Compute reward 
ğ‘Ÿ
=
1
r=1 if 
ğ‘¦
^
ğ‘¡
=
ğ‘¦
ğ‘¡
y
^
â€‹
  
t
â€‹
 =y 
t
â€‹
  else 0
Â Â d. Update 
ğ‘
ğ‘—
â†
ğ‘
ğ‘—
+
1
N 
j
â€‹
 â†N 
j
â€‹
 +1
Â Â e. Update 
ğ‘„
ğ‘—
â†
ğ‘„
ğ‘—
+
1
ğ‘
ğ‘—
(
ğ‘Ÿ
âˆ’
ğ‘„
ğ‘—
)
Q 
j
â€‹
 â†Q 
j
â€‹
 + 
N 
j
â€‹
 
1
â€‹
 (râˆ’Q 
j
â€‹
 )

Optional router training:

Train router 
ğ‘…
R to map input 
ğ‘¥
x to best 
ğ‘€
ğ‘–
M 
i
â€‹
  using accumulated data

Return:

ğ‘€
ğ‘“
ğ‘–
ğ‘›
ğ‘
ğ‘™
=
ğ‘†
M 
final
â€‹
 =S

Quality estimates 
ğ‘„
Q

Optional 
ğ‘…
R
