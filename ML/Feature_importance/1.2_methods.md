Got it — here’s your text in **Markdown format** (not inside a code block), so you can copy-paste directly:

---

# 4  Methods Overview

This chapter provides an overview of interpretability approaches. The goal is to give you a map so that when you dive into the individual models and methods, you can see the forest for the trees. Figure 4.1 provides a taxonomy of the different approaches.

![Short taxonomy of interpretability methods which reflects the structure of the book.](“z1.jpg”)

---

## Interpretability by design

Interpretability by design means that we train inherently interpretable models, such as using logistic regression instead of a random forest.

Post-hoc interpretability means that we use an interpretability method after the model is trained. Post-hoc interpretation methods can be model-agnostic, such as permutation feature importance, or model-specific, such as analyzing the features learned by a neural network. Model-agnostic methods can be further divided into local methods which focus on explaining individual predictions, and global methods which focus on datasets. This book focuses on post-hoc model-agnostic methods but also covers basic models that are interpretable by design and model-specific methods for neural networks.

---

### Interpretable models by design

Interpretability by design is decided on the level of the machine learning algorithm. The simplest example is linear regression: When you use ordinary least squares to fit/train a linear regression model, you are using an algorithm that will produce find models that are linear in the input features. Models that are interpretable by design are also called intrinsically or inherently interpretable.

![Interpretability by design means using machine learning algorithms that produce “inherently interpretable” models.](images.z3.jpeg)

* **Linear regression**: Fit a linear model by minimizing the sum of squared errors.
* **Logistic regression**: Extend linear regression for classification using a nonlinear transformation.
* **Linear model extensions**: Add penalties, interactions, and nonlinear terms for more flexibility.
* **Decision trees**: Recursively split data to create tree-based models.
* **Decision rules**: Extract if-then rules from data.
* **RuleFit**: Combine tree-based rules with Lasso regression to learn sparse rule-based models.

Other interpretable-by-design approaches:

* Prototype-based neural networks for image classification, called **ProtoViT (Ma et al. 2024)**.
* **Yang et al. (2024)**: inherently interpretable tree ensembles (boosted trees with small depth, feature effect sorting, pruning).
* **Model-based boosting** (Bühlmann & Hothorn 2007): additive weighted weak learners.
* **Generalized additive models** with automatic interaction detection (Caruana et al. 2015).

Levels of interpretability:

* **Entirely interpretable**: e.g., small decision tree or simple linear regression.
* **Parts interpretable**: e.g., coefficients of a regression with many features.
* **Predictions interpretable**: approaches for interpreting single predictions.

Advantages:

* Easier debugging and improvements.
* Faithful explanations of predictions.
* Alignment with domain expertise.

Challenges:

* May perform worse than black-box models.
* **Rashomon effect**: multiple equally good but incompatible models → unclear which to interpret.

---

## Rashomon effect

The Japanese movie *Rashomon* (1950) tells four different versions of a murder story. Each version explains events equally well, but they are incompatible. This phenomenon was named the **Rashomon effect** in ML.

---

## Post-hoc interpretability

Post-hoc methods are applied after the model has been trained.

* **Model-agnostic**: analyze outputs only (e.g., permutation feature importance).
* **Model-specific**: analyze internals (e.g., neurons in NN, Gini importance in RF).

### Model-agnostic post-hoc methods

Operate via **SIPA principle**:

1. Sample data
2. Intervene (perturb feature/input)
3. Predict with model
4. Aggregate results

Example: **Permutation Feature Importance** (PFI).

Model-agnostic interpretation = another layer:
World → Data → Model → **Interpretability layer** (humans).

---

### Local model-agnostic post-hoc methods

Explain **individual predictions**.

* **Ceteris paribus plots** – vary 1 feature, see effect.
* **ICE curves** – effect across many individuals.
* **LIME** – surrogate linear model locally.
* **Anchors** – if–then scoped rules.
* **Counterfactuals** – minimal feature changes to flip prediction.
* **Shapley values** – fair attribution of prediction to features.
* **SHAP** – efficient Shapley + global extensions.

Usefulness:

* Debugging unusual cases.
* Justifying predictions (esp. counterfactuals & ceteris paribus).
* Attribution methods (LIME/SHAP) less reliable for high-stakes decisions.

---

### Global model-agnostic post-hoc methods

Explain **average model behavior**.

* **Partial Dependence Plots (PDP)**
* **Accumulated Local Effects (ALE)**
* **H-statistic** (feature interactions)
* **Functional decomposition**
* **Permutation Feature Importance (PFI)**
* **Leave-one-feature-out (LOFO)**
* **Surrogate models** (simplify black-box)
* **Prototypes & criticisms**

Two main categories:

* **Feature effects**: PDP, ALE, H-stat, decomposition.
* **Feature importance**: PFI, LOFO, SHAP importance.

Usefulness:

* Debugging + feature selection (LOFO).
* Stakeholder justification (which features matter globally).
* Data insights (mechanisms in PDP/ALE).
* Can be **group-wise** by restricting to subsets.

---

## Model-specific post-hoc methods

Applied after training but **only for specific ML models**.

Examples:

* **Random Forests**: Gini importance.
* **Logistic regression**: odds ratios.
* **Neural networks**: many methods below.

NN interpretation methods:

* **Learned Features** (hidden layers).
* **Saliency Maps** (pixel contributions).
* **Concepts** (higher-level learned abstractions).
* **Adversarial Examples** (robustness).
* **Influential Instances** (impactful training data).

Strength: improve + debug models.
Weakness: require theoretical link between interpretation & data.

---

## Blurred lines

* **Logistic regression**: coefficients → odds ratios = post-hoc.
* **Boosted stumps** ≈ interpretable GAMs.
* **Linear regression effects** → can be converted into Shapley values (model-agnostic).

---

Would you like me to also **add a visual taxonomy (table/diagram)** of these methods for quick revision, or keep it purely textual?
