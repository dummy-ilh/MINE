# Chapter 5: Matrix Approach to Simple Linear Regression Analysis

Matrix algebra is widely used in mathematical and statistical analysis and becomes a
practical necessity in **multiple regression analysis**, where large systems of equations and
large data sets must be represented compactly and manipulated efficiently. Although
matrix algebra is not strictly required for **simple linear regression**, applying matrix
methods in this setting provides a natural and important transition to multiple regression,
which is developed in later chapters. Readers already familiar with matrix algebra may
focus mainly on the regression-related sections.

---

## 5.1 Matrices

### Definition of a Matrix

A **matrix** is a rectangular array of elements arranged in **rows** and **columns**. For
example:

$$
\begin{bmatrix}
16000 & 23 \\
33000 & 35 \\
21000 & 35
\end{bmatrix}
$$

In this example:
- each **row** corresponds to a person,
- each **column** corresponds to a characteristic (income, age),
- the element in row 1, column 1 represents the income of the first person.

The **dimension** of a matrix is written as  
\[
\text{number of rows} \times \text{number of columns}
\]
so the above matrix has dimension $3 \times 2$.

---

### General Notation

A matrix can be written symbolically as:

$$
A =
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1c} \\
a_{21} & a_{22} & \cdots & a_{2c} \\
\vdots & \vdots &        & \vdots \\
a_{r1} & a_{r2} & \cdots & a_{rc}
\end{bmatrix}
\tag{5.1}
$$

or compactly as:

$$
A = [a_{ij}], \quad i = 1,\ldots,r; \; j = 1,\ldots,c
$$

The first subscript identifies the **row**, and the second identifies the **column**.

> A matrix should not be thought of as a number. Only a $1 \times 1$ matrix can be treated
> interchangeably as a single number.

---

### Square Matrices and Vectors

- A **square matrix** has the same number of rows and columns.
- A **column vector** is a matrix with one column.
- A **row vector** is a matrix with one row.

Examples:

$$
\text{Column vector: } 
\begin{bmatrix}
a \\
b \\
c
\end{bmatrix}
\quad (3 \times 1)
$$

$$
\text{Row vector: } 
\begin{bmatrix}
15 & 25 & 50
\end{bmatrix}
\quad (1 \times 3)
$$

For vectors, a single subscript is sufficient to identify elements.

---

### Transpose of a Matrix

The **transpose** of a matrix $A$, denoted by $A'$, is obtained by interchanging rows and
columns.

If:

$$
A =
\begin{bmatrix}
2 & 5 \\
7 & 10 \\
3 & 4
\end{bmatrix}
\quad (3 \times 2)
$$

then:

$$
A' =
\begin{bmatrix}
2 & 7 & 3 \\
5 & 10 & 4
\end{bmatrix}
\quad (2 \times 3)
$$

In general:

$$
A = [a_{ij}], \quad i = 1,\ldots,r; \; j = 1,\ldots,c
$$

$$
A' = [a_{ji}], \quad j = 1,\ldots,c; \; i = 1,\ldots,r
\tag{5.3}
$$

Thus, the element in the $i$th row and $j$th column of $A$ appears in the $j$th row and
$i$th column of $A'$.

---

### Equality of Matrices

Two matrices $A$ and $B$ are **equal** if:
1. they have the same dimensions, and
2. all corresponding elements are equal.

For example, if:

$$
A =
\begin{bmatrix}
a_1 \\
a_2 \\
a_3
\end{bmatrix},
\quad
B =
\begin{bmatrix}
4 \\
7 \\
3
\end{bmatrix}
$$

then $A = B$ implies:

$$
a_1 = 4, \quad a_2 = 7, \quad a_3 = 3
$$

---

## Matrices in Regression Analysis

### Response Vector

In regression analysis, the **response vector** $Y$ contains the $n$ observed values of the
response variable:

$$
Y =
\begin{bmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{bmatrix}
\quad (n \times 1)
\tag{5.4}
$$

Its transpose is the row vector:

$$
Y' = [\, Y_1 \; Y_2 \; \cdots \; Y_n \,]
\tag{5.5}
$$

---

### Design Matrix for Simple Linear Regression

For simple linear regression, the **design matrix** $X$ is defined as:

$$
X =
\begin{bmatrix}
1 & X_1 \\
1 & X_2 \\
\vdots & \vdots \\
1 & X_n
\end{bmatrix}
\quad (n \times 2)
\tag{5.6}
$$

The first column of ones corresponds to the intercept term, and the second column contains
the observed predictor values.

The transpose of $X$ is:

$$
X' =
\begin{bmatrix}
1 & 1 & \cdots & 1 \\
X_1 & X_2 & \cdots & X_n
\end{bmatrix}
\tag{5.7}
$$

The matrix $X$ is called the **design matrix** because it encodes the structure of the
regression model and the observed predictor values.


## 5.2 Matrix Addition and Subtraction

Two matrices can be **added or subtracted only if they have the same dimension**. The
resulting matrix is obtained by adding or subtracting **corresponding elements**.

Suppose:

$$
A =
\begin{bmatrix}
1 & 4 \\
2 & 5 \\
3 & 6
\end{bmatrix},
\qquad
B =
\begin{bmatrix}
1 & 2 \\
2 & 3 \\
3 & 4
\end{bmatrix}
\quad (3 \times 2)
$$

Then:

$$
A + B =
\begin{bmatrix}
1+1 & 4+2 \\
2+2 & 5+3 \\
3+3 & 6+4
\end{bmatrix}
=
\begin{bmatrix}
2 & 6 \\
4 & 8 \\
6 & 10
\end{bmatrix}
$$

and:

$$
A - B =
\begin{bmatrix}
1-1 & 4-2 \\
2-2 & 5-3 \\
3-3 & 6-4
\end{bmatrix}
=
\begin{bmatrix}
0 & 2 \\
0 & 2 \\
0 & 2
\end{bmatrix}
$$

In general, if:

$$
A = [a_{ij}], \quad B = [b_{ij}], \quad i=1,\ldots,r;\; j=1,\ldots,c
$$

then:

$$
A + B = [a_{ij} + b_{ij}], \qquad
A - B = [a_{ij} - b_{ij}]
\tag{5.8}
$$

Matrix addition is **commutative**:

$$
A + B = B + A
$$

---

### Regression Model in Matrix Form

Define the **mean response vector**:

$$
E\{Y\} =
\begin{bmatrix}
E\{Y_1\} \\
E\{Y_2\} \\
\vdots \\
E\{Y_n\}
\end{bmatrix}
\quad (n \times 1)
\tag{5.9}
$$

and the **error vector**:

$$
\varepsilon =
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{bmatrix}
\quad (n \times 1)
\tag{5.10}
$$

Using the observation vector $Y$, the regression model can be written compactly as:

$$
Y = E\{Y\} + \varepsilon
$$

This shows that the observed responses are the sum of their expected values and random
errors.

---

## 5.3 Matrix Multiplication

### Multiplication of a Matrix by a Scalar

A **scalar** is an ordinary number. When a matrix is multiplied by a scalar, **every element
of the matrix is multiplied by that scalar**.

If:

$$
A = [a_{ij}]
$$

and $k$ is a scalar, then:

$$
kA = [k a_{ij}]
\tag{5.11}
$$

If all elements of a matrix have a common factor, it may be factored out as a scalar.

---

### Multiplication of a Matrix by a Matrix

Matrix multiplication is defined using **row–column cross products**.

Let:

$$
A =
\begin{bmatrix}
2 & 5 \\
4 & 1
\end{bmatrix},
\qquad
B =
\begin{bmatrix}
4 & 6 \\
5 & 8
\end{bmatrix}
$$

The element in row 1, column 1 of $AB$ is:

$$
2(4) + 5(5) = 33
$$

The element in row 1, column 2 of $AB$ is:

$$
2(6) + 5(8) = 52
$$

Thus:

$$
AB =
\begin{bmatrix}
33 & 52 \\
21 & 32
\end{bmatrix}
$$

> **Important:** In matrix algebra, multiplication is **not commutative** in general:
>
> $$
> AB \neq BA
> $$

---

### Dimension Rule for Matrix Multiplication

If:
- $A$ has dimension $r \times c$,
- $B$ has dimension $c \times s$,

then the product $AB$ exists and has dimension:

$$
r \times s
$$

The $(i,j)$ element of $AB$ is:

$$
(AB)_{ij} = \sum_{k=1}^{c} a_{ik} b_{kj}
\tag{5.12}
$$

---

### Special Cases and Regression Examples

1. **Scalar result (1 × 1 matrix):**

$$
\begin{bmatrix}
2 & 3 & 5
\end{bmatrix}
\begin{bmatrix}
2 \\
3 \\
5
\end{bmatrix}
=
[38]
$$

This equals the scalar $38$.

2. **Quadratic form in regression:**

For the response vector:

$$
Y =
\begin{bmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{bmatrix}
$$

we have:

$$
Y'Y = [Y_1^2 + Y_2^2 + \cdots + Y_n^2]
= \sum Y_i^2
\tag{5.13}
$$

3. **Design matrix products**

For the design matrix $X$ in simple linear regression:

$$
X =
\begin{bmatrix}
1 & X_1 \\
1 & X_2 \\
\vdots & \vdots \\
1 & X_n
\end{bmatrix}
$$

we frequently use:

$$
X'X \quad (2 \times 2 \text{ matrix})
\tag{5.14}
$$

and:

$$
X'Y \quad (2 \times 1 \text{ vector})
\tag{5.15}
$$

These matrix products form the foundation for the **least squares estimator** in matrix
regression theory.


## 5.4 Special Types of Matrices

Certain special types of matrices arise repeatedly in regression analysis. The most important
ones are described below.

### Symmetric Matrix

A matrix $A$ is said to be **symmetric** if:

$$
A = A'
$$

That is, the matrix equals its transpose. For example:

$$
A =
\begin{bmatrix}
1 & 4 & 6 \\
4 & 2 & 5 \\
6 & 5 & 3
\end{bmatrix}
\quad (3 \times 3)
$$

A symmetric matrix must necessarily be **square**. Symmetric matrices commonly arise in
regression analysis when a matrix is premultiplied by its transpose. For example, the matrix
$X'X$ is always symmetric, which follows directly from its definition in equation (5.14).

---

### Diagonal Matrix

A **diagonal matrix** is a square matrix whose off-diagonal elements are all zero. Examples
include:

$$
A =
\begin{bmatrix}
a_1 & 0 & 0 \\
0 & a_2 & 0 \\
0 & 0 & a_3
\end{bmatrix},
\qquad
B =
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 10 & 0 & 0 \\
0 & 0 & 5 & 0 \\
0 & 0 & 0 & 4
\end{bmatrix}
$$

Often, zeros are omitted for compactness, and the diagonal matrix is written simply by listing
its diagonal elements.

Two important special cases of diagonal matrices are the **identity matrix** and the
**scalar matrix**.

---

### Identity Matrix

The **identity matrix**, denoted by $I$, is a diagonal matrix whose diagonal elements are all
equal to 1. Premultiplying or postmultiplying any $r \times r$ matrix $A$ by $I$ leaves $A$
unchanged:

$$
IA = AI = A
\tag{5.16}
$$

Thus, the identity matrix plays the same role in matrix algebra as the number 1 does in
ordinary algebra.

---

### Scalar Matrix

A **scalar matrix** is a diagonal matrix whose diagonal elements are all equal to the same
scalar $k$:

$$
kI =
\begin{bmatrix}
k & 0 & 0 \\
0 & k & 0 \\
0 & 0 & k
\end{bmatrix}
$$

Multiplying a matrix $A$ by the scalar matrix $kI$ is equivalent to multiplying $A$ by the
scalar $k$.

---

### Vector and Matrix with All Elements Unity

A column vector with all elements equal to 1 is denoted by $\mathbf{1}$:

$$
\mathbf{1} =
\begin{bmatrix}
1 \\
1 \\
\vdots \\
1
\end{bmatrix}
\quad (r \times 1)
$$

A square matrix with all elements equal to 1 is denoted by $J$:

$$
J =
\begin{bmatrix}
1 & 1 & \cdots & 1 \\
1 & 1 & \cdots & 1 \\
\vdots & \vdots & & \vdots \\
1 & 1 & \cdots & 1
\end{bmatrix}
\quad (r \times r)
$$

For an $n \times 1$ vector of ones, we have:

$$
\mathbf{1}\mathbf{1}' = J
\tag{5.17}
$$

and:

$$
\mathbf{1}'\mathbf{1} = n
\tag{5.18}
$$

---

### Zero Vector

A **zero vector** is a vector whose elements are all zero. The zero column vector is denoted
by $\mathbf{0}$:

$$
\mathbf{0} =
\begin{bmatrix}
0 \\
0 \\
\vdots \\
0
\end{bmatrix}
\quad (r \times 1)
\tag{5.19}
$$

---

## 5.5 Linear Dependence and Rank of a Matrix

### Linear Dependence

Consider the matrix:

$$
A =
\begin{bmatrix}
1 & 2 & 5 & 1 \\
2 & 2 & 10 & 6 \\
3 & 4 & 15 & 1
\end{bmatrix}
$$

Viewing the columns as vectors, observe that the third column is a multiple of the first
column. Hence, the columns are **linearly dependent**.

Formally, a set of column vectors $C_1, \ldots, C_c$ is linearly dependent if there exist
scalars $k_1, \ldots, k_c$, not all zero, such that:

$$
k_1 C_1 + k_2 C_2 + \cdots + k_c C_c = \mathbf{0}
\tag{5.20}
$$

If the only solution is $k_1 = \cdots = k_c = 0$, the vectors are said to be **linearly
independent**.

---

### Rank of a Matrix

The **rank** of a matrix is the maximum number of linearly independent columns (or rows)
in the matrix. In the above example, although there are four columns, only three are linearly
independent; therefore, the rank is 3.

For an $r \times c$ matrix, the rank cannot exceed $\min(r, c)$. If $C = AB$, then:

$$
\text{rank}(C) \le \min(\text{rank}(A), \text{rank}(B))
$$

---

## 5.6 Inverse of a Matrix

In matrix algebra, the inverse of a square matrix $A$ is denoted by $A^{-1}$ and satisfies:

$$
A^{-1}A = AA^{-1} = I
\tag{5.21}
$$

Only **square matrices** can have inverses, and not all square matrices do. A matrix has an
inverse if and only if it has **full rank**.

---

### Inverse of a $2 \times 2$ Matrix

If:

$$
A =
\begin{bmatrix}
a & b \\
e & d
\end{bmatrix}
$$

then:

$$
A^{-1} =
\frac{1}{D}
\begin{bmatrix}
d & -b \\
-e & a
\end{bmatrix},
\qquad
D = ad - be
\tag{5.22}
$$

$D$ is called the **determinant** of $A$. If $D = 0$, the matrix is singular and has no inverse.

---

### Inverse of a Diagonal Matrix

The inverse of a diagonal matrix is obtained by taking reciprocals of the diagonal elements.
For example:

$$
A =
\begin{bmatrix}
3 & 0 & 0 \\
0 & 4 & 0 \\
0 & 0 & 2
\end{bmatrix}
\quad \Rightarrow \quad
A^{-1} =
\begin{bmatrix}
\frac{1}{3} & 0 & 0 \\
0 & \frac{1}{4} & 0 \\
0 & 0 & \frac{1}{2}
\end{bmatrix}
$$

---

### Inverse of $X'X$ in Simple Linear Regression

For the design matrix $X$, we have:

$$
X'X =
\begin{bmatrix}
n & \sum X_i \\
\sum X_i & \sum X_i^2
\end{bmatrix}
$$

Using equation (5.22):

$$
(X'X)^{-1} =
\frac{1}{n\sum (X_i - \bar{X})^2}
\begin{bmatrix}
\sum X_i^2 & -\sum X_i \\
-\sum X_i & n
\end{bmatrix}
\tag{5.24}
$$

---

## Uses of the Inverse Matrix

In ordinary algebra, an equation like:

$$
5y = 20
$$

is solved by multiplying both sides by $5^{-1}$. Similarly, in matrix algebra, if:

$$
AY = C
$$

and $A^{-1}$ exists, then:

$$
Y = A^{-1}C
$$

This principle is fundamental in solving systems of linear equations and in deriving the
least squares estimator in regression.

---

## 5.7 Some Basic Results for Matrices

The following identities are frequently used:

$$
A + B = B + A \tag{5.25}
$$

$$
(A + B) + C = A + (B + C) \tag{5.26}
$$

$$
(AB)C = A(BC) \tag{5.27}
$$

$$
C(A + B) = CA + CB \tag{5.28}
$$

$$
k(A + B) = kA + kB \tag{5.29}
$$

$$
(A')' = A \tag{5.30}
$$

$$
(A + B)' = A' + B' \tag{5.31}
$$

$$
(AB)' = B'A' \tag{5.32}
$$

$$
(ABC)' = C'B'A' \tag{5.33}
$$

$$
(AB)^{-1} = B^{-1}A^{-1} \tag{5.34}
$$

$$
(ABC)^{-1} = C^{-1}B^{-1}A^{-1} \tag{5.35}
$$

$$
(A^{-1})^{-1} = A \tag{5.36}
$$

$$
(A')^{-1} = (A^{-1})' \tag{5.37}
$$

## 5.11 Fitted Values and Residuals

### Fitted Values

Let the vector of fitted values $\hat{Y}_i$ be denoted by $\hat{Y}$:

$$
\hat{Y}
\tag{5.70}
$$

In matrix notation, the fitted values are given by:

$$
\hat{Y} = Xb
\tag{5.71}
$$

because:

$$
\begin{bmatrix}
\hat{Y}_1 \\
\hat{Y}_2 \\
\vdots \\
\hat{Y}_n
\end{bmatrix}
=
\begin{bmatrix}
1 & X_1 \\
1 & X_2 \\
\vdots & \vdots \\
1 & X_n
\end{bmatrix}
\begin{bmatrix}
b_0 \\
b_1
\end{bmatrix}
=
\begin{bmatrix}
b_0 + b_1 X_1 \\
b_0 + b_1 X_2 \\
\vdots \\
b_0 + b_1 X_n
\end{bmatrix}
$$

---

### Hat Matrix

Using the expression for $b$ from equation (5.60), we can write:

$$
\hat{Y} = HY
\tag{5.73}
$$

where:

$$
H = X(X'X)^{-1}X'
\tag{5.73a}
$$

The matrix $H$ is called the **hat matrix** because it maps $Y$ to $\hat{Y}$. The fitted values
are linear combinations of the observed responses, with coefficients determined entirely by
the predictor values $X$.  

The hat matrix has two important properties:

* **Symmetry**: $H' = H$
* **Idempotency**: $HH = H$

---

### Residuals

Let the vector of residuals $e_i = Y_i - \hat{Y}_i$ be denoted by $e$. Then:

$$
e = Y - \hat{Y}
\tag{5.74}
$$

Substituting $\hat{Y} = Xb$:

$$
e = Y - Xb
\tag{5.75}
$$

Using the hat matrix:

$$
e = (I - H)Y
\tag{5.78}
$$

The matrix $I - H$ is also symmetric and idempotent.

---

### Variance–Covariance Matrix of Residuals

For the normal error regression model:

$$
\operatorname{Var}(e) = \sigma^2 (I - H)
\tag{5.79}
$$

The estimated variance–covariance matrix is:

$$
s^2\{e\} = \text{MSE}(I - H)
\tag{5.80}
$$

---

## 5.12 Analysis of Variance Results

### Sums of Squares

The total sum of squares can be written as:

$$
\text{SSTO} = \sum (Y_i - \bar{Y})^2
           = \sum Y_i^2 - \frac{(\sum Y_i)^2}{n}
$$

Using matrix notation:

$$
Y'Y = \sum Y_i^2
\tag{5.81}
$$

and:

$$
\frac{(\sum Y_i)^2}{n} = \frac{1}{n} Y'JY
\tag{5.82}
$$

where $J$ is the matrix of ones. Hence:

$$
\text{SSTO} = Y'Y - \frac{1}{n}Y'JY
\tag{5.83}
$$

---

### Error Sum of Squares

The error sum of squares is:

$$
\text{SSE} = e'e = (Y - Xb)'(Y - Xb)
\tag{5.84}
$$

which simplifies to:

$$
\text{SSE} = Y'Y - b'X'Y
\tag{5.84a}
$$

---

### Regression Sum of Squares

The regression sum of squares is:

$$
\text{SSR} = b'X'Y - \frac{1}{n}Y'JY
\tag{5.85}
$$

---

### Sums of Squares as Quadratic Forms

A **quadratic form** is an expression of the form:

$$
Y'AY = \sum_{i=1}^n \sum_{j=1}^n a_{ij}Y_iY_j,
\quad a_{ij} = a_{ji}
\tag{5.87}
$$

The ANOVA sums of squares can be written as quadratic forms:

$$
\text{SSTO} = Y'\left(I - \frac{1}{n}J\right)Y
\tag{5.88}
$$

$$
\text{SSE} = Y'(I - H)Y
\tag{5.89b}
$$

$$
\text{SSR} = Y'\left(H - \frac{1}{n}J\right)Y
\tag{5.89c}
$$

Thus, all ANOVA sums of squares arise from symmetric quadratic forms.

---

## 5.13 Inferences in Regression Analysis

### Regression Coefficients

The variance–covariance matrix of $b$ is:

$$
\operatorname{Var}(b) = \sigma^2 (X'X)^{-1}
\tag{5.91}
$$

Substituting MSE for $\sigma^2$ gives the estimated variance–covariance matrix:

$$
s^2\{b\} = \text{MSE}(X'X)^{-1}
\tag{5.93}
$$

These elements correspond to:

* $\operatorname{Var}(b_0)$
* $\operatorname{Var}(b_1)$
* $\operatorname{Cov}(b_0, b_1)$

---

### Mean Response at $X_h$

Define:

$$
X_h' = [1 \ \ X_h]
\tag{5.95}
$$

The fitted mean response is:

$$
\hat{Y}_h = X_h' b
\tag{5.96}
$$

The variance of $\hat{Y}_h$ is:

$$
\operatorname{Var}(\hat{Y}_h) = \sigma^2 X_h'(X'X)^{-1}X_h
\tag{5.97}
$$

or equivalently:

$$
\operatorname{Var}(\hat{Y}_h) = X_h' \operatorname{Var}(b) X_h
\tag{5.97a}
$$

The estimated variance is:

$$
s^2\{\hat{Y}_h\} = \text{MSE} \, X_h'(X'X)^{-1}X_h
\tag{5.98}
$$

---

### Prediction of a New Observation

The estimated variance for predicting a new observation at $X_h$ is:

$$
s^2_{\text{pred}} = \text{MSE}\left(1 + X_h'(X'X)^{-1}X_h\right)
\tag{5.100}
$$

This expression includes both the uncertainty in estimating the mean response and the
inherent variability of a new observation.


