

# Chapter 5: Matrix Approach to Simple Linear Regression Analysis

## 5.1 Matrices (Page 176)

### Definition of Matrix (Page 176)
A **matrix** is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns.
* An $r \times c$ matrix has $r$ rows and $c$ columns.
* Each element (entry) is denoted by $a_{ij}$, where $i$ is the row index and $j$ is the column index.

Example: A $2 \times 3$ matrix $A$:
$A = \begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \end{pmatrix}$

### Square Matrix (Page 178)
A **square matrix** is a matrix that has the same number of rows and columns ($r=c$).

Example: A $2 \times 2$ square matrix $B$:
$B = \begin{pmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \end{pmatrix}$

### Vector (Page 178)
A **vector** is a matrix with only one row or one column.
* A **column vector** has $r$ rows and 1 column (e.g., $r \times 1$).
* A **row vector** has 1 row and $c$ columns (e.g., $1 \times c$).

Example: Column vector $\mathbf{v} = \begin{pmatrix} v_1 \\ v_2 \end{pmatrix}$; Row vector $\mathbf{w} = \begin{pmatrix} w_1 & w_2 & w_3 \end{pmatrix}$

### Transpose (Page 178)
The **transpose** of a matrix $A$, denoted by $A^T$, is formed by interchanging its rows and columns. The element $a_{ij}$ in $A$ becomes $a_{ji}$ in $A^T$.
If $A$ is an $r \times c$ matrix, $A^T$ is a $c \times r$ matrix.

Example: If $A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix}$, then $A^T = \begin{pmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{pmatrix}$.
The transpose of a column vector is a row vector, and vice-versa.

### Equality of Matrices (Page 179)
Two matrices $A$ and $B$ are equal if and only if:
1.  They have the same dimensions (same number of rows and columns).
2.  All their corresponding elements are equal ($a_{ij} = b_{ij}$ for all $i,j$).

## 5.2 Matrix Addition and Subtraction (Page 180)

Matrices can be added or subtracted if and only if they have the **same dimensions**.
* **Addition:** $A+B=C$, where $c_{ij} = a_{ij} + b_{ij}$. (Element-wise addition)
* **Subtraction:** $A-B=D$, where $d_{ij} = a_{ij} - b_{ij}$. (Element-wise subtraction)

Example:
$\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix} + \begin{pmatrix} 5 & 6 \\ 7 & 8 \end{pmatrix} = \begin{pmatrix} 1+5 & 2+6 \\ 3+7 & 4+8 \end{pmatrix} = \begin{pmatrix} 6 & 8 \\ 10 & 12 \end{pmatrix}$

## 5.3 Matrix Multiplication (Page 182)

### Multiplication of a Matrix by a Scalar (Page 182)
Multiplying a matrix $A$ by a scalar $k$ (a single number) results in a new matrix where every element of $A$ is multiplied by $k$.
$kA = (k \cdot a_{ij})$

Example: $2 \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix} = \begin{pmatrix} 2 & 4 \\ 6 & 8 \end{pmatrix}$

### Multiplication of a Matrix by a Matrix (Page 182)
The product of two matrices $A$ (dimensions $r_A \times c_A$) and $B$ (dimensions $r_B \times c_B$) is possible if and only if the number of **columns in the first matrix ($c_A$) equals the number of rows in the second matrix ($r_B$)**. This is called the "inner dimensions" matching.
The resulting matrix $C = AB$ will have dimensions $r_A \times c_B$ (the "outer dimensions").

The element $c_{ij}$ of the product matrix $C$ is found by taking the dot product of the $i$-th row of $A$ and the $j$-th column of $B$.
$c_{ij} = \sum_{k=1}^{c_A} a_{ik} b_{kj}$

**Key Point:** Matrix multiplication is generally **not commutative** ($AB \neq BA$). The order matters!

Example:
$A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$ ($2 \times 2$), $B = \begin{pmatrix} 5 \\ 6 \end{pmatrix}$ ($2 \times 1$)
$C = AB = \begin{pmatrix} (1 \cdot 5) + (2 \cdot 6) \\ (3 \cdot 5) + (4 \cdot 6) \end{pmatrix} = \begin{pmatrix} 5+12 \\ 15+24 \end{pmatrix} = \begin{pmatrix} 17 \\ 39 \end{pmatrix}$ ($2 \times 1$)

## 5.4 Special Types of Matrices (Page 185)

### Symmetric Matrix (Page 185)
A **symmetric matrix** is a square matrix $A$ such that $A = A^T$. This means $a_{ij} = a_{ji}$ for all $i,j$. The elements are symmetric about the main diagonal.

Example: $\begin{pmatrix} 1 & 2 & 3 \\ 2 & 4 & 5 \\ 3 & 5 & 6 \end{pmatrix}$

### Diagonal Matrix (Page 185)
A **diagonal matrix** is a square matrix where all elements off the main diagonal are zero. The elements on the main diagonal can be any value.

Example: $\begin{pmatrix} 1 & 0 & 0 \\ 0 & 5 & 0 \\ 0 & 0 & 9 \end{pmatrix}$

A special diagonal matrix is the **Identity Matrix (I)**. It's a square matrix with 1s on the main diagonal and 0s elsewhere. It acts like the number '1' in scalar multiplication ($AI = IA = A$).

Example of $3 \times 3$ identity matrix: $I = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}$

### Vector and Matrix with All Elements Unity (Page 187)
* **Vector of ones ($\mathbf{1}$):** A column vector where all elements are 1.
    Example: $\mathbf{1} = \begin{pmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{pmatrix}$
* **Matrix of ones ($\mathbf{J}$):** A matrix where all elements are 1.
    Example: $\mathbf{J} = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}$

### Zero Vector (Page 187)
A **zero vector** ($\mathbf{0}$) is a vector where all elements are 0. It acts like the number '0' in scalar addition ($A+\mathbf{0}=A$).

## 5.5 Linear Dependence and Rank of Matrix (Page 188)

### Linear Dependence (Page 188)
A set of vectors is **linearly dependent** if at least one vector in the set can be expressed as a linear combination of the others. If no vector can be expressed as a linear combination of the others, they are **linearly independent**.
* In regression context: If one predictor variable is a linear combination of other predictors, they are linearly dependent, which causes **multicollinearity**.

### Rank of Matrix (Page 188)
The **rank** of a matrix is the maximum number of linearly independent row vectors (or column vectors) in the matrix.
* For an $r \times c$ matrix, the rank is at most $\min(r, c)$.
* A square matrix is **full rank** if its rank equals its number of rows (or columns). A full rank matrix is invertible.

## 5.6 Inverse of a Matrix (Page 189)

### Inverse (Page 189)
The **inverse** of a square matrix $A$, denoted $A^{-1}$, is a unique matrix such that when multiplied by $A$, it yields the identity matrix $I$:
$AA^{-1} = A^{-1}A = I$
* Only square matrices can have inverses.
* A square matrix is invertible (or **non-singular**) if and only if its determinant is non-zero, or equivalently, if it is full rank. If its determinant is zero, it is **singular** and has no inverse.

### Finding the Inverse (Page 190)
For small matrices, there are direct formulas. For example, for a $2 \times 2$ matrix $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$:
$A^{-1} = \frac{1}{ad-bc} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}$ (where $ad-bc$ is the determinant, and must be non-zero).
For larger matrices, methods like Gaussian elimination or using the adjoint matrix are employed. In practice, computers calculate inverses numerically.

### Uses of Inverse Matrix (Page 192)
The inverse matrix is crucial for:
* **Solving systems of linear equations:** If $A\mathbf{x} = \mathbf{b}$, then $\mathbf{x} = A^{-1}\mathbf{b}$. This is directly used in solving the normal equations in regression.
* **Deriving variances and covariances of parameter estimates** in regression.

## 5.7 Some Basic Results for Matrices (Page 193)

This section covers various algebraic properties of matrices. Key properties include:
* $(A^T)^T = A$
* $(A+B)^T = A^T+B^T$
* $(AB)^T = B^TA^T$ (Order reverses!)
* $(AB)^{-1} = B^{-1}A^{-1}$ (Order reverses!)
* If $A$ is symmetric, then $A^{-1}$ (if it exists) is also symmetric.

## 5.8 Random Vectors and Matrices (Page 193)

These are vectors or matrices whose elements are random variables.

### Expectation of Random Vector or Matrix (Page 193)
The **expectation** of a random vector or matrix is a vector or matrix where each element is the expectation of the corresponding random variable.
$E\{\mathbf{Y}\} = \begin{pmatrix} E\{Y_1\} \\ E\{Y_2\} \\ \vdots \\ E\{Y_n\} \end{pmatrix}$

### Variance-Covariance Matrix of Random Vector (Page 194)
The **variance-covariance matrix** of a random vector $\mathbf{Y}$ (denoted $Var\{\mathbf{Y}\}$ or $\Sigma$) is a symmetric matrix that contains the variances of the elements of $\mathbf{Y}$ on its main diagonal and the covariances between pairs of elements on its off-diagonals.
For a vector $\mathbf{Y} = (Y_1, Y_2, \dots, Y_n)^T$:
$Var\{\mathbf{Y}\} = \begin{pmatrix} Var\{Y_1\} & Cov\{Y_1,Y_2\} & \dots & Cov\{Y_1,Y_n\} \\ Cov\{Y_2,Y_1\} & Var\{Y_2\} & \dots & Cov\{Y_2,Y_n\} \\ \vdots & \vdots & \ddots & \vdots \\ Cov\{Y_n,Y_1\} & Cov\{Y_n,Y_2\} & \dots & Var\{Y_n\} \end{pmatrix}$
This matrix is fundamental for understanding the variability and relationships among multiple random variables.

### Some Basic Results (Page 196)
* For a constant matrix $A$ and random vector $\mathbf{Y}$: $E\{A\mathbf{Y}\} = A E\{\mathbf{Y}\}$
* For a constant matrix $A$ and random vector $\mathbf{Y}$: $Var\{A\mathbf{Y}\} = A Var\{\mathbf{Y}\} A^T$

### Multivariate Normal Distribution (Page 196)
This is an extension of the normal distribution to multiple random variables. A random vector $\mathbf{Y}$ is said to follow a multivariate normal distribution if its joint probability distribution can be characterized by a mean vector ($E\{\mathbf{Y}\}$ or $\boldsymbol{\mu}$) and a variance-covariance matrix ($Var\{\mathbf{Y}\}$ or $\Sigma$).
This distribution is crucial for inference in multiple regression, as it allows us to derive the sampling distributions of the estimators.

## 5.9 Simple Linear Regression Model in Matrix Terms (Page 197)

The simple linear regression model $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$ for $n$ observations can be expressed concisely in matrix form:
$\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$

Where:
* $\mathbf{Y}$ is an $n \times 1$ vector of observed dependent variable values:
    $\mathbf{Y} = \begin{pmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{pmatrix}$
* $\mathbf{X}$ is an $n \times 2$ design matrix (or model matrix) containing a column of ones (for the intercept) and a column of independent variable values:
    $\mathbf{X} = \begin{pmatrix} 1 & X_1 \\ 1 & X_2 \\ \vdots & \vdots \\ 1 & X_n \end{pmatrix}$
* $\boldsymbol{\beta}$ is a $2 \times 1$ vector of regression parameters (coefficients):
    $\boldsymbol{\beta} = \begin{pmatrix} \beta_0 \\ \beta_1 \end{pmatrix}$
* $\boldsymbol{\epsilon}$ is an $n \times 1$ vector of random error terms:
    $\boldsymbol{\epsilon} = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{pmatrix}$

**Assumptions in Matrix Terms:**
* $E\{\boldsymbol{\epsilon}\} = \mathbf{0}$ (mean of errors is zero)
* $Var\{\boldsymbol{\epsilon}\} = \sigma^2 \mathbf{I}$ (errors have constant variance $\sigma^2$ and are uncorrelated, where $\mathbf{I}$ is the identity matrix)
* $\boldsymbol{\epsilon}$ follows a multivariate normal distribution (for inference).

## 5.10 Least Squares Estimation of Regression Parameters (Page 199)

The objective of least squares is to minimize the sum of squared errors, $Q = \sum \epsilon_i^2 = \boldsymbol{\epsilon}^T \boldsymbol{\epsilon} = (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})^T (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})$.

### Normal Equations (Page 199)
By differentiating $Q$ with respect to $\boldsymbol{\beta}$ and setting the result to zero, we obtain the **normal equations** in matrix form:
$\mathbf{X}^T \mathbf{X} \mathbf{b} = \mathbf{X}^T \mathbf{Y}$
These are a system of linear equations that need to be solved for the vector of estimated regression coefficients, $\mathbf{b}$.

### Estimated Regression Coefficients (Page 200)
Assuming that $(\mathbf{X}^T \mathbf{X})$ is non-singular (i.e., its inverse exists, which implies no perfect multicollinearity), we can solve the normal equations for $\mathbf{b}$:
$\mathbf{b} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$
This elegant formula provides the least squares estimators for $\beta_0$ and $\beta_1$ (and generalizes directly to multiple regression).

## 5.11 Fitted Values and Residuals (Page 202)

### Fitted Values (Page 202)
The vector of fitted (predicted) values $\hat{\mathbf{Y}}$ is obtained by substituting the estimated coefficients $\mathbf{b}$ into the model:
$\hat{\mathbf{Y}} = \mathbf{X}\mathbf{b}$
Substituting the formula for $\mathbf{b}$:
$\hat{\mathbf{Y}} = \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$
The matrix $\mathbf{H} = \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T$ is called the **Hat Matrix**. It's a projection matrix that maps the observed $\mathbf{Y}$ values onto the regression hyperplane.
So, $\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}$. The hat matrix is symmetric and idempotent ($\mathbf{H}^2 = \mathbf{H}$).

### Residuals (Page 203)
The vector of residuals $\mathbf{e}$ is the difference between the observed and fitted values:
$\mathbf{e} = \mathbf{Y} - \hat{\mathbf{Y}}$
Substituting $\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}$:
$\mathbf{e} = \mathbf{Y} - \mathbf{H}\mathbf{Y} = (\mathbf{I} - \mathbf{H})\mathbf{Y}$
The matrix $(\mathbf{I} - \mathbf{H})$ is also symmetric and idempotent.

## 5.12 Analysis of Variance Results (Page 204)

The sums of squares (SST, SSR, SSE) can also be expressed concisely using matrix notation.

### Sums of Squares (Page 204)
* **Total Sum of Squares (SST):**
    $SST = \sum (Y_i - \bar{Y})^2 = \mathbf{Y}^T \mathbf{Y} - \frac{1}{n}(\mathbf{1}^T \mathbf{Y})^2$ (where $\mathbf{1}$ is a vector of ones)
* **Regression Sum of Squares (SSR):**
    $SSR = \sum (\hat{Y}_i - \bar{Y})^2 = \mathbf{b}^T \mathbf{X}^T \mathbf{Y} - \frac{1}{n}(\mathbf{1}^T \mathbf{Y})^2$
* **Error Sum of Squares (SSE):**
    $SSE = \sum (Y_i - \hat{Y}_i)^2 = \mathbf{e}^T \mathbf{e} = \mathbf{Y}^T \mathbf{Y} - \mathbf{b}^T \mathbf{X}^T \mathbf{Y}$
    Also, $SST = SSR + SSE$.

### Sums of Squares as Quadratic Forms (Page 205)
A quadratic form is an expression of the type $\mathbf{Y}^T A \mathbf{Y}$, where $A$ is a symmetric matrix.
* $SST = \mathbf{Y}^T (\mathbf{I} - \frac{1}{n}\mathbf{J}) \mathbf{Y}$
* $SSR = \mathbf{Y}^T (\mathbf{H} - \frac{1}{n}\mathbf{J}) \mathbf{Y}$
* $SSE = \mathbf{Y}^T (\mathbf{I} - \mathbf{H}) \mathbf{Y}$
Here, $\mathbf{J}$ is a matrix of ones. This representation highlights the underlying geometry of sums of squares as projections.

## 5.13 Inferences in Regression Analysis (Page 206)

The matrix approach simplifies the derivation and understanding of inference procedures.

### Regression Coefficients (Page 207)
* **Expected Value of $\mathbf{b}$:** Under the assumption $E\{\boldsymbol{\epsilon}\} = \mathbf{0}$, the least squares estimator $\mathbf{b}$ is unbiased:
    $E\{\mathbf{b}\} = \boldsymbol{\beta}$
* **Variance-Covariance Matrix of $\mathbf{b}$:** This is a crucial result. It provides the variances of $b_0$ and $b_1$ on the diagonal, and their covariance on the off-diagonal:
    $Var\{\mathbf{b}\} = \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}$
    This matrix is often denoted as $\boldsymbol{\Sigma}_{\mathbf{b}}$. The diagonal elements are $Var\{b_0\}$ and $Var\{b_1\}$, and the off-diagonal element is $Cov\{b_0, b_1\}$.
* **Estimated Variance-Covariance Matrix of $\mathbf{b}$:** Since $\sigma^2$ is unknown, we estimate it with $s^2 = MSE$.
    $s^2\{\mathbf{b}\} = s^2 (\mathbf{X}^T \mathbf{X})^{-1}$
    This is used to construct confidence intervals and conduct hypothesis tests for individual coefficients. For example, $s\{b_1\} = \sqrt{[s^2 (\mathbf{X}^T \mathbf{X})^{-1}]_{22}}$ (the second diagonal element).

### Mean Response (Page 208)
For a new observation $X_h = \begin{pmatrix} 1 \\ X_h \end{pmatrix}$, the estimated mean response is $\hat{Y}_h = \mathbf{X}_h^T \mathbf{b}$.
* **Expected Value of $\hat{Y}_h$:** $E\{\hat{Y}_h\} = \mathbf{X}_h^T \boldsymbol{\beta}$ (unbiased).
* **Variance of $\hat{Y}_h$:**
    $Var\{\hat{Y}_h\} = \sigma^2 \mathbf{X}_h^T (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}_h$
    The estimated variance is $s^2\{\hat{Y}_h\} = s^2 \mathbf{X}_h^T (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}_h$. This is used for confidence intervals for the mean response.

### Prediction of New Observation (Page 209)
For a new observation $Y_{h(new)}$ at $\mathbf{X}_h$:
* **Variance of Prediction Error:** The variance of the prediction error ($Y_{h(new)} - \hat{Y}_h$) is:
    $Var\{Y_{h(new)} - \hat{Y}_h\} = \sigma^2 [1 + \mathbf{X}_h^T (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}_h]$
    The estimated variance for the prediction interval is $s^2_{pred} = s^2 [1 + \mathbf{X}_h^T (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}_h]$. This is used for prediction intervals.

The matrix approach provides a unified and elegant framework for linear regression analysis, essential for understanding multiple regression and more advanced topics.
