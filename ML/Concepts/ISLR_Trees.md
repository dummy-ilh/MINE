md
# üå≥ Tree-Based Methods ‚Äî Core Notes (ISLR-aligned)

Tree-based methods model the regression or classification function by **partitioning the predictor space into rectangular regions** and making **simple predictions within each region**.  
They are **non-parametric**, **interpretable**, and naturally capture **non-linearities and interactions**.

---

## 1Ô∏è‚É£ Big Picture: What Is a Decision Tree?

A **decision tree** represents a set of **hierarchical decision rules** that recursively split the feature space.

Key idea:
> Instead of fitting a global function \( f(X) \), we approximate it **piecewise-constant** over regions.

---

## 2Ô∏è‚É£ Prediction via Stratification of the Feature Space

Assume predictors:
\[
X = (X_1, X_2, \dots, X_p)
\]

### Step 1: Partition the Predictor Space

Divide the feature space into:
\[
R_1, R_2, \dots, R_J
\]

- Regions are **non-overlapping**
- Each region corresponds to a **leaf node**

---

### Step 2: Make Predictions Within Each Region

For **regression trees**:
\[
\hat{y}(x) = \hat{y}_{R_j} = \frac{1}{|R_j|} \sum_{i \in R_j} y_i
\quad \text{if } x \in R_j
\]

üìå Every observation in the same region gets the **same prediction**.

---

## 3Ô∏è‚É£ Objective Function (Regression Trees)

We want regions that minimize the **Residual Sum of Squares (RSS)**:

\[
\text{RSS} =
\sum_{j=1}^{J}
\sum_{i \in R_j}
\left(y_i - \hat{y}_{R_j}\right)^2
\tag{8.1}
\]

---

## 4Ô∏è‚É£ Why Not Try All Possible Partitions?

- Number of possible partitions grows **exponentially**
- Exhaustive search is **computationally infeasible**

üëâ Solution: **Recursive Binary Splitting**

---

## 5Ô∏è‚É£ Recursive Binary Splitting (Core Algorithm)

### Key Characteristics

| Property | Meaning |
|------|--------|
| **Top-down** | Start with all data in one region |
| **Greedy** | Choose the best split *now*, not globally |
| **Binary** | Each split produces exactly two regions |

---

## 6Ô∏è‚É£ Single Split: Mathematical Formulation

Choose:
- Predictor \( X_j \)
- Cutpoint \( s \)

to form two regions:

\[
R_1(j,s) = \{X \mid X_j < s\}
\]
\[
R_2(j,s) = \{X \mid X_j \ge s\}
\tag{8.2}
\]

---

### Split Criterion

We select \( (j, s) \) that minimizes:

\[
\sum_{i: x_i \in R_1(j,s)}
(y_i - \hat{y}_{R_1})^2
+
\sum_{i: x_i \in R_2(j,s)}
(y_i - \hat{y}_{R_2})^2
\tag{8.3}
\]

where:
\[
\hat{y}_{R_k} = \text{mean of } y_i \text{ in region } R_k
\]

üìå This is a **greedy local optimization**.

---

## 7Ô∏è‚É£ Growing the Tree

After the first split:

1. Choose **one existing region**
2. Find the **best variable + cutpoint**
3. Split it into two subregions
4. Repeat

This continues until a **stopping rule** is met.

---

### Common Stopping Criteria

- Maximum tree depth
- Minimum number of observations in a node
- No further RSS reduction
- Pure node (classification)

---

## 8Ô∏è‚É£ Final Prediction Rule

For a new observation \( x_0 \):

1. Traverse the tree using decision rules
2. Land in region \( R_j \)
3. Predict:
\[
\hat{y}(x_0) = \hat{y}_{R_j}
\]

---

## 9Ô∏è‚É£ Why Trees Are Appealing

### ‚úÖ Advantages

- **Highly interpretable**
- Automatically capture **non-linearities**
- Naturally model **feature interactions**
- No need for feature scaling
- Works with mixed data types

---

### ‚ùå Limitations (Foreshadowing)

- High variance
- Greedy splits ‚â† globally optimal
- Piecewise-constant approximation
- Poor predictive accuracy alone

üìå These issues motivate:
- **Pruning**
- **Bagging**
- **Random Forests**
- **Boosting**

---

##  üîÅ Conceptual Comparison

| Method | Function Form | Bias | Variance |
|------|--------------|------|----------|
| Linear regression | Global linear | High (if non-linear truth) | Low |
| KNN | Local averaging | Low | High |
| Decision tree | Piecewise constant | Moderate | High |

---

## üß† ISLR Mental Model

> Trees trade **smoothness** for **interpretability**  
> and **global structure** for **local adaptivity**

---


md
# üå≥ Tree Pruning (ISLR ‚Äî Cost-Complexity / Weakest-Link Pruning)

Growing a tree via recursive binary splitting almost always **overfits**:
- Training RSS ‚Üì monotonically as the tree grows
- Test error typically follows a **U-shaped curve**

Tree pruning is the mechanism that **controls variance** while retaining
most of the signal.

---

## 1Ô∏è‚É£ Why Pruning Is Necessary

### Fully Grown Trees
- Very low **bias**
- Very high **variance**
- Extremely sensitive to small data perturbations

### Smaller Trees
- Slightly higher bias
- Much lower variance
- Easier to interpret
- Better test-set performance

üìå **Goal**: Find the tree that minimizes **test error**, not training error.

---

## 2Ô∏è‚É£ Why Early Stopping Is Not Ideal

A naive idea:
> Stop splitting when RSS reduction < threshold

‚ùå Problem:
- Greedy & myopic
- Early ‚Äúweak‚Äù splits may enable **strong splits later**
- Can block important interactions

---

## 3Ô∏è‚É£ Proper Strategy: Grow First, Prune Later

### High-Level Plan

1. Grow a **maximal tree** \( T_0 \)
2. Prune it back to form a **nested sequence of subtrees**
3. Use **cross-validation** to select the optimal subtree

This is **Cost-Complexity Pruning** (aka **Weakest-Link Pruning**).

---

## 4Ô∏è‚É£ Cost-Complexity Objective Function

For any subtree \( T \subset T_0 \), define:

\[
\underbrace{
\sum_{m=1}^{|T|}
\sum_{i: x_i \in R_m}
(y_i - \hat{y}_{R_m})^2
}_{\text{Training RSS}}
\;+\;
\underbrace{
\alpha |T|
}_{\text{Complexity Penalty}}
\tag{8.4}
\]

### Terms Explained

| Symbol | Meaning |
|----|----|
| \( |T| \) | Number of terminal nodes (leaves) |
| \( R_m \) | Region corresponding to leaf \( m \) |
| \( \hat{y}_{R_m} \) | Mean response in region \( R_m \) |
| \( \alpha \ge 0 \) | Complexity tuning parameter |

---

## 5Ô∏è‚É£ Role of the Tuning Parameter \( \alpha \)

### Extreme Cases

- **\( \alpha = 0 \)**  
  \(\Rightarrow\) No penalty ‚Üí fully grown tree \( T_0 \)

- **Large \( \alpha \)**  
  \(\Rightarrow\) Heavy penalty ‚Üí very small tree

### Interpretation

\[
\alpha = \text{cost of adding one more leaf}
\]

üìå This is directly analogous to:
- Lasso penalty in linear regression
- AIC / BIC‚Äìstyle regularization

---

## 6Ô∏è‚É£ Weakest-Link Pruning: Why It Works

As \( \alpha \) increases:
- Branches are pruned **one at a time**
- Resulting subtrees are **nested**:
\[
T_0 \supset T_1 \supset T_2 \supset \dots
\]

This yields a **small, manageable sequence of candidate trees**.

---

## 7Ô∏è‚É£ Full Algorithm (ISLR Algorithm 8.1)

### Step 1: Grow a Large Tree
- Recursive binary splitting
- Stop only when nodes are very small

---

### Step 2: Generate Subtrees
- Apply cost-complexity pruning
- Obtain optimal subtree for each \( \alpha \)

---

### Step 3: Choose \( \alpha \) via Cross-Validation

For each fold \( k \):
1. Grow and prune tree on \( K-1 \) folds
2. Evaluate MSE on the held-out fold

Average CV error over folds:
\[
\text{CV}(\alpha)
\]

Select:
\[
\alpha^\* = \arg\min_\alpha \text{CV}(\alpha)
\]

---

### Step 4: Refit on Full Training Data
- Use chosen \( \alpha^\* \)
- Return corresponding subtree

---

## 8Ô∏è‚É£ Bias‚ÄìVariance Perspective

| Tree Size | Bias | Variance |
|----|----|----|
| Large tree | Low | High |
| Pruned tree | Moderate | Lower |
| Very small tree | High | Low |

Pruning moves the model **leftward on the bias‚Äìvariance curve**.

---

## 9Ô∏è‚É£ Interpretation of the Hitters Example

### Observations

- Training error ‚Üì monotonically
- CV error has a **clear minimum**
- Test error closely tracks CV error

üìå **Key takeaway**:
> CV reliably identifies the tree size that balances bias and variance.

---

## 1Ô∏è‚É£0Ô∏è‚É£ Conceptual Connections

### Tree Pruning vs Other Regularization

| Method | Complexity Control |
|----|----|
| Linear regression | No control |
| Ridge | \( \lambda \sum \beta_j^2 \) |
| Lasso | \( \lambda \sum |\beta_j| \) |
| Trees | \( \alpha |T| \) |

---

## 1Ô∏è‚É£1Ô∏è‚É£ When Pruned Trees Are Still Not Enough

Even optimally pruned trees:
- Remain high-variance
- Are unstable to small data changes

üëâ Leads to:
- **Bagging**
- **Random Forests**
- **Boosting**

---

## üß† One-Sentence ISLR Insight

> **Grow aggressively, prune judiciously, validate rigorously.**

md
# üå≥ Classification Trees: Gini, Cross-Entropy, and Trees vs Linear Models (ISLR Notes)

---

## 1Ô∏è‚É£ Node Purity in Classification Trees

In **classification trees**, each terminal node contains observations from
(possibly) multiple classes.  
To decide **how good a split is**, we measure **node impurity**.

A node is **pure** if it contains observations from **only one class**.

---

## 2Ô∏è‚É£ Gini Index (Node Impurity)

For node \( m \), let  
\[
\hat{p}_{mk} = \text{proportion of observations in node } m \text{ that belong to class } k
\]

The **Gini index** is

\[
G_m = \sum_{k=1}^{K} \hat{p}_{mk}(1 - \hat{p}_{mk})
\]

### Key Properties
- \( G_m \ge 0 \)
- **Small Gini** ‚áí node is **pure**
- **Large Gini** ‚áí classes are mixed
- Maximum impurity when all classes are equally likely

### Intuition
\[
\hat{p}_{mk}(1 - \hat{p}_{mk}) = \text{probability of misclassification}
\]

So Gini measures **how often we would misclassify** if we randomly labeled
according to class proportions.

---

## 3Ô∏è‚É£ Cross-Entropy (Deviance)

An alternative impurity measure is **cross-entropy**:

\[
D_m = - \sum_{k=1}^{K} \hat{p}_{mk} \log \hat{p}_{mk}
\tag{8.7}
\]

### Mathematical Insight
Since  
\[
0 \le \hat{p}_{mk} \le 1 \quad \Rightarrow \quad -\hat{p}_{mk} \log \hat{p}_{mk} \ge 0
\]

- If \( \hat{p}_{mk} \approx 0 \) or \( \hat{p}_{mk} \approx 1 \):  
  ‚áí contribution ‚âà 0
- If classes are evenly mixed:  
  ‚áí cross-entropy is large

### Interpretation
- Measures **uncertainty** in class labels
- Closely related to **log-likelihood**
- Penalizes impure nodes **more strongly** than Gini

---

## 4Ô∏è‚É£ Gini vs Cross-Entropy vs Classification Error

| Criterion | Formula | Sensitivity | When Used |
|----|----|----|----|
| Classification Error | \( 1 - \max_k \hat{p}_{mk} \) | Low | Tree pruning |
| Gini Index | \( \sum \hat{p}_{mk}(1-\hat{p}_{mk}) \) | Medium | Tree growing |
| Cross-Entropy | \( -\sum \hat{p}_{mk}\log \hat{p}_{mk} \) | High | Tree growing |

### Why Not Use Classification Error for Splitting?
- Too **insensitive** to changes in node purity
- Gini & entropy detect **subtle improvements** in purity

üìå **ISLR rule of thumb**:
- **Grow tree** ‚Üí Gini or Cross-Entropy  
- **Prune tree** ‚Üí Classification Error (prediction-focused)

---

## 5Ô∏è‚É£ Trees vs Linear Models: Model Forms

### Linear Models (Chapters 3 & 4)

\[
f(X) = \beta_0 + \sum_{j=1}^{p} X_j \beta_j
\tag{8.8}
\]

- Global linear relationship
- Same coefficients apply everywhere in feature space

---

### Regression / Classification Trees

\[
f(X) = \sum_{m=1}^{M} c_m \cdot \mathbb{1}(X \in R_m)
\tag{8.9}
\]

- Feature space partitioned into regions \( R_1, \dots, R_M \)
- Constant prediction within each region
- Highly **non-linear**, **piecewise-constant**

---

## 6Ô∏è‚É£ When Do Linear Models Win?

Linear regression/classification works best when:
- True relationship is **approximately linear**
- Effects are **additive**
- Signal-to-noise ratio is high
- Interpretability and inference matter

üìå Trees **fail to exploit linear structure** efficiently.

---

## 7Ô∏è‚É£ When Do Trees Win?

Decision trees excel when:
- Relationship is **highly non-linear**
- Strong **interactions** between variables
- Important decision boundaries are **axis-aligned**
- Interpretability in rule form is needed

Example:
> ‚ÄúIf income < 50k AND age < 30 ‚Üí class A‚Äù

---

## 8Ô∏è‚É£ Advantages of Trees over Classical Models

### ‚úÖ Interpretability
- Easier to explain than regression coefficients
- Readable decision rules

### ‚úÖ Human Decision Analogy
- Mirrors real-world decision processes

### ‚úÖ Visualization
- Graphical display of model structure

### ‚úÖ Handles Qualitative Predictors Naturally
- No dummy variables required

---

## 9Ô∏è‚É£ Major Disadvantage of Trees

### ‚ùå Predictive Accuracy
- High variance
- Unstable to small data changes
- Often inferior to:
  - Linear models (when linearity holds)
  - Ensemble methods (Random Forests, Boosting)

üìå **Key ISLR insight**:
> A single tree is interpretable but weak; many trees together are powerful.

---

## üîë One-Line Summary

> **Gini and cross-entropy guide how trees grow; classification error guides how trees are pruned, and trees trade accuracy for interpretability compared to linear models.**



md
# üå≤ Resampling Methods in Tree-Based Models (ISLR Notes)

These notes consolidate and *conceptually deepen* the ISLR discussion on **bootstrap**, **bagging**, **random forests**, and **boosting**, with emphasis on **variance reduction**, **bias‚Äìvariance tradeoff**, and **error estimation**.

---

## 1. Motivation: Why Resampling for Trees?

Decision trees (Section 8.1) suffer from **high variance**:

- Small changes in training data  
  $\Rightarrow$ very different trees  
- Deep trees fit noise strongly  
- Prediction instability is the core issue

Formally:
- Trees have **low bias**
- But **high variance**

Goal: **Reduce variance without increasing bias too much**

---

## 2. Bootstrap Recap (Why It Works)

Given $Z_1,\dots,Z_n$ i.i.d. with
\[
\mathrm{Var}(Z_i) = \sigma^2
\]

The variance of the mean:
\[
\mathrm{Var}(\bar Z) = \frac{\sigma^2}{n}
\]

**Key insight**  
> *Averaging reduces variance*

Bootstrap mimics repeated sampling from the population using resampling *with replacement* from the training data.

---

## 3. Bagging (Bootstrap Aggregation)

### 3.1 Core Idea

Instead of fitting **one unstable model**, fit **many unstable models** and average them.

#### Ideal (but impossible):
\[
\hat f_{\text{avg}}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat f_b(x)
\]

#### Practical (bootstrap-based):
\[
\hat f_{\text{bag}}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat f_b^{*}(x)
\]

Where:
- Each $\hat f_b^{*}$ is trained on a bootstrap sample
- Trees are **grown deep (unpruned)**

---

### 3.2 Bias‚ÄìVariance View

For regression:
\[
\mathrm{Var}\left(\frac{1}{B}\sum_{b=1}^B \hat f_b(x)\right)
= \frac{1}{B^2}\sum_{b=1}^B \mathrm{Var}(\hat f_b(x))
\]

If trees are weakly correlated:
\[
\approx \frac{1}{B}\mathrm{Var}(\hat f(x))
\]

‚úÖ **Variance ‚Üì dramatically**  
‚ùå Bias remains roughly unchanged

Hence:
> Bagging works best for **high-variance, low-bias** learners (like trees)

---

### 3.3 Classification via Bagging

For qualitative $Y$:
- Each tree predicts a class
- Final prediction = **majority vote**

---

## 4. Out-of-Bag (OOB) Error

### 4.1 Why OOB Works

In a bootstrap sample of size $n$:
\[
P(\text{observation not selected}) = \left(1-\frac{1}{n}\right)^n \approx e^{-1} \approx 0.37
\]

So:
- Each tree uses ~2/3 of data
- ~1/3 are **out-of-bag**

---

### 4.2 OOB Prediction

For observation $i$:
- Predict using only trees where $i$ was OOB
- Aggregate predictions

Compute:
- MSE (regression)
- Misclassification rate (classification)

üìå **OOB error ‚âà LOOCV error (for large $B$)**  
üìå No need for cross-validation

---

## 5. Random Forests (Brief Contrast)

### Problem with Bagging:
Trees remain **highly correlated** because:
- Strong predictors dominate splits

### Random Forest Fix:
At each split:
- Randomly select $m < p$ predictors

Effect:
- Trees decorrelated
- Variance ‚Üì further
- Bias slightly ‚Üë

---

## 6. Boosting (Very Different Philosophy)

### 6.1 Conceptual Shift

Bagging:
- Independent trees
- Parallel
- Variance reduction

Boosting:
- Sequential trees
- Each tree fixes **previous mistakes**
- Bias + variance reduction

---

## 7. Boosting for Regression Trees (Algorithm 8.2)

### Initialization
\[
\hat f(x) = 0, \quad r_i = y_i
\]

### Iteration $b = 1,\dots,B$

1. Fit tree $\hat f_b$ (depth $d$) to residuals $(X, r)$
2. Update model:
\[
\hat f(x) \leftarrow \hat f(x) + \lambda \hat f_b(x)
\]
3. Update residuals:
\[
r_i \leftarrow r_i - \lambda \hat f_b(x_i)
\]

Final model:
\[
\hat f(x) = \sum_{b=1}^B \lambda \hat f_b(x)
\]

---

## 8. Boosting Intuition (Why It Works)

- Fits **small trees** (weak learners)
- Learns **slowly**
- Focuses on **hard-to-predict regions**
- Shrinkage $(\lambda)$ prevents overfitting

> *Slow learning = strong generalization*

---

## 9. Boosting Tuning Parameters

### 1. Number of Trees $B$
- Too large $\Rightarrow$ overfitting possible
- Chosen via cross-validation

### 2. Shrinkage $\lambda$
- Typical values: $0.01$, $0.001$
- Smaller $\lambda$ ‚Üí need larger $B$

### 3. Tree Depth $d$
- $d=1$: stumps ‚Üí additive model
- Larger $d$: higher-order interactions

---

## 10. Bias‚ÄìVariance Comparison Summary

| Method | Bias | Variance | Overfitting |
|------|------|----------|-------------|
| Single Tree | Low | High | Yes |
| Bagging | Low | ‚Üì‚Üì‚Üì | No |
| Random Forest | Slightly ‚Üë | ‚Üì‚Üì‚Üì‚Üì | No |
| Boosting | ‚Üì | ‚Üì | Possible |

---

## 11. Big Picture Takeaway

- **Bagging**: variance reduction via averaging
- **Random forests**: variance reduction via decorrelation
- **Boosting**: bias + variance reduction via sequential learning
- **OOB error**: free test error estimate

> Trees are weak alone, but extraordinarily powerful in ensembles.

---
md
# üå≥ Tree-Based Classification ‚Äî Deep Mathematical & Conceptual Notes (ISLR)

These notes address **four core conceptual questions** from ISLR with
**mathematical grounding**, **bias‚Äìvariance intuition**, and **modeling tradeoffs**.

---

## 1. Mathematical Comparison: **Gini Index vs Cross-Entropy**

Consider a node $m$ with class proportions:
\[
\hat p_{mk}, \quad k = 1,\dots,K
\]

---

### 1.1 Gini Index

\[
\text{Gini}(m) = \sum_{k=1}^K \hat p_{mk}(1 - \hat p_{mk})
= 1 - \sum_{k=1}^K \hat p_{mk}^2
\]

**Interpretation**
- Measures probability of misclassification if labels are assigned randomly
- Penalizes mixed nodes
- Quadratic penalty

---

### 1.2 Cross-Entropy (Deviance)

\[
D(m) = - \sum_{k=1}^K \hat p_{mk} \log \hat p_{mk}
\]

**Interpretation**
- Negative log-likelihood of multinomial model
- Stronger penalty near $0$ and $1$
- Logarithmic sensitivity

---

### 1.3 Mathematical Relationship (Binary Case)

Let $\hat p = P(Y=1)$.

| $\hat p$ | Gini | Entropy |
|--------|------|---------|
| 0 or 1 | 0 | 0 |
| 0.5 | 0.5 | $\log 2 \approx 0.693$ |

**Second-order Taylor expansion near $0.5$:**
\[
-\hat p \log \hat p - (1-\hat p)\log(1-\hat p)
\approx 2(\hat p - 0.5)^2
\]

This is **quadratic**, like Gini.

üìå **Conclusion**
> Gini and entropy are numerically very similar; entropy is slightly more sensitive to extreme probabilities.

---

### 1.4 Why Not Classification Error for Splitting?

\[
E(m) = 1 - \max_k \hat p_{mk}
\]

- Piecewise constant
- Insensitive to purity changes
- Poor gradient signal

üìå Used mainly for **pruning**, not splitting.

---

## 2. Classification Trees vs Logistic Regression

---

### 2.1 Model Form

#### Logistic Regression
\[
\log\left(\frac{P(Y=1 \mid X)}{1-P(Y=1 \mid X)}\right)
= \beta_0 + \sum_{j=1}^p X_j \beta_j
\]

- Linear decision boundary
- Parametric
- Global model

---

#### Classification Tree
\[
f(X) = \sum_{m=1}^M c_m \cdot \mathbf{1}(X \in R_m)
\]

- Axis-aligned splits
- Piecewise constant
- Non-parametric, local

---

### 2.2 Decision Boundary Geometry

| Model | Boundary Shape |
|-----|----------------|
| Logistic | Linear |
| Tree | Rectangular, stepwise |

---

### 2.3 Interpretability

- Trees: rule-based logic  
  *‚ÄúIf age > 50 and cholesterol > 200 ‚Üí class A‚Äù*
- Logistic: coefficient interpretation via odds ratios

---

### 2.4 Statistical Efficiency

- Logistic regression:
  - Lower variance
  - Efficient when model is correct
- Trees:
  - Higher variance
  - Robust to nonlinearity and interactions

---

## 3. Why Ensembles Fix Tree Weaknesses

---

### 3.1 Core Tree Weakness

\[
\text{High Variance}
\]

Small perturbations in data $\Rightarrow$ very different trees.

---

### 3.2 Bagging: Variance Reduction by Averaging

For predictors $\hat f_b(x)$:
\[
\mathrm{Var}\left(\frac{1}{B}\sum_{b=1}^B \hat f_b(x)\right)
= \rho \sigma^2 + \frac{1-\rho}{B}\sigma^2
\]

Where:
- $\rho$ = correlation between trees
- $\sigma^2$ = variance of individual tree

üìå Bagging works when:
- $\sigma^2$ is large
- $\rho$ is moderate

---

### 3.3 Random Forests: Decorrelating Trees

- Random feature selection reduces $\rho$
- Massive variance reduction

---

### 3.4 Boosting: Bias + Variance Reduction

Boosting minimizes:
\[
\sum_i L(y_i, \hat f(x_i))
\]

via **stagewise gradient descent**.

- Focuses on hard observations
- Sequential correction
- Lowers bias **and** variance

---

## 4. Bias‚ÄìVariance Comparison: Trees vs Linear Models

---

### 4.1 Bias‚ÄìVariance Decomposition

For squared error:
\[
\mathbb{E}[(Y - \hat f(X))^2]
= \text{Bias}^2 + \text{Variance} + \sigma^2
\]

---

### 4.2 Linear Models

\[
\hat f(X) = \beta_0 + \sum_{j=1}^p X_j \beta_j
\]

- **High bias** if true relationship is nonlinear
- **Low variance** when $n \gg p$

üìå Stable estimators

---

### 4.3 Trees

\[
\hat f(X) = \sum_{m=1}^M c_m \mathbf{1}(X \in R_m)
\]

- **Low bias** (very flexible)
- **High variance** (unstable)

---

### 4.4 Ensemble Effect

| Model | Bias | Variance |
|-----|------|----------|
| Linear | High | Low |
| Tree | Low | High |
| Bagged Trees | Low | ‚Üì |
| Random Forest | Slightly ‚Üë | ‚Üì‚Üì |
| Boosted Trees | ‚Üì | ‚Üì |

---

## 5. When to Prefer What?

| Scenario | Preferred Model |
|-------|----------------|
| Linear signal, small data | Logistic / Linear |
| Strong interactions | Trees |
| High accuracy needed | Random Forest / Boosting |
| Interpretability | Small trees / Logistic |
| Noisy data | Ensembles |

---

## 6. Unified Insight

> Linear models control variance by restricting form.  
> Trees control bias by increasing flexibility.  
> Ensembles combine many unstable low-bias learners to create a stable high-performance model.

---
md
# üå≤ Advanced Tree-Based Learning: Deep Theoretical Notes

These notes go **beyond textbook intuition**, focusing on **class imbalance**, **optimization theory**, and **noise sensitivity**‚Äîall central to interviews and research-level understanding.

---

## 1. Entropy vs Gini Under **Class Imbalance**

Consider a binary classification node with:

\[
\hat p = P(Y=1), \quad 1-\hat p = P(Y=0)
\]

---

### 1.1 Definitions

**Gini Index**
\[
G(\hat p) = 2\hat p(1-\hat p)
\]

**Entropy**
\[
H(\hat p) = -\hat p \log \hat p - (1-\hat p)\log(1-\hat p)
\]

---

### 1.2 Behavior Near Class Imbalance

Assume **severe imbalance**: $\hat p \ll 1$

#### Entropy (Taylor expansion near 0)
\[
H(\hat p) \approx -\hat p \log \hat p
\]

- Decays **slowly**
- Strong penalty for minority uncertainty
- Encourages splits that isolate rare class

---

#### Gini (quadratic)
\[
G(\hat p) \approx 2\hat p
\]

- Decays **linearly**
- Less sensitive to minority purity

---

### 1.3 Numerical Illustration

| $\hat p$ | Gini | Entropy |
|--------|------|---------|
| 0.01 | 0.0198 | 0.056 |
| 0.05 | 0.095 | 0.199 |
| 0.1 | 0.18 | 0.325 |

üìå **Key Insight**
> Entropy reacts more strongly to improvements in minority-class purity ‚Üí preferred when rare classes matter.

---

### 1.4 Practical Consequence

- **Entropy** ‚Üí better minority recall
- **Gini** ‚Üí slightly more balanced splits
- Differences are subtle but matter in **imbalanced medical / fraud data**

---

## 2. Boosting as **Functional Gradient Descent**

Boosting is often misunderstood as ‚Äúreweighting data.‚Äù
It is actually **gradient descent in function space**.

---

### 2.1 Objective Function

Minimize empirical risk:
\[
\mathcal{L}(f) = \sum_{i=1}^n L(y_i, f(x_i))
\]

Where:
- $f(x)$ is a function, not a vector
- $L$ is a loss (squared, logistic, exponential)

---

### 2.2 Additive Model Form

Boosting builds:
\[
f_B(x) = \sum_{b=1}^B \lambda h_b(x)
\]

Each $h_b(x)$ is a **weak learner** (tree).

---

### 2.3 Functional Gradient Step

At iteration $b$:
\[
r_i^{(b)} = -\left.\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}\right|_{f = f_{b-1}}
\]

We:
1. Compute **negative gradient** (pseudo-residuals)
2. Fit a tree to $r_i^{(b)}$
3. Update:
\[
f_b(x) = f_{b-1}(x) + \lambda h_b(x)
\]

---

### 2.4 Special Case: Squared Error Loss

\[
L(y,f) = (y - f)^2
\]

\[
r_i = y_i - f(x_i)
\]

üìå Boosting becomes **residual fitting**.

---

### 2.5 Classification Losses

| Loss | Boosting Variant |
|----|----------------|
| Exponential | AdaBoost |
| Logistic | LogitBoost |
| Any differentiable | Gradient Boosting |

---

### 2.6 Why Small Trees?

- Trees approximate local gradient directions
- Depth controls **interaction order**
- Depth-1 ‚Üí additive model
- Depth-$d$ ‚Üí $d$-way interactions

---

## 3. Why Random Forests Beat Boosting on **Noisy Data**

---

### 3.1 Noise Sensitivity

Noise = mislabeled or irreducible variance.

---

### 3.2 Boosting: Noise Amplifier

Boosting:
- Focuses on **hard-to-predict points**
- Treats noise as signal
- Keeps increasing weight on mislabeled observations

Mathematically:
\[
\text{Boosting minimizes } \sum_i L(y_i, f(x_i))
\]
Noise inflates gradients ‚Üí wrong direction

üìå Leads to **overfitting to noise**

---

### 3.3 Random Forests: Noise Averager

RF:
- Each tree sees bootstrap sample
- No sequential dependence
- No emphasis on hard points

\[
\hat f_{\text{RF}}(x) = \frac{1}{B}\sum_{b=1}^B \hat f_b(x)
\]

üìå Noise averages out

---

### 3.4 Bias‚ÄìVariance Tradeoff

| Method | Bias | Variance | Noise Robust |
|------|------|----------|-------------|
| Single Tree | Low | High | ‚ùå |
| RF | Slightly ‚Üë | ‚Üì‚Üì‚Üì | ‚úÖ |
| Boosting | ‚Üì‚Üì | ‚Üì | ‚ùå (label noise) |

---

### 3.5 Empirical Rule of Thumb

| Data Property | Winner |
|--------------|-------|
| Clean signal | Boosting |
| High noise | Random Forest |
| Many features | RF |
| Strong interactions | Boosting |
| Small data | Boosting (careful Œª) |

---

## 4. Unified Big Picture

> **Entropy** sees rare classes more sharply.  
> **Boosting** is gradient descent over functions.  
> **Random forests** win when noise dominates signal.










