import torch
import pandas as pd
from collections import defaultdict
from transformers import BertTokenizer, BertForSequenceClassification
from captum.attr import IntegratedGradients

# Load your BERT model and tokenizer
model_path = "path_to_your_bert_model_checkpoint"
tokenizer = BertTokenizer.from_pretrained(model_path)
model = BertForSequenceClassification.from_pretrained(model_path)

# Put the model in evaluation mode
model.eval()

# Define your training DataFrame (assuming you have 'text' and 'label' columns)
# For demonstration, here's a dummy DataFrame
data = {'text': ["The product is great!", "Not worth the price.", "Amazing performance and quality."],
        'label': [1, 0, 1]}
df = pd.DataFrame(data)

# Define the function to predict using the model
def predict(input_ids, attention_mask):
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.logits

# Initialize Integrated Gradients
ig = IntegratedGradients(predict)

# Create a function to compute attributions for a given text
def compute_word_importances(text):
    # Tokenize the input text
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    # Compute attributions using Integrated Gradients
    attributions = ig.attribute(inputs=input_ids, additional_forward_args=(attention_mask,), target=1, n_steps=50)
    attributions = attributions.sum(dim=-1).squeeze(0)

    # Convert token ids back to words
    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())

    # Return a dictionary of word importances
    return {token: float(attr) for token, attr in zip(tokens, attributions)}

# Accumulate word-level importance across all texts in the dataset
word_importance_aggregate = defaultdict(list)

# Iterate over each text in the DataFrame
for text in df['text']:
    word_importances = compute_word_importances(text)
    for word, importance in word_importances.items():
        # Accumulate the importance scores for each word
        word_importance_aggregate[word].append(importance)

# Aggregate the word importance by averaging the values
word_importance_final = {word: sum(importances) / len(importances) 
                         for word, importances in word_importance_aggregate.items()}

# Sort the words by their global importance scores
sorted_word_importances = sorted(word_importance_final.items(), key=lambda x: x[1], reverse=True)

# Display the top words with highest importance
for word, importance in sorted_word_importances[:10]:
    print(f"Word: {word}, Global Importance: {importance:.4f}")
