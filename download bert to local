from transformers import BertModel, BertTokenizer

# Choose a BERT model variant
bert_variant = "bert-base-uncased"

# Create a directory to save the BERT model
save_dir = "path/to/your/local/directory"  # Replace with your desired directory path

# Load the BERT model and tokenizer
model = BertModel.from_pretrained(bert_variant)
tokenizer = BertTokenizer.from_pretrained(bert_variant)

# Save the model and tokenizer to the specified directory
model.save_pretrained(save_dir)
tokenizer.save_pretrained(save_dir)

print("BERT model and tokenizer saved to", save_dir)












from transformers import BertModel, BertTokenizer

# Load the BERT model and tokenizer from the local directory
load_dir = "path/to/your/local/directory"  # Replace with the actual directory path

model = BertModel.from_pretrained(load_dir)
tokenizer = BertTokenizer.from_pretrained(load_dir)

# Example text
text = "This is an example sentence."

# Tokenize the text and encode it
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
outputs = model(**inputs)

# Access the encoded features
last_hidden_states = outputs.last_hidden_state

print("Original Text:", text)
print("Tokenized Text:", tokenizer.convert_ids_to_tokens(inputs["input_ids"][0].tolist()))
print("Encoded Features Shape:", last_hidden_states.shape)
