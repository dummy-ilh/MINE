
# ğŸ“˜ RAG Daily Tutorial

## **Day 5 â€” Retrieval Strategies: From Basic to Advanced**

---

# 1ï¸âƒ£ The NaÃ¯ve Retrieval Strategy (Baseline)

```
query_embedding â†’ top-k similarity â†’ send to LLM
```

Usually:

* k = 3 or 5
* cosine similarity
* no filtering

This works for demos.

It fails in production.

Why?

* Too little context â†’ incomplete answer
* Too much context â†’ hallucination
* Wrong chunk ranked high â†’ misleading answer

---

# 2ï¸âƒ£ Choosing k (Underrated Design Decision)

### If k is too small:

* Miss critical info
* Multi-hop queries fail

### If k is too large:

* Context overflow
* Noise injection
* Increased hallucination risk

### Better approach:

Instead of fixed k:

### ğŸ”¹ Threshold-based retrieval

Retrieve all chunks where:

$[
\text{similarity} > \tau
]$

Or:

### ğŸ”¹ Dynamic k

Keep retrieving until:

* similarity drops sharply
* token budget reached

This is more robust than fixed k=5.

---

# 3ï¸âƒ£ Hybrid Retrieval (Dense + Sparse)

Dense retrieval:

* Good for paraphrasing

Sparse retrieval (BM25):

* Good for exact matches
* Good for numbers
* Good for rare terms

Real systems combine both.

### Scoring method:

$[
Score = \alpha \cdot Dense + (1-\alpha) \cdot Sparse
]$

or

Two independent retrievals â†’ merge results.

Hybrid is almost always better than pure dense.

---

# 4ï¸âƒ£ Re-Ranking (This Is Where Quality Jumps)

Dense retrieval retrieves candidates.

But similarity search is approximate.

So we apply:

## Cross-Encoder Re-Ranker

Instead of embedding query and doc separately:

Feed both together into a model:

```
$[Query]$ + $[Document]$
â†’ relevance score
```

This allows:

* Deep semantic comparison
* Token-level interaction

Pipeline:

```
Query
 â†“
Dense retrieve (top 20)
 â†“
Re-rank with cross-encoder
 â†“
Top 5 to LLM
```

This dramatically improves answer grounding.

Cost:

* More compute
* Higher latency

Worth it in high-accuracy systems.

---

# 5ï¸âƒ£ Query Expansion (Often Ignored)

User query:

> â€œLate refund rulesâ€

Better retrieval query:

> â€œRefunds requested after policy deadline manual review processâ€

How to generate expansions?

* LLM rewriting
* Synonym expansion
* Multi-query retrieval (generate 3 variations)

Then merge results.

This improves recall significantly.

---

# 6ï¸âƒ£ Multi-Query Retrieval (Advanced but Powerful)

Instead of:

```
Retrieve once
```

Do:

```
Generate 3 reformulations
Retrieve for each
Merge unique chunks
Re-rank
```

Why?
Because embeddings sometimes anchor on different aspects of meaning.

This is used in:

* Enterprise QA
* Legal RAG
* Research retrieval

---

# 7ï¸âƒ£ Iterative Retrieval (Multi-Hop Questions)

Question:

> â€œWhat penalties apply if a refund exceeds the 30-day window in Germany?â€

You need:

1. Refund policy
2. Germany-specific clause
3. Penalty subsection

Single retrieval may miss this.

Better approach:

Step 1 â†’ retrieve
Step 2 â†’ refine query using retrieved context
Step 3 â†’ retrieve again

This is primitive multi-hop RAG.

---

# 8ï¸âƒ£ Retrieval Failure Modes

## âŒ High similarity but irrelevant

Cause:

* Generic chunk
* Large chunk
* Embedding bias

Fix:

* Better chunking
* Re-ranking

---

## âŒ Relevant doc not retrieved

Cause:

* Embedding model mismatch
* Poor query phrasing
* Low ANN recall

Fix:

* Hybrid search
* Query rewriting
* Increase search depth

---

## âŒ Over-retrieval causing hallucination

Cause:

* Too many chunks
* Conflicting info

Fix:

* Aggressive re-ranking
* Context compression

---

# 9ï¸âƒ£ Retrieval Strategy Ladder (Maturity Model)

Level 1:

* Top-k dense

Level 2:

* Hybrid search

Level 3:

* Dense + re-ranking

Level 4:

* Multi-query + re-ranking

Level 5:

* Iterative retrieval + verification

Most real systems stop at Level 2.

High-performing systems reach Level 4+.

---

# ğŸ”Ÿ Interview-Level Answer

If asked:

> â€œHow would you improve retrieval quality in a RAG system?â€

Strong answer:

> â€œI would move beyond naive top-k dense retrieval by introducing hybrid search for exact term recall, cross-encoder re-ranking for semantic precision, and multi-query expansion to improve recall coverage while maintaining a strict token budget.â€

That shows system depth.

---

# ğŸ§  Mental Model

Retrieval has two goals:

* Maximize recall (donâ€™t miss relevant info)
* Maximize precision (donâ€™t send noise to LLM)

RAG quality depends more on retrieval strategy than model size.

---

Perfect.
Retrieval strategy is where most senior interviews separate â€œRAG userâ€ from â€œRAG architect.â€

Iâ€™ll give you:

1. ğŸ§  Concept Build-Up
2. ğŸ¯ Retrieval Strategy Interview Questions (with strong answers)
3. ğŸ“Œ Retrieval Strategy Cheatsheet (condensed high-impact reference)

This will cover hybrid search, query rewriting, multi-stage retrieval, routing, fusion, and evaluation.

---

# ğŸ§  PART 1: Concept Build-Up â€” What Is a Retrieval Strategy?

A retrieval strategy answers:

> Given a query and a corpus, how do I efficiently retrieve the most relevant, diverse, grounded information?

It is not just:

* â€œUse vector search.â€

It includes:

* Query rewriting
* Metadata filtering
* Shard routing
* ANN search
* Hybrid search
* Reranking
* Context assembly
* Failure handling

Retrieval strategy = **decision logic over retrieval components**.

---

# ğŸ¯ PART 2: Retrieval Strategy â€” Interview Q&A

---

## Q1ï¸âƒ£ What is a retrieval strategy in a RAG system?

**Answer:**

A retrieval strategy is the structured pipeline that determines:

* How queries are processed
* Which retrieval mechanisms are used
* How candidates are filtered, ranked, and assembled
* How relevance vs latency tradeoffs are managed

It optimizes for:

* Recall
* Precision
* Latency
* Cost
* Faithfulness

---

## Q2ï¸âƒ£ When would you use hybrid search?

Hybrid search combines:

* Sparse retrieval (BM25)
* Dense retrieval (vector similarity)

Use hybrid when:

* Exact keyword matching matters
* Rare terminology exists
* Legal or technical terms must match exactly
* Embeddings underperform for specific jargon

Example:
Query: â€œSection 409A compliance clauseâ€

Vector may miss exact clause reference.
BM25 retrieves exact phrase.

Hybrid = best of both worlds.

---

## Q3ï¸âƒ£ How do you combine sparse and dense scores?

Common methods:

1. Weighted sum
2. Reciprocal Rank Fusion (RRF)
3. Two-stage reranking

RRF is popular:

Score = Î£ (1 / (k + rank_i))

Itâ€™s robust and simple.

---

## Q4ï¸âƒ£ How would you design retrieval for multi-hop reasoning?

Problem:
Query requires combining multiple documents.

Example:
â€œWhat changed in policy between 2022 and 2023?â€

Strategy:

* Retrieve docs for 2022
* Retrieve docs for 2023
* Compare sections
* Expand graph neighbors

Techniques:

* Graph-based expansion
* Iterative retrieval
* Self-ask prompting

---

## Q5ï¸âƒ£ How do you handle ambiguous queries?

Example:
â€œVacation policyâ€

Strategy:

* Query rewriting:

  * Expand with synonyms
  * Add likely entity types
* Intent classification
* Ask clarifying question
* Retrieve top clusters and disambiguate

---

## Q6ï¸âƒ£ What is query rewriting and why use it?

Query rewriting transforms user queries into:

* More searchable form
* Expanded terms
* Clarified intent

Techniques:

* LLM-based expansion
* Synonym injection
* Entity normalization
* Time constraint inference

Example:
User: â€œWFH rulesâ€
Rewrite:
â€œWork from home policy eligibility rulesâ€

Improves recall.

---

## Q7ï¸âƒ£ When would you use multi-stage retrieval?

When:

* Corpus > 1M documents
* Latency constraints exist
* Precision is critical

Pipeline:

```
Metadata filter
 â†’ ANN top 100
 â†’ Reranker top 10
 â†’ Diversity filter top 5
 â†’ LLM
```

This reduces hallucination.

---

## Q8ï¸âƒ£ How do you reduce retrieval noise?

* Strong metadata filtering
* Smaller chunk sizes
* Better embeddings
* Reranking
* Remove near-duplicates
* Namespace isolation

Noise reduction improves:

* Faithfulness
* Token efficiency

---

## Q9ï¸âƒ£ How do you handle time-sensitive queries?

Add time-awareness:

* Index by timestamp
* Boost recent documents
* Filter by date
* Decay older content

This prevents outdated answers.

---

## QğŸ”Ÿ How do you evaluate retrieval strategy?

Metrics:

Retrieval:

* Recall@K
* Precision@K
* NDCG

Generation:

* Faithfulness
* Citation correctness

System:

* P95 latency
* QPS

You must measure:
Recall vs latency tradeoff.

---

# ğŸ“Œ PART 3: RETRIEVAL STRATEGY CHEATSHEET

This is your high-impact condensed reference.

---

## ğŸ”· Retrieval Types

| Strategy     | When to Use                 |
| ------------ | --------------------------- |
| Pure Dense   | Semantic similarity queries |
| Pure Sparse  | Exact term queries          |
| Hybrid       | Technical/legal queries     |
| Graph-based  | Multi-hop reasoning         |
| Hierarchical | Structured documents        |
| Query-aware  | Mixed query types           |

---

## ğŸ”· Query Processing Techniques

* Intent classification
* Query rewriting
* Expansion
* Normalization
* Time inference
* Entity extraction

---

## ğŸ”· Retrieval Pipeline Template

Basic:

```
Query â†’ Embedding â†’ ANN â†’ LLM
```

Intermediate:

```
Query â†’ Rewrite â†’ Metadata Filter â†’ ANN â†’ Rerank â†’ LLM
```

Advanced:

```
Query â†’ Intent Classifier
       â†’ Rewrite
       â†’ Shard Routing
       â†’ Hybrid Search
       â†’ ANN Top 100
       â†’ Cross-Encoder Rerank
       â†’ Diversity Filter
       â†’ Context Assembly
       â†’ LLM
```

---

## ğŸ”· Optimization Levers

| Lever              | Improves            |
| ------------------ | ------------------- |
| Smaller chunk size | Recall              |
| Reranker           | Precision           |
| Metadata filtering | Latency + Precision |
| Query rewriting    | Recall              |
| Diversity filter   | Context quality     |
| Sharding           | Scalability         |
| Caching            | Latency             |

---

## ğŸ”· Common Failure Modes

| Problem              | Likely Cause      |
| -------------------- | ----------------- |
| Wrong doc retrieved  | Poor embedding    |
| Right doc ignored    | Bad chunking      |
| Too slow             | efSearch too high |
| Inconsistent answers | ANN randomness    |
| Outdated answer      | No time filter    |

---

## ğŸ”· Staff-Level Insight

Retrieval strategy is not static.

It should adapt to:

* Query intent
* Corpus size
* Latency budget
* Risk tolerance
* Compliance level

Elite systems dynamically change strategy per query.

---

# Final Interview-Level Summary

Retrieval strategy is the art of balancing:

Recall
Precision
Latency
Cost
Scalability
Faithfulness

If you optimize only one, you degrade others.

---

If you want next:

* âš”ï¸ Hybrid Search Deep Dive
* ğŸ§  Query Routing Architectures
* ğŸ—ï¸ Billion-scale Retrieval System Design
* ğŸ“‰ Retrieval-Induced Hallucination Analysis
* ğŸ”¬ End-to-End RAG Evaluation Framework

Choose the next depth.

