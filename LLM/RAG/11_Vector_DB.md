---

# üìò Day 11 ‚Äî Vector Stores in Practice (Architecture & Tradeoffs)

---

# 1Ô∏è‚É£ First Principle

A vector store must handle:

1. Fast similarity search
2. Persistent storage
3. Metadata filtering
4. Scaling
5. Updates (inserts/deletes)

Not all tools handle these equally.

---

# 2Ô∏è‚É£ The Main Options

We‚Äôll group them into categories:

---

## üîπ FAISS (Library, Not a Database)

What it is:

* C++/Python library
* Extremely fast
* Used internally by many systems

Pros:

* Full control
* High performance
* Flexible index types

Cons:

* No built-in persistence (you manage it)
* No metadata filtering (you build it)
* No distributed scaling out of the box

Best for:

* Research
* Custom infra
* Small/medium-scale self-hosted systems

Think of FAISS as an engine, not a car.

---

## üîπ Pinecone (Managed Vector DB)

What it is:

* Fully managed vector database
* API-based

Pros:

* Automatic scaling
* Built-in metadata filtering
* Persistent storage
* Easy deployment

Cons:

* Cost
* Vendor lock-in
* Less algorithmic control

Best for:

* Startups
* Production apps needing quick deployment

---

## üîπ Weaviate / Milvus (Open-Source Vector Databases)

What they are:

* Full databases
* REST/gRPC APIs
* Distributed

Pros:

* Metadata + vector search integrated
* Horizontal scaling
* On-prem deployment

Cons:

* Operational overhead
* DevOps complexity

Best for:

* Enterprise
* Privacy-sensitive workloads
* Large-scale systems

---

## üîπ Chroma (Developer-Friendly)

What it is:

* Lightweight vector store
* Good for prototypes

Pros:

* Simple
* Easy to integrate

Cons:

* Not ideal for large-scale production
* Limited advanced scaling features

Best for:

* Prototyping
* Local experiments

---

# 3Ô∏è‚É£ Real Comparison Table

| Feature            | FAISS  | Pinecone | Weaviate/Milvus | Chroma  |
| ------------------ | ------ | -------- | --------------- | ------- |
| Persistence        | Manual | Yes      | Yes             | Yes     |
| Metadata filtering | Manual | Yes      | Yes             | Basic   |
| Scaling            | Manual | Auto     | Distributed     | Limited |
| Control over index | Full   | Limited  | Moderate        | Limited |
| DevOps effort      | High   | Low      | Medium-High     | Low     |

---

# 4Ô∏è‚É£ The Hidden Question: Scale

The choice depends mostly on:

* Number of vectors
* Query per second (QPS)
* Update frequency
* Latency SLA

Let‚Äôs break it down.

---

## üîπ Small Scale (< 100k vectors)

* FAISS exact search works
* Low infra complexity
* No need for distributed DB

---

## üîπ Medium Scale (100k‚Äì5M vectors)

* HNSW required
* Persistent storage required
* Metadata filtering required

Now you need:

* Pinecone
* Weaviate
* Milvus
* Or heavy FAISS customization

---

## üîπ Large Scale (5M+ vectors)

Now problems arise:

* Memory pressure
* Sharding
* Load balancing
* Index rebuild times

You must think about:

* Horizontal scaling
* Partitioning
* Index replication

At this stage:
Managed DBs or distributed vector DBs become practical.

---

# 5Ô∏è‚É£ Sharding Strategy (Often Ignored)

You can shard by:

* Namespace (HR vs Legal)
* Time (2023 vs 2024)
* Geography
* Access level

Why?

It:

* Improves recall
* Reduces search space
* Improves latency
* Simplifies access control

Sharding is often more impactful than switching DB vendors.

---

# 6Ô∏è‚É£ Update & Re-Indexing Strategy

Production systems must handle:

* New documents daily
* Document edits
* Document deletions

Key questions:

* Does index support deletes efficiently?
* Does update require full rebuild?
* Is downtime acceptable?

HNSW:

* Supports incremental inserts well
* Deletes more complex

IVF:

* Requires careful rebalancing

These details matter at scale.

---

# 7Ô∏è‚É£ Observability (What Most Teams Forget)

A production vector system must log:

* Retrieved chunk IDs
* Similarity scores
* Latency
* Missed retrievals
* ANN recall degradation

Without observability:
You cannot debug retrieval failures.

---

# 8Ô∏è‚É£ When Switching DBs Won‚Äôt Fix Your Problem

If your RAG fails:

90% of the time it‚Äôs:

* Bad chunking
* Weak embeddings
* Poor retrieval strategy

Not:

* Wrong vector database

Infrastructure is rarely the root cause.

---

# 9Ô∏è‚É£ Enterprise Architecture Example

A robust enterprise setup might look like:

```
Ingestion Service
   ‚Üì
Embedding Service
   ‚Üì
Vector DB (Milvus)
   ‚Üì
Metadata DB (Postgres)
   ‚Üì
Retriever Service
   ‚Üì
LLM Service
```

Notice:
Vector search is just one component.

---

# üîü Interview-Level Answer

If asked:

> ‚ÄúHow do you choose a vector store for production?‚Äù

Strong answer:

> ‚ÄúI base the decision on dataset size, QPS requirements, update frequency, and operational constraints. For small-scale systems, FAISS is sufficient. For managed scalability, Pinecone works well. For enterprise on-prem deployments, distributed systems like Milvus or Weaviate are more appropriate.‚Äù

That shows architectural maturity.

---

# üß† Mental Model

Embeddings define space.
ANN navigates space.
Vector DB operationalizes space.

But:

RAG performance ‚â† database choice.
It‚Äôs a system-level design problem.

---

Excellent. Now we move into **production-grade vector database architecture**.

We‚Äôll go deep into:

* Persistence
* Sharding
* Replication
* Indexing strategies
* Consistency
* Compaction
* Metadata filtering
* Hybrid search
* Failure recovery
* Scaling patterns

This is **senior/staff-level infra knowledge**.

---

# 1Ô∏è‚É£ Persistence in Vector Databases

## üî• Core Question

How do embeddings survive process restarts and crashes?

---

## üß† Why Persistence is Harder for Vector DBs

Unlike traditional DBs:

* Vectors are large (e.g., 768‚Äì4096 dims)
* ANN indices (like HNSW, IVF) are complex graph structures
* Index rebuild can take hours for millions of vectors

So we must persist:

1. Raw vectors
2. Metadata
3. Index structure

---

## üèóÔ∏è How Persistence Works

### 1Ô∏è‚É£ Write-Ahead Log (WAL)

Before inserting:

```
Append vector + metadata ‚Üí WAL
Then update in-memory index
```

If crash:

* Replay WAL
* Rebuild in-memory state

Exactly like traditional DBs.

---

### 2Ô∏è‚É£ Snapshotting

Periodically:

* Save full index to disk
* Clear WAL

Prevents WAL from growing infinitely.

---

### 3Ô∏è‚É£ Memory-Mapped Indexes

Some systems store index files on disk but memory-map them:

* OS loads pages lazily
* Faster startup
* Less RAM pressure

---

### 4Ô∏è‚É£ Background Compaction

If vectors are deleted:

* Mark tombstone
* Rebuild compacted index later

Otherwise search quality degrades.

---

## üî• Design Tradeoff

| Approach        | Fast Writes | Fast Reads | Crash Safe |
| --------------- | ----------- | ---------- | ---------- |
| In-memory only  | ‚úÖ           | ‚úÖ          | ‚ùå          |
| WAL + snapshot  | ‚úÖ           | ‚úÖ          | ‚úÖ          |
| Disk-only index | ‚ùå           | ‚ö†Ô∏è         | ‚úÖ          |

---

# 2Ô∏è‚É£ Sharding

## üî• Core Question

How do you scale to billions of vectors?

---

## üß† What is Sharding?

Split dataset across multiple machines.

Instead of:

```
1 machine ‚Üí 1B vectors
```

You do:

```
Machine 1 ‚Üí 200M
Machine 2 ‚Üí 200M
Machine 3 ‚Üí 200M
...
```

---

## üî• Types of Sharding

### 1Ô∏è‚É£ Hash-Based Sharding

Shard = hash(vector_id) % N

Pros:

* Even distribution
* Simple

Cons:

* No semantic locality
* Hard to scale N later

---

### 2Ô∏è‚É£ Range-Based Sharding

Shard by:

* Time
* User ID
* Category

Good for:

* Multi-tenant systems

---

### 3Ô∏è‚É£ Semantic Sharding (Advanced)

Cluster embeddings first.
Each shard holds similar vectors.

Improves:

* Locality
* Retrieval speed

Harder to rebalance.

---

## üî• Query Flow in Sharded System

User Query:

1. Embed query
2. Send to all shards (fan-out)
3. Each shard returns top-k
4. Merge & re-rank globally

This is called **scatter-gather architecture**.

---

## ‚ö†Ô∏è Bottleneck

If 50 shards:

* 50 parallel searches
* Network latency increases

Solutions:

* Hierarchical routing
* Shard selection model

---

# 3Ô∏è‚É£ Replication

## Why Needed?

* Fault tolerance
* Read scaling

---

### Replication Modes

### 1Ô∏è‚É£ Leader-Follower

* Leader handles writes
* Followers handle reads

Strong consistency if synchronous.

---

### 2Ô∏è‚É£ Eventual Consistency

Writes propagate async.

Pros:

* Fast writes
  Cons:
* Slight stale reads

Most vector DBs use eventual consistency.

---

# 4Ô∏è‚É£ Indexing Concepts

## üî• Exact Search

Brute force:

[
\text{distance}(q, x_i)
]

Time complexity:

[
O(Nd)
]

Not scalable.

---

## üî• ANN (Approximate Nearest Neighbor)

### HNSW

Graph-based index:

* Nodes = vectors
* Edges = neighbors
* Search is greedy traversal

Fast:
[
O(\log N)
]

---

### IVF (Inverted File Index)

1. Cluster vectors (k-means)
2. Search nearest cluster
3. Search inside cluster

Tradeoff:

* Speed vs accuracy

---

### PQ (Product Quantization)

Compress vectors into codes.
Reduces memory drastically.

---

# 5Ô∏è‚É£ Metadata Filtering

Critical in production.

Example:

‚ÄúFind documents about tax law after 2021.‚Äù

Flow:

1. Filter metadata
2. Perform vector search inside filtered subset

If not:

* Retrieve irrelevant vectors

Two strategies:

* Pre-filter (recommended)
* Post-filter (less efficient)

---

# 6Ô∏è‚É£ Hybrid Search

Combine:

* BM25 (keyword)
* Vector similarity

Score:

[
Score = \alpha \cdot BM25 + (1-\alpha) \cdot Cosine
]

Used heavily in enterprise RAG.

---

# 7Ô∏è‚É£ Consistency Models

### Strong Consistency

All replicas reflect latest writes.

Slow but safe.

---

### Eventual Consistency

Updates propagate asynchronously.

Good for:

* Large-scale retrieval
* Embedding systems

---

# 8Ô∏è‚É£ Failure Recovery

Scenario:
Node crashes.

Recovery:

1. Restart
2. Load snapshot
3. Replay WAL
4. Rejoin cluster

If shard lost entirely:

* Replica promoted

---

# 9Ô∏è‚É£ Rebalancing Shards

If you add machines:

Problem:
Hash-based sharding breaks.

Solution:
Consistent hashing ring.

Only small fraction of data moves.

---

# üîü Multi-Tenancy

Two strategies:

1. Shared index + metadata filtering
2. Dedicated shard per tenant

Tradeoff:
Isolation vs cost

---

# üî• Real Production Challenges

* Index rebuild time
* Cold start latency
* High memory usage
* Deletion fragmentation
* Embedding drift over time
* Re-indexing after model upgrade

---

# üî• FAANG-Level Interview Question

‚ÄúDesign a vector DB that scales to 10B vectors.‚Äù

Strong answer must include:

* Sharding
* Replication
* WAL
* Snapshotting
* ANN index choice
* Metadata filtering
* Rebalancing
* Monitoring

---

# üî• Final Mental Model

Vector DB is:

Not just:

> A similarity search library

It is:

> A distributed storage + indexing + replication + retrieval + persistence system optimized for high-dimensional geometry.

---

When building production-grade retrieval systems (RAG, semantic search, recommendation engines), understanding **vector database internals** is critical.

Let‚Äôs go deep into the core infrastructure concepts:

* Persistence
* Sharding
* Replication
* Indexing
* Partitioning
* Compaction
* Consistency
* Write-Ahead Logging (WAL)
* Metadata filtering
* Caching
* Horizontal vs Vertical scaling

---

# 1Ô∏è‚É£ Persistence in Vector Databases

### What is Persistence?

Persistence means **vectors survive process restarts and crashes**.

If your server restarts, your embeddings must still exist.

---

## How It Works Internally

Vector DBs store data in:

1. **Disk-based storage (SSD)**
2. **Memory-mapped files**
3. **Write-Ahead Logs (WAL)**
4. **Snapshot checkpoints**

---

### Typical Write Flow

When inserting a vector:

```
Client ‚Üí Memory Buffer ‚Üí WAL ‚Üí Disk Index ‚Üí Acknowledgement
```

Steps:

1. Vector is written to WAL (append-only log)
2. Inserted into in-memory index
3. Periodically flushed to disk
4. Snapshot created

If crash occurs:

* WAL replays missing inserts

---

## Types of Persistence Models

| Model                  | Description          | Tradeoff       |
| ---------------------- | -------------------- | -------------- |
| In-memory only         | Fast but volatile    | Data loss risk |
| Hybrid (memory + disk) | Most common          | Balanced       |
| Fully disk-based       | Large-scale datasets | Slower         |

---

## Why This Matters for RAG

Without persistence:

* Embeddings must be regenerated
* Cost explosion (OpenAI embedding API cost)
* Latency spikes

---

# 2Ô∏è‚É£ Sharding in Vector Databases

### What is Sharding?

Sharding = splitting dataset across multiple machines.

Instead of:

```
1 machine storing 1B vectors
```

You do:

```
Shard 1 ‚Üí 250M vectors
Shard 2 ‚Üí 250M vectors
Shard 3 ‚Üí 250M vectors
Shard 4 ‚Üí 250M vectors
```

---

## Why Sharding?

Because:

* Memory limits
* CPU constraints
* Parallel search
* Horizontal scaling

---

## How Vector Sharding Works

Two main strategies:

---

### A. Hash-based Sharding

```
shard_id = hash(vector_id) % N
```

Pros:

* Uniform distribution
* Simple

Cons:

* Poor locality
* Rebalancing expensive

---

### B. Semantic / Partition-based Sharding

Cluster vectors first:

```
Cluster 1 ‚Üí Shard 1
Cluster 2 ‚Üí Shard 2
Cluster 3 ‚Üí Shard 3
```

Pros:

* Faster ANN search
* Better pruning

Cons:

* Rebalancing complex

---

## Query Flow in Sharded System

```
User Query ‚Üí Broadcast to all shards ‚Üí Each shard returns top-k ‚Üí Merge ‚Üí Final top-k
```

Time complexity:

* Search parallelized
* Merge cost = O(shards √ó k log k)

---

# 3Ô∏è‚É£ Replication

### What is Replication?

Replication = storing same shard on multiple machines.

Why?

* Fault tolerance
* High availability
* Load balancing

---

## Types of Replication

| Type            | Description                           |
| --------------- | ------------------------------------- |
| Leader-Follower | Writes to leader, reads from replicas |
| Multi-leader    | Writes anywhere                       |
| Quorum-based    | Majority consensus                    |

---

## Tradeoffs

| Strong Consistency | Eventual Consistency |
| ------------------ | -------------------- |
| Slower             | Faster               |
| Safer              | Slight stale reads   |

---

# 4Ô∏è‚É£ Indexing (ANN Internals)

Vector search is expensive:

Brute force = O(N √ó d)

Where:

* N = vectors
* d = dimension

So we use ANN (Approximate Nearest Neighbor).

---

## Common Index Types

### HNSW (Hierarchical Navigable Small World)

* Graph-based
* Logarithmic search
* High memory usage
* Very fast recall

Used by:

* Pinecone
* Qdrant

---

### IVF (Inverted File Index)

Steps:

1. K-means cluster centroids
2. Assign vectors to clusters
3. Search only top clusters

Used in:

* Milvus
* Faiss

---

### Product Quantization (PQ)

Compress vectors:

* Split into subvectors
* Quantize each

Used for:

* Billion-scale datasets

---

# 5Ô∏è‚É£ Partitioning vs Sharding

| Concept      | Meaning            |
| ------------ | ------------------ |
| Sharding     | Across machines    |
| Partitioning | Logical separation |

Example:

```
Partition: user_id=123
Partition: user_id=456
```

Used for:

* Multi-tenancy
* Faster filtering

---

# 6Ô∏è‚É£ Write-Ahead Logging (WAL)

WAL ensures durability.

Before modifying index:

```
Append change to log file
```

If crash:

* Replay WAL
* Restore state

Same concept used in:

* PostgreSQL

---

# 7Ô∏è‚É£ Compaction

Vector DBs often use LSM-tree style storage.

Over time:

* Many small files created
* Deletes marked but not removed

Compaction:

* Merges segments
* Reclaims disk
* Improves search speed

---

# 8Ô∏è‚É£ Consistency Models

Important for distributed vector DB.

### Strong Consistency

* Query sees latest write

### Eventual Consistency

* Slight delay in visibility

In RAG:

* Eventual consistency usually fine

In financial retrieval:

* Strong consistency required

---

# 9Ô∏è‚É£ Metadata Filtering

Real systems need hybrid search:

```
Vector similarity + metadata filter
```

Example:

```
embedding similarity
WHERE country = 'India'
AND date > 2024
```

Requires:

* Inverted index for metadata
* Filter-first or search-first strategy

---

# üîü Caching

Types:

1. Embedding cache
2. Query result cache
3. ANN graph cache
4. Hot shard cache

Critical for:

* High QPS production systems

---

# 1Ô∏è‚É£1Ô∏è‚É£ Horizontal vs Vertical Scaling

### Vertical Scaling

Increase:

* RAM
* CPU

Limit:

* Hardware ceiling

---

### Horizontal Scaling

Add more nodes:

* Sharding
* Replication
* Distributed routing

Production vector DBs must support horizontal scaling.

---

# üî• Production Architecture Example

Typical large-scale RAG system:

```
API Layer
    ‚Üì
Query Router
    ‚Üì
Shard Nodes (HNSW index)
    ‚Üì
Replica Nodes
    ‚Üì
Persistent Storage + WAL
```

---

# üöÄ Advanced Concepts (FAANG-level)

### 1. Hybrid Search

BM25 + Vector fusion

### 2. Re-ranking Layer

Cross-encoder after ANN

### 3. Tiered Storage

Hot (RAM) + Cold (Disk)

### 4. Auto Rebalancing

Dynamic shard redistribution

### 5. Online Index Rebuild

Rebuild HNSW without downtime

---

# üí° Real-World Vector Databases

* Pinecone
* Weaviate
* Milvus
* Qdrant
* Chroma

Each differs in:

* Index types
* Distributed design
* Storage engine
* Consistency guarantees

---

# üéØ If You Want to Master This

To deeply understand vector DBs, study:

1. ANN algorithms (HNSW, IVF, PQ)
2. Distributed systems fundamentals
3. Storage engines (LSM vs B-tree)
4. CAP theorem
5. Memory mapping and cache locality

---

Good. Let‚Äôs treat this like an architecture interview.

You‚Äôre asking:

> **Which vector DB should I use ‚Äî and why?**

There is no universal best.
There is only: *best for your constraints.*

We‚Äôll analyze this systematically.

---

# üîç Step 1: First Principles ‚Äî What Actually Matters?

Before naming tools, ask:

1. Dataset size? (10K vs 10M vs 1B vectors)
2. Update frequency? (static vs streaming inserts)
3. Latency target? (100ms vs 10ms)
4. Budget? (self-hosted vs managed)
5. Filtering needs? (heavy metadata filtering?)
6. Multi-tenant?
7. Enterprise compliance?

Now let‚Äôs compare the major options properly.

---

# ü•á 1Ô∏è‚É£ Pinecone

![Image](https://dezyre.gumlet.io/images/blog/pinecone-vector-database/Pinecone_Vectorb_Database_Architecture_Diagram.webp?dpr=2.6\&w=376)

![Image](https://s3.amazonaws.com/dd-app-listings/pinecone/media/pinecone-dashboard.png)

![Image](https://cdn.sanity.io/images/vr8gru94/production/606382d0ca90a8d24f26780f5f9954123e37be91-575x603.png)

![Image](https://cdn.sanity.io/images/vr8gru94/production/791910350d7d2140dbe684b405ef5ee761c8fc6a-1060x720.png)

## Why Choose It?

* Fully managed
* Strong distributed system
* Handles sharding + replication automatically
* Excellent reliability
* Very production-ready

## Internals

* HNSW-based ANN
* Automatic sharding
* Replication for HA
* Metadata filtering
* Strong infra design

## Best For

* Startup building production RAG
* Teams that don‚Äôt want DevOps overhead
* High availability requirements

## Tradeoffs

* Expensive at scale
* Less low-level control

---

# ü•à 2Ô∏è‚É£ Weaviate

![Image](https://docs.weaviate.io/img/docs/replication-architecture/replication-main-quorum.png)

![Image](https://weaviate.io/assets/images/hybrid-search-2242a956a4d86dba7f67814dc2077800.png)

![Image](https://docs.weaviate.io/img/docs/replication-architecture/replication-factor.png)

![Image](https://weaviate.io/assets/images/hero-7992ecb4f1fac8cd1a421d4986341437.png)

## Why Choose It?

* Hybrid search built-in (BM25 + vectors)
* Graph-style schema
* Good for semantic knowledge graphs

## Best For

* Retrieval + structured relations
* Knowledge graph systems
* Hybrid search systems

## Tradeoffs

* More complex schema
* Slightly heavier footprint

---

# ü•â 3Ô∏è‚É£ Milvus

![Image](https://milvus.io/docs/v2.6.x/assets/milvus_architecture_2_6.png)

![Image](https://milvus.io/docs/v2.6.x/assets/IVF-FLAT-workflow.png)

![Image](https://milvus.io/docs/v2.5.x/assets/distributed_architecture.jpg)

![Image](https://assets.zilliz.com/milvus_operator_deployment_architecture_3ff8570480.png)

## Why Choose It?

* Open source
* Highly scalable
* Supports IVF, HNSW, PQ
* Designed for billion-scale datasets

## Best For

* Large datasets
* Self-hosted infra
* Research or infra-heavy teams

## Tradeoffs

* Operational complexity
* Requires infra knowledge

---

# üß† 4Ô∏è‚É£ Qdrant

![Image](https://qdrant.tech/courses/day2/hnsw-layers.png)

![Image](https://qdrant.tech/articles_data/vector-search-filtering/pre-filtering.png)

![Image](https://miro.medium.com/1%2A9o1eY4QYEeSzDdc7KTwLxA.png)

![Image](https://qdrant.tech/docs/concurrent-operations-replicas.png)

## Why Choose It?

* Strong filtering engine
* Rust-based (efficient)
* Simple distributed setup
* Fast growing ecosystem

## Best For

* Hybrid metadata + vector filtering
* Mid-scale production systems

## Tradeoffs

* Not as battle-tested as Pinecone at massive scale

---

# üß™ 5Ô∏è‚É£ Chroma

![Image](https://miro.medium.com/0%2AjmcTxk-vcU9d1Rla.png)

![Image](https://how.wtf/images/L9tqZZ.webp)

![Image](https://www.trychroma.com/blog_0.4.0/old_vs_new.png)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/0%2AHbgO-VYi6ajDd1rc.png)

## Why Choose It?

* Extremely simple
* Python-friendly
* Great for prototyping

## Best For

* Local RAG experiments
* Small projects
* Hackathons

## Tradeoffs

* Not built for large-scale distributed production

---

# üìä Quick Decision Table

| Use Case               | Best Choice |
| ---------------------- | ----------- |
| Quick prototype        | Chroma      |
| Production SaaS RAG    | Pinecone    |
| Heavy filtering        | Qdrant      |
| Knowledge graph hybrid | Weaviate    |
| Billion-scale infra    | Milvus      |

---

# üß† Now Let‚Äôs Do Follow-Up Q&A

---

## Q1: What should I use for 1M‚Äì10M vectors?

If:

* You want managed ‚Üí Pinecone
* You want self-hosted ‚Üí Qdrant
* You want infra control ‚Üí Milvus

---

## Q2: What if I need frequent updates?

Graph-based HNSW handles inserts well.
IVF requires periodic re-clustering.

Better:

* Pinecone
* Qdrant

---

## Q3: What about cost efficiency?

Cheapest path:

* Self-host Qdrant
* Use cloud VMs
* Use disk-based HNSW

Most expensive:

* Fully managed Pinecone at high scale

---

## Q4: What about hybrid search (BM25 + vector)?

Best:

* Weaviate (native)
* Or use Elasticsearch + vector plugin

---

## Q5: What about extreme scale (100M+ vectors)?

You need:

* Sharding
* PQ compression
* Tiered storage

Best:

* Milvus
* Pinecone enterprise tier

---

## Q6: What do FAANG-like systems build?

They often:

* Use Faiss internally
* Custom sharding layer
* Custom index tuning
* Separate reranker service

Because:

* They optimize for hardware locality

---

# üéØ If I Were You

Given your history of working with:

* RAG
* Feature importance
* ML pipelines

I would suggest:

### Phase 1 (Learning / Building)

‚Üí Qdrant

### Phase 2 (Scaling Startup)

‚Üí Pinecone

### Phase 3 (Infra Mastery)

‚Üí Milvus

---




