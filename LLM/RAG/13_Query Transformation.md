# ðŸš€ RAG Mastery â€“ Day 13

## ðŸ”Ž Query Transformation & Multi-Hop Retrieval

Today we move from **basic retrieval** â†’ **intelligent retrieval**.

Up to now, your RAG pipeline probably looks like:

```
User Query â†’ Embed â†’ Retrieve â†’ Rerank â†’ Generate
```

But hereâ€™s the harsh truth:

> Most RAG failures are NOT because of bad embeddings.
> They happen because the query itself is bad.

Day 13 is about fixing that.

---

# ðŸ§  Why Query Transformation Matters

Real user queries are:

* Ambiguous
* Underspecified
* Conversational
* Multi-hop
* Noisy

Example:

> â€œWhy did revenue drop after the compliance update?â€

This actually requires:

1. Retrieve info about revenue
2. Retrieve info about compliance update
3. Retrieve timeline correlation
4. Combine reasoning

A single vector search wonâ€™t cut it.

---

# ðŸ” 1ï¸âƒ£ Query Rewriting

### Problem

User queries are often poorly phrased for retrieval.

Example:

> â€œWhat did they change in the security thing last quarter?â€

Embedding this directly â†’ poor recall.

---

### Solution: LLM-based Rewrite

Rewrite query into a retrieval-optimized form.

**Original:**

> What did they change in the security thing last quarter?

**Rewritten:**

> What security policy updates were implemented in Q4 2025?

Much better semantic alignment.

---

### Architecture

```
User Query
    â†“
LLM Rewriter
    â†“
Optimized Query
    â†“
Retriever
```

---

### Code Example

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

llm = ChatOpenAI()

rewrite_prompt = PromptTemplate.from_template("""
Rewrite the query for optimal document retrieval.
Query: {query}
Optimized:
""")

def rewrite_query(query):
    return llm.invoke(rewrite_prompt.format(query=query)).content
```

---

# ðŸ” 2ï¸âƒ£ Multi-Query Retrieval (Improves Recall)

Instead of 1 embedding â†’ generate 3â€“5 variations.

Example:

User:

> â€œHow does rate limiting affect API latency?â€

Generate:

1. Impact of rate limiting on response time
2. API throttling and latency relationship
3. Performance implications of request limiting

Now retrieve for all â†’ merge results.

---

### Why This Works

Vector search recall improves dramatically because:

* Embedding space is imperfect
* Different phrasing lands in different regions

This is especially powerful in technical corpora.

---

### Implementation Concept

```
LLM â†’ Generate N queries
For each:
    retrieve top-k
Merge & deduplicate
```

---

# ðŸ§© 3ï¸âƒ£ Multi-Hop Retrieval

Now we go deeper.

Some questions require sequential retrieval.

Example:

> Which papers cited the work that introduced Transformers?

This requires:

1. Retrieve paper introducing Transformers
   â†’ Attention Is All You Need
2. Extract citation info
3. Retrieve citing papers

Thatâ€™s multi-hop reasoning.

---

### Strategy: Iterative RAG

```
Query â†’ Retrieve
     â†“
Extract intermediate entity
     â†“
New query
     â†“
Retrieve again
     â†“
Combine evidence
```

---

### Example (Enterprise Setting)

User:

> Which customers complained after the pricing policy change?

Hop 1:
Retrieve pricing policy change date.

Hop 2:
Retrieve complaints after that date.

Hop 3:
Join results.

This is basically building a retrieval agent.

---

# ðŸ§  4ï¸âƒ£ Self-Ask Pattern

LLM decomposes complex question into subquestions.

Example:

> Did revenue increase after we launched feature X in Europe?

Self-Ask:

1. When was feature X launched in Europe?
2. What was revenue before?
3. What was revenue after?
4. Compare.

This is structured reasoning before retrieval.

---

# ðŸ“Š 5ï¸âƒ£ Tradeoffs

| Method      | Pros                      | Cons                |
| ----------- | ------------------------- | ------------------- |
| Rewrite     | High precision            | Slight latency      |
| Multi-query | High recall               | More vector cost    |
| Multi-hop   | Handles complex reasoning | Pipeline complexity |
| Self-ask    | Transparent reasoning     | LLM tokens â†‘        |

In production, you combine them.

---

# ðŸ— Real-World Systems Using This

* Perplexity AI â†’ heavy query rewriting
* Google search â†’ multi-stage retrieval
* OpenAI â†’ query planning in deep research mode

Modern RAG â‰  single vector search.

---

# ðŸ§ª Practical Exercise (Critical)

Build this today:

1. Add query rewriting
2. Add multi-query generation (3 variations)
3. Merge retrieval results
4. Compare against baseline

Measure:

* Recall@k
* Answer faithfulness
* Latency impact

---

# ðŸ§  Deep Insight

Think of RAG as:

> Retrieval = Database
> LLM = Query Planner

The more complex the question â†’ the more it should behave like SQL planning.

---

# ðŸŽ¯ Tomorrow (Day 14)

We go into:

> ðŸ§¬ Hybrid Retrieval (BM25 + Dense + Sparse + Metadata Filters)

This is where most production RAG systems start looking serious.

---

If you want, tomorrow we can:

* Implement full multi-hop agent
* Or run a mini research-style RAG experiment

Your call.
