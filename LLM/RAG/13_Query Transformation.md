## ðŸ”Ž Query Transformation & Multi-Hop Retrieval

We move from **basic retrieval** â†’ **intelligent retrieval**.

Up to now, your RAG pipeline probably looks like:

```
User Query â†’ Embed â†’ Retrieve â†’ Rerank â†’ Generate
```

But hereâ€™s the harsh truth:

> Most RAG failures are NOT because of bad embeddings.
> They happen because the query itself is bad.


---

# ðŸ§  Why Query Transformation Matters

Real user queries are:

* Ambiguous
* Underspecified
* Conversational
* Multi-hop
* Noisy

Example:

> â€œWhy did revenue drop after the compliance update?â€

This actually requires:

1. Retrieve info about revenue
2. Retrieve info about compliance update
3. Retrieve timeline correlation
4. Combine reasoning

A single vector search wonâ€™t cut it.

When building retrieval systems (RAG, search engines, agents), the *surface form* of a query is often not sufficient to retrieve the right information. Query transformation reformulates the user input into something the system can actually reason over.

Letâ€™s go deeper.

---

# 1ï¸âƒ£ Multi-Hop Queries (Deep Explanation)

### ðŸ”¹ What is Multi-Hop?

A **multi-hop query** is a question that requires **chaining multiple pieces of information together** before producing the final answer.

Instead of:

> Single lookup â†’ answer

You need:

> Retrieve A â†’ Use A to retrieve B â†’ Combine â†’ Answer

This is essentially **compositional reasoning over multiple documents or facts**.

---

## ðŸ” Example 1

> "Who is the spouse of the CEO of Tesla?"

Step-by-step reasoning:

1. Identify CEO of Tesla â†’ Tesla
2. CEO = Elon Musk
3. Retrieve spouse of Elon Musk
4. Answer

Thatâ€™s **two hops**:

* Hop 1: Tesla â†’ CEO
* Hop 2: CEO â†’ Spouse

---

## ðŸ” Example 2 (Harder)

> "Which university did the author of The Hobbit attend?"

Steps:

1. Identify author of The Hobbit
2. Author = J. R. R. Tolkien
3. Retrieve Tolkienâ€™s university
4. Answer = University of Oxford

Again: multi-hop reasoning.

---

## ðŸ§  Why Multi-Hop Is Hard

### 1. Retrieval Challenge

Embedding similarity may retrieve:

* Docs about Tesla
* Docs about Elon Musk
* Docs about spouses

But not necessarily in the right order.

### 2. Context Explosion

Each hop expands search space.

### 3. Query Decomposition Required

You often need to transform:

> â€œWhich university did the author of The Hobbit attend?â€

Into:

* Subquery 1: Who wrote The Hobbit?
* Subquery 2: Where did Tolkien study?

---

## ðŸ— How Systems Handle Multi-Hop

### Approach 1: Query Decomposition

Break into smaller questions.

### Approach 2: Iterative Retrieval (Agent-style)

Retrieve â†’ Update query â†’ Retrieve again.

### Approach 3: Graph-Based Retrieval

Use knowledge graphs to traverse relations.

---

## ðŸŽ¯ Real-World Applications

* Legal reasoning
* Financial due diligence
* Medical diagnosis chains
* Research assistants
* Complex analytics queries

---

# 2ï¸âƒ£ Ambiguous Queries (Brief)

### Definition:

Query has multiple interpretations.

Example:

> â€œApple revenueâ€

Could mean:

* Apple Inc. revenue
* Apple (fruit) industry revenue

Query transformation may expand to:

> â€œApple Inc. annual revenue 2025â€

---

# 3ï¸âƒ£ Underspecified Queries (Brief)

### Definition:

Missing necessary constraints.

Example:

> â€œBest laptopâ€

Missing:

* Budget?
* Gaming?
* Coding?
* Lightweight?

Transformation might add inferred context:

> â€œBest lightweight laptop under $1000 for programmingâ€

---

# 4ï¸âƒ£ Conversational Queries (Brief)

Context-dependent follow-ups.

Example:

User:

> Who is the CEO of Tesla?

System:

> Elon Musk.

User:

> Where did he study?

"He" must resolve to Elon Musk.

This requires **coreference resolution + context memory**.

---

# 5ï¸âƒ£ Noisy Queries (Brief)

Contain:

* Typos
* Slang
* Speech-to-text errors
* Broken grammar

Example:

> â€œwhats da ceo tesla study?â€

Needs normalization before retrieval.

---

# ðŸ”¥ Big Picture

Multi-hop queries are fundamentally different because:

| Type           | Main Challenge                      |
| -------------- | ----------------------------------- |
| Ambiguous      | Disambiguation                      |
| Underspecified | Add constraints                     |
| Conversational | Context tracking                    |
| Noisy          | Cleaning                            |
| **Multi-hop**  | **Reasoning across multiple facts** |

Multi-hop is the most structurally complex because it requires **composition of knowledge**, not just better retrieval.

---





---

# ðŸ” 1ï¸âƒ£ Query Rewriting

### Problem

User queries are often poorly phrased for retrieval.

Example:

> â€œWhat did they change in the security thing last quarter?â€

Embedding this directly â†’ poor recall.

---

### Solution: LLM-based Rewrite

Rewrite query into a retrieval-optimized form.

**Original:**

> What did they change in the security thing last quarter?

**Rewritten:**

> What security policy updates were implemented in Q4 2025?

Much better semantic alignment.

---

### Architecture

```
User Query
    â†“
LLM Rewriter
    â†“
Optimized Query
    â†“
Retriever
```

---

### Code Example

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

llm = ChatOpenAI()

rewrite_prompt = PromptTemplate.from_template("""
Rewrite the query for optimal document retrieval.
Query: {query}
Optimized:
""")

def rewrite_query(query):
    return llm.invoke(rewrite_prompt.format(query=query)).content
```

---

# ðŸ” 2ï¸âƒ£ Multi-Query Retrieval (Improves Recall)

Instead of 1 embedding â†’ generate 3â€“5 variations.

Example:

User:

> â€œHow does rate limiting affect API latency?â€

Generate:

1. Impact of rate limiting on response time
2. API throttling and latency relationship
3. Performance implications of request limiting

Now retrieve for all â†’ merge results.

---

### Why This Works

Vector search recall improves dramatically because:

* Embedding space is imperfect
* Different phrasing lands in different regions

This is especially powerful in technical corpora.

---

### Implementation Concept

```
LLM â†’ Generate N queries
For each:
    retrieve top-k
Merge & deduplicate
```

---

# ðŸ§© 3ï¸âƒ£ Multi-Hop Retrieval

Now we go deeper.

Some questions require sequential retrieval.

Example:

> Which papers cited the work that introduced Transformers?

This requires:

1. Retrieve paper introducing Transformers
   â†’ Attention Is All You Need
2. Extract citation info
3. Retrieve citing papers

Thatâ€™s multi-hop reasoning.

---

### Strategy: Iterative RAG

```
Query â†’ Retrieve
     â†“
Extract intermediate entity
     â†“
New query
     â†“
Retrieve again
     â†“
Combine evidence
```

---

### Example (Enterprise Setting)

User:

> Which customers complained after the pricing policy change?

Hop 1:
Retrieve pricing policy change date.

Hop 2:
Retrieve complaints after that date.

Hop 3:
Join results.

This is basically building a retrieval agent.

---

# ðŸ§  4ï¸âƒ£ Self-Ask Pattern

LLM decomposes complex question into subquestions.

Example:

> Did revenue increase after we launched feature X in Europe?

Self-Ask:

1. When was feature X launched in Europe?
2. What was revenue before?
3. What was revenue after?
4. Compare.

This is structured reasoning before retrieval.

---

# ðŸ“Š 5ï¸âƒ£ Tradeoffs

| Method      | Pros                      | Cons                |
| ----------- | ------------------------- | ------------------- |
| Rewrite     | High precision            | Slight latency      |
| Multi-query | High recall               | More vector cost    |
| Multi-hop   | Handles complex reasoning | Pipeline complexity |
| Self-ask    | Transparent reasoning     | LLM tokens â†‘        |

In production, you combine them.

---

# ðŸ— Real-World Systems Using This

* Perplexity AI â†’ heavy query rewriting
* Google search â†’ multi-stage retrieval
* OpenAI â†’ query planning in deep research mode

Modern RAG â‰  single vector search.

---

# ðŸ§ª Practical Exercise (Critical)

Build this today:

1. Add query rewriting
2. Add multi-query generation (3 variations)
3. Merge retrieval results
4. Compare against baseline

Measure:

* Recall@k
* Answer faithfulness
* Latency impact

---

# ðŸ§  Deep Insight

Think of RAG as:

> Retrieval = Database
> LLM = Query Planner

The more complex the question â†’ the more it should behave like SQL planning.

---

# ðŸŽ¯ Tomorrow (Day 14)

We go into:

> ðŸ§¬ Hybrid Retrieval (BM25 + Dense + Sparse + Metadata Filters)

This is where most production RAG systems start looking serious.

---

If you want, tomorrow we can:

* Implement full multi-hop agent
* Or run a mini research-style RAG experiment

Your call.
