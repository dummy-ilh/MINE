Excellent. Today we move into **enterprise-grade RAG design** ‚Äî where data is not just PDFs, but databases, graphs, and structured systems.

---

# üìò Day 18 ‚Äì Structured + Unstructured RAG

This is about combining:

* **Structured data** ‚Üí SQL databases, tables
* **Semi-structured data** ‚Üí JSON, logs
* **Unstructured data** ‚Üí documents, PDFs, emails
* **Relational knowledge** ‚Üí knowledge graphs

Modern production systems *must* handle all four.

---

# 1Ô∏è‚É£ Why Pure Vector RAG Is Not Enough

Vector RAG is good for:

* Explanations
* Policies
* Reports
* Narrative content

But it fails at:

* Exact counts
* Aggregations
* Filtering
* Joins
* Temporal constraints

Example:

> ‚ÄúHow many customers purchased product X in Q3 2024?‚Äù

Embeddings cannot compute aggregates.
You need SQL.

---

# 2Ô∏è‚É£ SQL + Vector Hybrid RAG

This is sometimes called:

> Retrieval-Augmented Generation with Tool Use

---

## Architecture

```
User Question
      ‚Üì
Router (LLM classifier)
      ‚Üì
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ Structured    ‚îÇ Unstructured   ‚îÇ
 ‚îÇ (SQL Query)   ‚îÇ (Vector RAG)   ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚Üì
Results merged
      ‚Üì
LLM synthesis
```

---

## Example

Question:

> ‚ÄúWhat was the revenue of Apple in 2023 and how did analysts describe it?‚Äù

Structured part:

* Revenue ‚Üí SQL query

Unstructured part:

* Analyst commentary ‚Üí vector retrieval

We anchor the company entity:

* **Apple Inc.**

![Image](https://upload.wikimedia.org/wikipedia/commons/5/5a/Aerial_view_of_Apple_Park_dllu.jpg)

![Image](https://www.researchgate.net/publication/279757421/figure/fig1/AS%3A327870753853440%401455181887320/The-Financial-database-schema.png)

![Image](https://docs-download.pingcap.com/media/images/docs/vector-search/embedding-search.png)

![Image](https://docs.cloud.google.com/static/vertex-ai/docs/vector-search/images/infrastructure-architecture.png)

---

## Pipeline Breakdown

### Step 1: Query Classification

Prompt:

```
Is this:
A) SQL
B) Vector
C) Both
```

### Step 2: SQL Generation

```python
sql = llm.generate_sql(question, schema)
result = execute(sql)
```

### Step 3: Vector Retrieval

```python
docs = retriever.search(question)
```

### Step 4: Grounded Synthesis

```
Answer using:
- Structured results
- Retrieved documents
```

---

## Critical Production Safeguards

1. SQL sandboxing
2. Read-only database role
3. Query validation
4. Schema-aware prompting
5. Injection filtering

---

# 3Ô∏è‚É£ Knowledge Graph RAG

When data is highly relational.

Nodes = Entities
Edges = Relations

Perfect for:

* Biomedical
* Legal
* Supply chain
* Fraud detection

---

## Example

Suppose we query:

> ‚ÄúWhich drugs target proteins associated with Alzheimer‚Äôs disease?‚Äù

Entities:

* Drug
* Protein
* Disease

![Image](https://www.scai.fraunhofer.de/en/projects/Biomedical-Knowledge-Graphs/jcr%3Acontent/fixedContent/pressArticleParsys/textwithinlinedimage/imageComponent2/image.img.jpg/1693989429907/Subset-of-PHAGO-Graph.jpg)

![Image](https://snap.stanford.edu/decagon/polypharmacy-graph.png)

![Image](https://www.yfiles.com/assets/images/landing-pages/neo4j-database-visualization.c1c877444b.png)

![Image](https://s3.amazonaws.com/guides.neo4j.com/img/style_actedin_relationship.png)

Graph traversal:

```
Disease ‚Üí associated_with ‚Üí Protein
Protein ‚Üí targeted_by ‚Üí Drug
```

Then fetch textual evidence for explanation.

---

## Graph RAG Architecture

```
Question
   ‚Üì
Entity linking
   ‚Üì
Graph traversal (k hops)
   ‚Üì
Retrieve supporting docs
   ‚Üì
LLM explanation
```

---

## Why Graphs Matter

Vector similarity:

* Finds similar text

Graph traversal:

* Follows explicit relationships

Graphs provide:

* Path explainability
* Deterministic multi-hop
* Reduced hallucination

---

# 4Ô∏è‚É£ Tabular Grounding

This is under-discussed but extremely important.

LLMs hallucinate numbers easily.

Tabular grounding forces:

* Numeric precision
* Aggregation correctness
* Referential integrity

---

## Example Question

> ‚ÄúWhat is the average salary in the engineering department?‚Äù

Requires:

```
SELECT AVG(salary)
FROM employees
WHERE department = 'engineering'
```

---

## Grounding Strategy

1. Convert table ‚Üí DataFrame
2. Compute via SQL or Pandas
3. Pass numeric result to LLM
4. Ask LLM only to explain

Never let LLM compute aggregates.

---

## Advanced Tabular RAG

Instead of retrieving entire table:

* Use schema summarization
* Column embedding
* Query-to-column matching
* Constraint extraction

---

# 5Ô∏è‚É£ Hybrid Structured + Graph + Vector (Enterprise Pattern)

Modern enterprise assistants combine:

| Component    | Used For             |
| ------------ | -------------------- |
| SQL          | Metrics, aggregation |
| Graph DB     | Relationships        |
| Vector Store | Reports, policies    |
| LLM          | Synthesis            |

This becomes:

> Tool-augmented reasoning system

---

# 6Ô∏è‚É£ Query Routing Strategy (Critical Interview Topic)

Three common approaches:

### 1. Rule-Based Router

* Regex / heuristics
* Fast but brittle

### 2. LLM Classifier

* Flexible
* Costly

### 3. Learned Router

* Fine-tuned classifier
* Best long-term solution

---

# 7Ô∏è‚É£ Failure Modes

1. SQL injection
2. Schema hallucination
3. Graph entity mislinking
4. Numeric drift
5. Tool misuse
6. Over-fetching large tables

---

# 8Ô∏è‚É£ Evaluation Metrics

For structured + unstructured RAG:

* SQL accuracy
* Execution correctness
* Aggregation correctness
* Numeric exact match
* Retrieval recall
* End-to-end factual consistency

---
Perfect. Let‚Äôs answer **all Day 18 interview questions** at a level suitable for strong ML / LLM systems interviews.

We‚Äôll go tier by tier.

---

# üü¢ Basic-Level Answers

---

### 1Ô∏è‚É£ Why is vector RAG insufficient for enterprise data?

Vector RAG relies on **semantic similarity**, not logical operations.

It fails when queries require:

* Exact counts
* Aggregations (SUM, AVG, GROUP BY)
* Filtering with conditions
* Joins across tables
* Strict numeric precision
* Temporal constraints

Example:

> ‚ÄúHow many employees joined in Q2 2023?‚Äù

Embeddings cannot compute counts ‚Äî they retrieve similar text.

Enterprise systems require:

* Deterministic correctness
* Auditable numeric answers
* Compliance guarantees

Thus: vector RAG must be augmented with SQL / structured systems.

---

### 2Ô∏è‚É£ What is SQL-augmented RAG?

It is a hybrid system where:

* LLM generates SQL from natural language
* SQL is executed against a database
* Results are grounded
* LLM synthesizes explanation

Pipeline:

```
Question ‚Üí LLM ‚Üí SQL
          ‚Üí DB execution
          ‚Üí Results
          ‚Üí LLM explanation
```

Important:
LLM does **not compute results**, it only generates queries.

---

### 3Ô∏è‚É£ How do you prevent hallucination in table queries?

Key strategies:

1. Schema grounding

   * Provide schema context only
   * No imaginary columns

2. SQL execution verification

   * If query fails ‚Üí regenerate

3. Strict output grounding

   * LLM receives actual results

4. Disable free-form numeric reasoning

5. Read-only DB role

6. Result size constraints

---

# üü° Intermediate-Level Answers

---

### 4Ô∏è‚É£ How would you design a router for structured vs unstructured queries?

Three-layer strategy:

#### Step 1: Intent classification

Use LLM or classifier:

* Aggregation?
* Count?
* Filter?
* Relationship?
* Explanation?

#### Step 2: Tool selection

| Pattern               | Tool   |
| --------------------- | ------ |
| Numeric aggregation   | SQL    |
| Relationship chain    | Graph  |
| Narrative explanation | Vector |
| Mixed                 | Hybrid |

#### Step 3: Confidence fallback

If SQL fails ‚Üí fallback to vector explanation.

Production-grade routers often use:

* Fine-tuned classifier (cheap)
* LLM only for ambiguous cases

---

### 5Ô∏è‚É£ What safeguards are required for SQL generation?

Critical:

1. Read-only DB credentials
2. Query validator (AST parser)
3. No DDL/DML allowed
4. Timeout enforcement
5. Max row limit
6. Injection filtering
7. Schema-aware prompt

Never allow:

```
DROP TABLE
DELETE
UPDATE
```

---

### 6Ô∏è‚É£ How do knowledge graphs reduce hallucination?

Because:

* Relations are explicit
* Traversal is deterministic
* Paths are verifiable
* Entities are canonical

Example graph structure:

Drug ‚Üí targets ‚Üí Protein ‚Üí associated_with ‚Üí Disease

Instead of guessing relationship textually, system traverses edges.

This reduces:

* Relationship hallucination
* Entity confusion
* Multi-hop drift

---

### 7Ô∏è‚É£ Graph traversal vs Multi-hop retrieval?

| Graph Traversal         | Multi-Hop Retrieval       |
| ----------------------- | ------------------------- |
| Structured              | Unstructured              |
| Deterministic           | Probabilistic             |
| Explicit relations      | Inferred relations        |
| Fast for entity queries | Flexible for text queries |

Graph works best when data is relational.
Multi-hop vector works better for narrative reasoning.

Hybrid is strongest.

---

# üî¥ Senior-Level Answers

---

### 8Ô∏è‚É£ Design hybrid SQL + vector + graph RAG for financial enterprise

Architecture:

```
User Query
      ‚Üì
Intent Classifier
      ‚Üì
Tool Selector
      ‚Üì
Parallel Execution:
   - SQL (metrics)
   - Graph (ownership, hierarchy)
   - Vector (reports, filings)
      ‚Üì
Evidence Merger
      ‚Üì
LLM Synthesis
      ‚Üì
Verifier
```

Components:

* PostgreSQL / Snowflake (structured)
* Neo4j (graph)
* FAISS / Pinecone (vector)
* LLM planner
* Result cache

Example:

Query:

> ‚ÄúWhat was the revenue of the subsidiaries of Company X and how did analysts describe their performance?‚Äù

Requires:

* Graph traversal (subsidiaries)
* SQL aggregation (revenue)
* Vector retrieval (analyst reports)

---

### 9Ô∏è‚É£ How to optimize latency?

Key techniques:

1. Parallel tool execution
2. Caching:

   * Embeddings
   * Frequent SQL results
3. Limit hops
4. Async execution
5. Early stopping if sufficient confidence
6. Smaller rerankers
7. Precomputed graph neighborhoods

Target:
<2 seconds total pipeline

---

### üîü How evaluate tabular reasoning correctness?

Metrics:

1. SQL exact match
2. Execution accuracy
3. Aggregation correctness
4. Numeric exact match
5. Logical form accuracy

Also:

* Compare against ground truth SQL
* Test edge cases (null values, grouping)

---

### 1Ô∏è‚É£1Ô∏è‚É£ Detect and mitigate schema hallucination?

Detection:

* SQL parser error
* Unknown column detection
* Column embedding similarity check

Mitigation:

1. Provide only schema summary
2. Force structured output JSON
3. Penalize nonexistent fields
4. Few-shot examples with schema constraints

Best practice:
Never allow open-schema generation.

---

### 1Ô∏è‚É£2Ô∏è‚É£ When precompute graph embeddings?

Use graph embeddings when:

* Graph is large (>10M nodes)
* Need semantic similarity over graph nodes
* Want hybrid graph + vector search

Methods:

* Node2Vec
* GraphSAGE
* GNN embeddings

Use cases:

* Fraud detection
* Recommendation
* Biomedical discovery

---

# üß† Deeper Insight

Structured + Unstructured RAG is merging:

* Symbolic systems (SQL, graphs)
* Neural systems (embeddings)
* Generative models (LLMs)

It is effectively:

> Neuro-symbolic AI in production

---

# üî• Master-Level Insight

Vector RAG answers:
"What is said?"

SQL answers:
"What is computed?"

Graph answers:
"What is connected?"

LLM synthesizes:
"What does it mean?"

The future is **orchestration**, not just retrieval.

---



