Good. Now we move from algorithms to **real deployment decisions**.

Up to now:

* You understand embeddings.
* You understand ANN (HNSW, IVF).
* You built FAISS locally.

Today we answer:

> When do you use FAISS? When do you use Pinecone? When does it actually matter?

---

# ğŸ“˜ Day 11 â€” Vector Stores in Practice (Architecture & Tradeoffs)

---

# 1ï¸âƒ£ First Principle

A vector store must handle:

1. Fast similarity search
2. Persistent storage
3. Metadata filtering
4. Scaling
5. Updates (inserts/deletes)

Not all tools handle these equally.

---

# 2ï¸âƒ£ The Main Options

Weâ€™ll group them into categories:

---

## ğŸ”¹ FAISS (Library, Not a Database)

What it is:

* C++/Python library
* Extremely fast
* Used internally by many systems

Pros:

* Full control
* High performance
* Flexible index types

Cons:

* No built-in persistence (you manage it)
* No metadata filtering (you build it)
* No distributed scaling out of the box

Best for:

* Research
* Custom infra
* Small/medium-scale self-hosted systems

Think of FAISS as an engine, not a car.

---

## ğŸ”¹ Pinecone (Managed Vector DB)

What it is:

* Fully managed vector database
* API-based

Pros:

* Automatic scaling
* Built-in metadata filtering
* Persistent storage
* Easy deployment

Cons:

* Cost
* Vendor lock-in
* Less algorithmic control

Best for:

* Startups
* Production apps needing quick deployment

---

## ğŸ”¹ Weaviate / Milvus (Open-Source Vector Databases)

What they are:

* Full databases
* REST/gRPC APIs
* Distributed

Pros:

* Metadata + vector search integrated
* Horizontal scaling
* On-prem deployment

Cons:

* Operational overhead
* DevOps complexity

Best for:

* Enterprise
* Privacy-sensitive workloads
* Large-scale systems

---

## ğŸ”¹ Chroma (Developer-Friendly)

What it is:

* Lightweight vector store
* Good for prototypes

Pros:

* Simple
* Easy to integrate

Cons:

* Not ideal for large-scale production
* Limited advanced scaling features

Best for:

* Prototyping
* Local experiments

---

# 3ï¸âƒ£ Real Comparison Table

| Feature            | FAISS  | Pinecone | Weaviate/Milvus | Chroma  |
| ------------------ | ------ | -------- | --------------- | ------- |
| Persistence        | Manual | Yes      | Yes             | Yes     |
| Metadata filtering | Manual | Yes      | Yes             | Basic   |
| Scaling            | Manual | Auto     | Distributed     | Limited |
| Control over index | Full   | Limited  | Moderate        | Limited |
| DevOps effort      | High   | Low      | Medium-High     | Low     |

---

# 4ï¸âƒ£ The Hidden Question: Scale

The choice depends mostly on:

* Number of vectors
* Query per second (QPS)
* Update frequency
* Latency SLA

Letâ€™s break it down.

---

## ğŸ”¹ Small Scale (< 100k vectors)

* FAISS exact search works
* Low infra complexity
* No need for distributed DB

---

## ğŸ”¹ Medium Scale (100kâ€“5M vectors)

* HNSW required
* Persistent storage required
* Metadata filtering required

Now you need:

* Pinecone
* Weaviate
* Milvus
* Or heavy FAISS customization

---

## ğŸ”¹ Large Scale (5M+ vectors)

Now problems arise:

* Memory pressure
* Sharding
* Load balancing
* Index rebuild times

You must think about:

* Horizontal scaling
* Partitioning
* Index replication

At this stage:
Managed DBs or distributed vector DBs become practical.

---

# 5ï¸âƒ£ Sharding Strategy (Often Ignored)

You can shard by:

* Namespace (HR vs Legal)
* Time (2023 vs 2024)
* Geography
* Access level

Why?

It:

* Improves recall
* Reduces search space
* Improves latency
* Simplifies access control

Sharding is often more impactful than switching DB vendors.

---

# 6ï¸âƒ£ Update & Re-Indexing Strategy

Production systems must handle:

* New documents daily
* Document edits
* Document deletions

Key questions:

* Does index support deletes efficiently?
* Does update require full rebuild?
* Is downtime acceptable?

HNSW:

* Supports incremental inserts well
* Deletes more complex

IVF:

* Requires careful rebalancing

These details matter at scale.

---

# 7ï¸âƒ£ Observability (What Most Teams Forget)

A production vector system must log:

* Retrieved chunk IDs
* Similarity scores
* Latency
* Missed retrievals
* ANN recall degradation

Without observability:
You cannot debug retrieval failures.

---

# 8ï¸âƒ£ When Switching DBs Wonâ€™t Fix Your Problem

If your RAG fails:

90% of the time itâ€™s:

* Bad chunking
* Weak embeddings
* Poor retrieval strategy

Not:

* Wrong vector database

Infrastructure is rarely the root cause.

---

# 9ï¸âƒ£ Enterprise Architecture Example

A robust enterprise setup might look like:

```
Ingestion Service
   â†“
Embedding Service
   â†“
Vector DB (Milvus)
   â†“
Metadata DB (Postgres)
   â†“
Retriever Service
   â†“
LLM Service
```

Notice:
Vector search is just one component.

---

# ğŸ”Ÿ Interview-Level Answer

If asked:

> â€œHow do you choose a vector store for production?â€

Strong answer:

> â€œI base the decision on dataset size, QPS requirements, update frequency, and operational constraints. For small-scale systems, FAISS is sufficient. For managed scalability, Pinecone works well. For enterprise on-prem deployments, distributed systems like Milvus or Weaviate are more appropriate.â€

That shows architectural maturity.

---

# ğŸ§  Mental Model

Embeddings define space.
ANN navigates space.
Vector DB operationalizes space.

But:

RAG performance â‰  database choice.
Itâ€™s a system-level design problem.

---

## ğŸ“… Day 12 Preview â€” Retrieval Evaluation

Tomorrow we tackle one of the hardest problems:

* How do you measure RAG quality?
* Precision@k
* Recall@k
* MRR
* Human eval pitfalls
* Why â€œit sounds goodâ€ is meaningless

Say **â€œDay 12â€** when ready.
