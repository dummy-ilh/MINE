# ğŸ“˜ Day 13 â€“ RAG with LangChain & LlamaIndex

**Theme:** Abstractions â€” what they give you, what they hide, and when to avoid them.

---

# ğŸ§  Why This Matters

As you're going deep into RAG mastery, this is where you shift from:

> â€œHow do I make RAG work?â€
> to
> â€œHow do I make RAG scalable, debuggable, and production-grade?â€

Frameworks like:

* LangChain
* LlamaIndex

exist to abstract away complexity.

But abstraction is a double-edged sword.

---

# 1ï¸âƒ£ What Abstractions Help With

## A. Pipeline Orchestration

Instead of manually writing:

* Embed
* Store
* Retrieve
* Format prompt
* Call LLM
* Parse output

You get high-level constructs like:

* Chains
* Agents
* Retrievers
* Query Engines

### Example Conceptual Flow (LangChain)

```
User Query
   â†“
Retriever
   â†“
PromptTemplate
   â†“
LLM
   â†“
OutputParser
```

Instead of wiring all that manually.

---

## B. Unified Interfaces

They standardize:

* Vector stores (Pinecone, FAISS, Weaviate, Chroma)
* LLM providers (OpenAI, Anthropic, local models)
* Embedding models
* Tools

You swap backends without rewriting everything.

Thatâ€™s architectural leverage.

---

## C. Production Utilities

They give you:

* Memory modules
* Caching
* Streaming
* Async support
* Retries
* Callback hooks
* Basic tracing

Without writing infra glue code.

---

## D. Index Abstractions (LlamaIndex)

LlamaIndex shines in:

* Document ingestion
* Chunking pipelines
* Hierarchical indices
* Composable retrieval
* Structured query engines

You donâ€™t just retrieve chunks â€” you retrieve structured knowledge.

---

# 2ï¸âƒ£ What Abstractions Hide (The Dangerous Part)

This is where Week 3 engineers separate from Week 1 enthusiasts.

---

## âš ï¸ Hidden #1: What Actually Gets Retrieved

When you call:

```python
retriever.invoke(query)
```

Do you know:

* Which embedding model?
* What chunk size?
* What overlap?
* What similarity metric?
* What top_k?
* Is MMR enabled?
* Is metadata filtering applied?

Most people donâ€™t.

They debug hallucinations blindly.

---

## âš ï¸ Hidden #2: Prompt Construction

Frameworks auto-build prompts like:

```
Use the following context to answer:
{context}
Question: {query}
```

But:

* How many tokens?
* Is truncation happening?
* Are sources preserved?
* Is system prompt overridden?

You lose fine-grained control.

---

## âš ï¸ Hidden #3: Latency Explosion

High-level chain = multiple hidden LLM calls.

Example:

* Query rewriting
* Multi-retrieval
* Answer generation
* Source formatting

You thought it was 1 call.

It was 4.

Latency + cost spike silently.

---

## âš ï¸ Hidden #4: Retrieval Scoring Logic

Frameworks often:

* Default to cosine similarity
* Donâ€™t expose raw scores clearly
* Normalize differently

You canâ€™t easily debug recall issues.

---

## âš ï¸ Hidden #5: Token Budget Management

They may:

* Stuff too many chunks
* Truncate mid-sentence
* Drop lowest-ranked docs silently

Production systems must explicitly manage token budgets.

---

# 3ï¸âƒ£ When to Avoid Abstractions

You avoid them when:

---

## ğŸ§ª A. You're Experimenting with Retrieval Science

If youâ€™re tuning:

* Chunk size sensitivity
* Embedding comparisons
* Custom reranking
* Hybrid search (BM25 + dense)
* Dynamic top-k

Write your own retrieval layer.

Frameworks become friction.

---

## âš™ï¸ B. You Need Extreme Performance Control

High-scale production:

* Custom batching
* GPU embedding pipelines
* Async streaming retrieval
* Redis-level caching
* Custom telemetry

Abstractions can limit optimization.

---

## ğŸ§© C. You Need Custom Logic

Example:

* If query is legal â†’ use legal index
* If query is code â†’ use AST index
* If query is ambiguous â†’ rewrite first

Agents may overcomplicate this.

Sometimes plain Python logic is cleaner.

---

## ğŸš¨ D. Youâ€™re Debugging Hallucinations

When answers are wrong:

Step 1: Log retrieved chunks
Step 2: Inspect similarity scores
Step 3: Inspect prompt
Step 4: Inspect token truncation

High-level abstractions make this harder.

---

# 4ï¸âƒ£ Mental Model: The Abstraction Ladder

Think in layers:

```
Level 0 â€“ Raw API calls (manual)
Level 1 â€“ Custom Python pipeline
Level 2 â€“ Light framework usage
Level 3 â€“ Full chain abstraction
Level 4 â€“ Agent orchestration
```

Mastery = Being comfortable moving between layers.

Most people get stuck at Level 3.

Experts drop down to Level 1 when needed.

---

# 5ï¸âƒ£ LangChain vs LlamaIndex â€” Philosophical Difference

| Dimension  | LangChain           | LlamaIndex                     |
| ---------- | ------------------- | ------------------------------ |
| Philosophy | LLM orchestration   | Data-centric retrieval         |
| Strength   | Agents, tool chains | Indexing, structured retrieval |
| Debugging  | Medium difficulty   | Often easier for retrieval     |
| RAG Purity | General-purpose     | RAG-native                     |

If your focus is retrieval science â†’ LlamaIndex often cleaner.
If your focus is multi-tool AI systems â†’ LangChain stronger.

---

# 6ï¸âƒ£ Production Pattern I Recommend

For serious RAG systems:

### ğŸ”¹ Use framework for:

* Document ingestion
* Basic retriever abstraction
* Prompt templating
* Source tracking

### ğŸ”¹ Avoid framework for:

* Advanced retrieval tuning
* Re-ranking logic
* Multi-query orchestration
* Observability stack

Instead:

* Log everything
* Expose raw scores
* Track retrieval latency
* Track token counts

---

# 7ï¸âƒ£ Architecture Diagram (Conceptual)

```
User Query
   â†“
Query Preprocessor (optional rewrite)
   â†“
Retriever (controlled manually)
   â†“
Re-ranker (optional)
   â†“
Context Builder (token budget aware)
   â†“
LLM Call
   â†“
Answer + Structured Sources
   â†“
Logging Layer
```

Frameworks can implement this.

But you should understand each block independently.

---

# ğŸ§  Deep Insight (Week 3 Mindset)

Frameworks are:

* Accelerators early
* Constraints later

Theyâ€™re scaffolding.

Eventually, you must understand the steel beams underneath.

---

Excellent. Now we move from **framework user** â†’ **RAG engineer**.

Below is:

1. ğŸ”¹ Minimal Raw RAG (no LangChain / no LlamaIndex)
2. ğŸ”¹ Equivalent LangChain version
3. ğŸ”¹ Equivalent LlamaIndex version
4. ğŸ”¹ Line-by-line conceptual comparison
5. ğŸ”¹ Interview-level Q&A (architect depth)

---

# PART 1 â€” Minimal Raw RAG (No Framework)

This example assumes:

* OpenAI-style embeddings + chat model
* FAISS as vector store
* Manual chunking
* Manual prompt construction

---

## ğŸ§± Step 0 â€” Install

```bash
pip install openai faiss-cpu numpy tiktoken
```

---

## ğŸ§  Step 1 â€” Document Chunking (Manual)

```python
def chunk_text(text, chunk_size=500, overlap=50):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks
```

âš ï¸ YOU control:

* chunk size
* overlap
* boundaries

Frameworks hide this.

---

## ğŸ§® Step 2 â€” Embedding

```python
from openai import OpenAI
import numpy as np

client = OpenAI()

def embed_texts(texts):
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    return np.array([d.embedding for d in response.data])
```

You explicitly see:

* embedding model
* batching
* output format

---

## ğŸ—‚ Step 3 â€” FAISS Index

```python
import faiss

def build_faiss_index(vectors):
    dim = vectors.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(vectors)
    return index
```

âš ï¸ You choose:

* similarity metric (L2 vs cosine)
* index type (IVF? HNSW?)

Frameworks default this silently.

---

## ğŸ” Step 4 â€” Retrieval

```python
def retrieve(query, index, documents, k=5):
    q_embed = embed_texts([query])
    distances, indices = index.search(q_embed, k)
    return [documents[i] for i in indices[0]]
```

You control:

* k
* scoring
* post-processing

---

## ğŸ§¾ Step 5 â€” Prompt Construction (Manual)

```python
def build_prompt(context_chunks, query):
    context = "\n\n".join(context_chunks)
    return f"""
Use the following context to answer the question.

Context:
{context}

Question: {query}
Answer:
"""
```

You see exactly:

* token size
* formatting
* system instructions

---

## ğŸ¤– Step 6 â€” LLM Call

```python
def generate_answer(prompt):
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content
```

---

## ğŸ”„ Full Flow

```python
query = "What is retrieval augmented generation?"

retrieved_chunks = retrieve(query, index, documents, k=5)
prompt = build_prompt(retrieved_chunks, query)
answer = generate_answer(prompt)

print(answer)
```

---

# WHAT YOU SEE CLEARLY

You explicitly control:

* chunking
* embedding model
* similarity metric
* top_k
* prompt
* LLM model
* token flow

Nothing hidden.

This is architectural literacy.

---

# PART 2 â€” Same Thing in LangChain

Using LangChain

```python
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts(documents, embeddings)

retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

llm = ChatOpenAI(model="gpt-4o-mini")

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever
)

answer = qa_chain.run("What is retrieval augmented generation?")
```

---

## What Disappeared?

You no longer see:

* How chunking is done
* How prompt is built
* How context is formatted
* Exact token budget handling
* Raw similarity scores

Cleaner.

But opaque.

---

# PART 3 â€” Same Thing in LlamaIndex

Using LlamaIndex

```python
from llama_index.core import VectorStoreIndex
from llama_index.core import SimpleDirectoryReader

documents = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(documents)

query_engine = index.as_query_engine()
response = query_engine.query("What is retrieval augmented generation?")

print(response)
```

---

## Whatâ€™s Abstracted?

* Ingestion
* Chunking
* Embedding
* Index building
* Retrieval
* Prompting
* Answer synthesis

Extremely compact.

But very high-level.

---

# ğŸ” Line-by-Line Conceptual Comparison

| Stage             | Raw        | LangChain | LlamaIndex |
| ----------------- | ---------- | --------- | ---------- |
| Chunking          | Explicit   | Hidden    | Hidden     |
| Embedding         | Explicit   | Wrapped   | Wrapped    |
| Index type        | Explicit   | Defaulted | Defaulted  |
| Similarity metric | Explicit   | Defaulted | Defaulted  |
| Retrieval scores  | Accessible | Harder    | Medium     |
| Prompt format     | Manual     | Template  | Internal   |
| Token control     | Full       | Partial   | Limited    |
| Debug visibility  | High       | Medium    | Medium     |

---

# INTERVIEW SECTION â€” ARCHITECT LEVEL

Now we shift mindset.

These answers must sound like someone who has built systems.

---

## 1ï¸âƒ£ What Parts of RAG Are Data-Centric vs LLM-Centric?

### Data-Centric Components

* Document ingestion
* Chunking strategy
* Embedding model choice
* Vector index design
* Similarity metric
* Hybrid search logic
* Metadata filtering
* Re-ranking
* Passage selection

These affect **recall and precision**.

If retrieval fails, LLM cannot recover.

---

### LLM-Centric Components

* Prompt structure
* Instruction tuning
* Answer synthesis
* Source citation formatting
* Guardrails
* Hallucination control

These affect **answer quality and grounding**.

---

### Deep Insight

Retrieval quality ceiling defines system ceiling.

LLM cannot answer what retrieval does not surface.

---

## 2ï¸âƒ£ What Parts Require Tight Control?

In production systems:

### Must Control:

* Chunk size & overlap
* Embedding model versioning
* top_k dynamically
* Token budget allocation
* Retrieval latency
* Similarity threshold
* Re-ranking pipeline
* Logging of retrieved context

If you don't control these, scaling breaks.

---

## 3ï¸âƒ£ Where Is Latency Hiding?

Latency hides in:

### ğŸ”¹ Embedding calls (batching matters)

### ğŸ”¹ Vector search (especially remote DBs)

### ğŸ”¹ Re-ranking models (cross-encoders are slow)

### ğŸ”¹ Multiple LLM calls (rewrite + answer + summarize)

### ğŸ”¹ Large context windows (token processing cost)

Hidden latency source in frameworks:

* Chains triggering multiple internal calls.

Always measure:

* Retrieval latency
* LLM latency
* Total pipeline latency

---

## 4ï¸âƒ£ How Would You Debug Poor Recall?

Systematic process:

### Step 1 â€” Log retrieved chunks

Are relevant chunks present?

### Step 2 â€” Inspect similarity scores

Is relevant chunk ranked low?

### Step 3 â€” Evaluate embedding model

Try different embedding model.

### Step 4 â€” Adjust chunk size

Too small â†’ semantic fragmentation
Too large â†’ semantic dilution

### Step 5 â€” Try hybrid search

BM25 + dense

### Step 6 â€” Increase top_k

But manage token budget.

---

If relevant chunk is never retrieved:
Problem = data/index/embedding.

If retrieved but not used:
Problem = prompt/token truncation.

Architect thinking separates these.

---

## 5ï¸âƒ£ When Would You Remove a Framework?

You remove abstraction when:

* You need fine-grained retrieval control.
* Youâ€™re building high-scale systems.
* Youâ€™re experimenting with new retrieval science.
* You need strict latency optimization.
* You require advanced logging and observability.
* You want deterministic, testable pipelines.

Frameworks are great for:

* Rapid prototyping
* MVPs
* Standard RAG flows

But production RAG often evolves toward custom pipelines.

---

# ğŸ§  Architect-Level Summary

A builder asks:

> â€œHow do I make RAG work?â€

An architect asks:

> â€œWhere can this system fail under load, scale, or adversarial queries?â€

If you can:

* Diagnose recall vs synthesis failures
* Identify hidden latency
* Decide when abstraction helps or hurts
* Control token budgets explicitly

Then you're not just using RAG.

You're engineering it.

---




