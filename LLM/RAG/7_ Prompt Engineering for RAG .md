---

# ğŸ“˜ RAG Daily Tutorial

## **Day 6 â€” Prompt Engineering for RAG (Grounding Without Hallucination)**

---

# 1ï¸âƒ£ The Core Problem

You retrieved:

* The correct chunks
* High-quality sources
* Properly ranked context

But the LLM:

* Hallucinates
* Over-generalizes
* Ignores context
* Mixes prior knowledge with retrieved text

Why?

Because **the model is trained to be helpful, not faithful.**

Prompting determines whether it:

* Treats retrieved context as truth
* Or treats it as optional inspiration

---

# 2ï¸âƒ£ The Fundamental Rule of RAG Prompting

You must explicitly instruct the model to:

1. Use only the provided context
2. Admit uncertainty
3. Cite sources
4. Avoid external knowledge

If you donâ€™t say this clearly, it will improvise.

---

# 3ï¸âƒ£ Basic RAG Prompt Template (NaÃ¯ve)

```
Answer the question using the context below.

Context:
{retrieved_chunks}

Question:
{user_query}
```

This works â€” but itâ€™s fragile.

---

# 4ï¸âƒ£ Strong Grounded Prompt (Production-Level)

Better structure:

```
You are a system that answers questions strictly using the provided context.
If the answer is not contained in the context, say:
"I don't have enough information from the provided documents."

Use direct quotes where appropriate.
Cite the source section after each claim.

Context:
-----------------
{retrieved_chunks}
-----------------

Question:
{user_query}

Answer:
```

This reduces hallucination significantly.

---

# 5ï¸âƒ£ Why Citation Prompts Work

When you ask the model to:

> â€œCite the section for each claimâ€

You force:

* Context alignment
* Claim-by-claim grounding
* Reduced fabrication

It creates a **faithfulness constraint**.

---

# 6ï¸âƒ£ Context Formatting Matters

Bad:

```
Chunk1 text Chunk2 text Chunk3 text
```

Better:

```
[Document: Refund_Policy.md | Section: Late Refunds]

Refunds requested after 30 days...

---

[Document: Germany_Addendum.md | Section: Penalties]

For Germany customers...
```

Why?

* Structure improves attention
* Attribution improves precision
* LLM reasons per document

---

# 7ï¸âƒ£ Guarding Against Context Overload

LLMs suffer from:

* Recency bias
* Lost-in-the-middle problem

If you send:

* 10 long chunks
* Mixed relevance

The model may:

* Focus on the first
* Ignore the most relevant

Solutions:

* Re-rank aggressively
* Put most relevant chunk first
* Use summary compression

---

# 8ï¸âƒ£ Anti-Hallucination Techniques

## ğŸ”¹ Explicit refusal clause

â€œIf the context does not contain the answer, say so.â€

## ğŸ”¹ Extract-then-answer

Step 1:

* Extract relevant passages

Step 2:

* Generate final answer only from extracted passages

This reduces drift.

## ğŸ”¹ Structured output

Force JSON:

```
{
  "answer": "...",
  "sources": ["doc1", "doc2"],
  "confidence": "high/low"
}
```

Structured prompts reduce creative deviation.

---

# 9ï¸âƒ£ Common Prompting Mistakes

âŒ Allowing â€œgeneral knowledgeâ€
âŒ Mixing system instructions with context
âŒ Overly verbose system prompts
âŒ No refusal instruction
âŒ No citation requirement

Prompt design = alignment engineering.

---

# ğŸ”Ÿ Interview-Level Answer

If asked:

> â€œHow do you reduce hallucinations in RAG?â€

Strong answer:

> â€œI enforce strict context grounding through explicit refusal instructions, citation requirements, structured output formats, and careful context ordering to mitigate recency and attention biases.â€

That shows system-level thinking.

---

# ğŸ§  Mental Model

Retrieval gives facts.
Prompting enforces discipline.

Without disciplined prompting:

RAG = â€œLLM with vibesâ€.

---

