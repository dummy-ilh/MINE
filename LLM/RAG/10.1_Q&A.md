Excellent. These questions test whether youâ€™re thinking like a **system designer**, not just a prompt engineer.

Letâ€™s go one by one, deeply.

---

# 1ï¸âƒ£ If Recall@5 is low, which layer do you improve?

### What Recall@5 tells you

Recall@5 =

> â€œIs at least one relevant document appearing in the top 5 retrieved results?â€

If itâ€™s low, that means:

* The correct document is **not being retrieved early**
* Or not being retrieved at all

This is **not** a generation problem.
Itâ€™s a **retrieval-stage failure**.

---

## ğŸ” Layers to Improve

### A) Retrieval Layer (Primary Suspect)

Focus on:

* Hybrid retrieval (add BM25 if missing)
* Query rewriting
* Multi-query expansion
* Better embeddings
* Better chunking

If the relevant doc is not even in top 20 â†’ recall issue.

---

### B) Chunking Strategy

Sometimes the right information is split badly.

Example:

* Relevant sentence split across two chunks
* Chunk too large â†’ semantic dilution

Fix:

* Adjust chunk size (300â€“800 tokens sweet spot)
* Add overlap (10â€“20%)
* Try semantic chunking

---

### C) Metadata Filtering

Sometimes Recall@5 is low because:

* Retrieval space is too large
* No filtering applied

Fix:

* Apply structured filters first
* Narrow candidate pool

---

### ğŸ”¥ Core Insight

Low Recall@5 =

> You are failing before reranking and generation even start.

Fix recall first. Always.

---

# 2ï¸âƒ£ If Recall@20 is high but answers are wrong, what do you inspect?

Now this is interesting.

High Recall@20 means:

> The correct document is in the candidate pool.

But answer is wrong.

This implies:

* Either ranking failed
* Or generation failed

Letâ€™s separate.

---

## Step 1: Inspect Ranking

Is the correct doc in top 3â€“5 after reranking?

If not â†’ ranking failure.

Fix:

* Add cross-encoder reranker
* Improve scoring
* Use RRF
* Reduce chunk noise

---

## Step 2: Inspect Generation

If correct chunk is in top 5 but answer is wrong:

Thatâ€™s a **generation failure**.

Possibilities:

* Weak grounding prompt
* Context too large
* Lost-in-the-middle issue
* Model ignoring relevant chunk

Fix:

* Reorder chunks by relevance
* Reduce context size
* Add explicit grounding instruction:

  > â€œAnswer only using the provided context.â€

---

### ğŸ”¥ System Diagnosis Rule

Recall@20 high + wrong answer
= NOT a retrieval problem.

Itâ€™s ranking or generation.

---

# 3ï¸âƒ£ If answers are correct but hallucinated extra details appear, where is the issue?

This is a **faithfulness problem**.

Answer core is correct.
But model adds unsupported claims.

This is almost always:

### ğŸ§  Generation + Prompting issue

Root causes:

* Overly open-ended prompt
* Too much context
* LLM prior knowledge bleeding in
* No grounding enforcement

---

## Fixes

### A) Strong Grounding Instruction

Example:

> Only use information explicitly present in the provided context.
> If information is missing, say â€œNot found in context.â€

---

### B) Reduce Context

Large context increases hallucination.

Remember:
More tokens â‰  better answers.

---

### C) Add Attribution Requirement

Force citations:

> Cite the source chunk ID for each claim.

This reduces hallucination dramatically.

---

### D) Add Faithfulness Evaluator

Use LLM-as-judge to score unsupported claims.

---

### ğŸ”¥ Core Insight

Hallucination in RAG is rarely retrieval failure.

Itâ€™s usually:

> Generation freedom exceeding retrieval constraints.

---

# 4ï¸âƒ£ If latency explodes after adding reranker, what are your optimization levers?

This is where system engineering begins.

Cross-encoders are expensive because:

If K = 50 retrieved docs â†’
You run 50 forward passes.

Latency scales linearly.

---

## Optimization Levers

### 1ï¸âƒ£ Reduce Candidate Pool

Instead of:

```
Retrieve 100 â†’ Rerank 100
```

Try:

```
Retrieve 30 â†’ Rerank 30
```

Often no quality drop.

---

### 2ï¸âƒ£ Smaller Cross-Encoder

Use:

* MiniLM-based reranker
* Distilled model

Trade 1â€“2% precision for 50% latency gain.

---

### 3ï¸âƒ£ Batch Inference (GPU)

Batch (query, doc) pairs.

Huge speedup if using GPU.

---

### 4ï¸âƒ£ Two-Stage Reranking

Example:

```
Hybrid Retrieve 50
â†“
Light reranker â†’ 20
â†“
Heavy reranker â†’ 5
```

Multi-stage ranking reduces cost.

---

### 5ï¸âƒ£ Cache Frequent Queries

Enterprise systems often have repeated questions.

Cache reranked results.

---

### 6ï¸âƒ£ Distillation

Train small reranker on outputs of large one.

Used heavily in production search systems.

---

# ğŸ§  Meta-Level Understanding

These four questions test whether you understand:

| Problem                      | Layer                     |
| ---------------------------- | ------------------------- |
| Low Recall@5                 | Retrieval                 |
| Recall@20 high, wrong answer | Ranking or Generation     |
| Hallucinated extra details   | Generation / Faithfulness |
| High latency after reranking | Systems optimization      |

If you can classify failures correctly,
you are thinking like a **retrieval engineer**, not a chatbot builder.

---

# ğŸ¯ Next Step

We now move to:

# ğŸš€ Day 18 â€” Conversational RAG & Memory

This is where:

* Context spans multiple turns
* Queries depend on previous answers
* Retrieval becomes stateful

And the system stops being a QA tool
and starts becoming an assistant.

Ready?
