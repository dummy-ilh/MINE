# ðŸš€ RAG Mastery â€” Day 16

# ðŸ“ RAG Evaluation: Measuring What Actually Matters

Until now, weâ€™ve engineered retrieval pipelines.

Today we answer the hardest question:

> **How do you know your RAG system is actually good?**

If you cannot measure it, you cannot improve it.

Modern RAG evaluation has **three layers**:

```
1ï¸âƒ£ Retrieval Quality
2ï¸âƒ£ Generation Quality
3ï¸âƒ£ End-to-End Faithfulness
```

Letâ€™s go deep.

---

# ðŸ§± Layer 1: Retrieval Evaluation

Before judging answers, judge retrieval.

Because:

> If the right document is not retrieved, the LLM never had a chance.

---

## Core Retrieval Metrics

### 1ï¸âƒ£ Recall@K

**Definition:**
Did we retrieve at least one relevant document in top-K?

```
Recall@5 = % of queries where at least one correct doc is in top 5
```

This is the most important retrieval metric.

---

### 2ï¸âƒ£ Precision@K

How many of top-K are relevant?

More useful in reranking evaluation.

---

### 3ï¸âƒ£ MRR (Mean Reciprocal Rank)

Measures how high the first relevant doc appears.

```
MRR = average(1 / rank_of_first_relevant_doc)
```

Higher = better ranking quality.

---

## âš ï¸ Important Insight

For RAG systems:

> High Recall@K matters more than high Precision@K
> because rerankers + LLM can fix precision later.

---

# ðŸ§  Layer 2: Generation Evaluation

Now assume correct docs were retrieved.

We evaluate:

* Is the answer correct?
* Is it grounded in context?
* Is it hallucinated?

---

## 1ï¸âƒ£ Exact Match (EM)

Used in QA datasets.

Very strict â€” not ideal for generative systems.

---

## 2ï¸âƒ£ F1 Score

Measures token overlap between generated answer and reference.

Still brittle for long-form answers.

---

## 3ï¸âƒ£ LLM-as-Judge (Modern Approach)

Use an LLM to evaluate:

* Correctness
* Completeness
* Faithfulness
* Relevance

This is what:

* OpenAI eval pipelines use
* Anthropic research teams use
* Google Gemini eval pipelines use

---

# ðŸ§¬ Layer 3: Faithfulness / Groundedness

This is RAG-specific.

We care about:

> Did the model use only retrieved context?

This is where hallucination detection comes in.

---

## 1ï¸âƒ£ Context Precision

Does the answer contain unsupported claims?

Approach:

* Break answer into atomic statements
* Check if each statement is supported in retrieved text

---

## 2ï¸âƒ£ Attribution Score

How well does answer cite evidence?

Used heavily in:

* Perplexity AI

---

## 3ï¸âƒ£ RAGAS Framework

Ragas provides:

* Faithfulness
* Answer relevance
* Context precision
* Context recall

It uses LLM-based scoring.

---

# ðŸ— Full Evaluation Architecture

```
Dataset (Query, Gold Answer, Gold Docs)
        â†“
Run RAG Pipeline
        â†“
Compute:

Retrieval:
- Recall@K
- MRR

Generation:
- LLM correctness score

Faithfulness:
- Statement grounding check
```

---

# ðŸ”¬ Building a Proper Eval Dataset

You need:

For each query:

* Ground truth answer
* Ground truth supporting documents

Without this, you are guessing.

In enterprise systems, teams manually annotate:

* 100â€“500 queries
* High-quality ground truth labels

This is your benchmark suite.

---

# âš–ï¸ Offline vs Online Evaluation

### Offline

* Controlled dataset
* Reproducible
* Used for model comparison

### Online (A/B testing)

* Real users
* Measure:

  * Click-through rate
  * User satisfaction
  * Resolution rate

Large companies rely heavily on online eval.

---

# ðŸ§  Failure Taxonomy (Extremely Important)

When RAG fails, classify:

### 1ï¸âƒ£ Retrieval Failure

Correct doc not retrieved.

Fix:

* Hybrid retrieval
* Better chunking
* Query rewriting

---

### 2ï¸âƒ£ Ranking Failure

Correct doc retrieved but ranked low.

Fix:

* Reranker
* Better scoring

---

### 3ï¸âƒ£ Generation Failure

Correct doc retrieved but LLM ignored it.

Fix:

* Better prompting
* Stronger grounding instructions

---

### 4ï¸âƒ£ Hallucination

Answer not supported by context.

Fix:

* Smaller context
* Faithfulness constraints
* Citation enforcement

---

# ðŸ§ª Example Eval Loop (Pseudo-Code)

```python
for query in eval_dataset:
    retrieved_docs = retrieve(query)
    answer = generate(query, retrieved_docs)

    retrieval_score = compute_recall(query, retrieved_docs)
    faithfulness_score = llm_judge(answer, retrieved_docs)
    correctness_score = llm_compare(answer, ground_truth)
```

Aggregate metrics at end.

---

# ðŸ“Š What Production Teams Track

Serious RAG systems track:

* Recall@5
* Recall@20
* MRR
* Faithfulness score
* Hallucination rate
* Latency
* Cost per query

Evaluation is multi-dimensional.

---

# ðŸ§  Deep Insight

In RAG:

> Retrieval quality sets the ceiling.
> Generation quality determines how close you get to that ceiling.

You cannot prompt-engineer your way out of bad retrieval.

---

# ðŸ”¥ Research Direction

Recent work explores:

* LLM-based retrieval evaluation
* Synthetic test set generation
* Adversarial queries
* Robustness to query reformulation

This is active research.

---

# ðŸ§ª Todayâ€™s Exercise

1. Build small evaluation dataset (20â€“50 queries).
2. Implement:

   * Recall@5
   * MRR
3. Add LLM-as-judge scoring.
4. Classify failures into:

   * Retrieval
   * Ranking
   * Generation

You now move from engineer â†’ researcher.

---

# ðŸŽ¯ Tomorrow (Day 17)

We enter:

> ðŸ§© Context Optimization â€” Chunking, Windowing, and Compression

This dramatically impacts retrieval performance.

Your RAG stack is becoming production-grade.
