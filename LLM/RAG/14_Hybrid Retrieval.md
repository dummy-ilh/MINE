# üöÄ RAG Mastery ‚Äì Day 14

# üß¨ Hybrid Retrieval (Dense + Sparse + Filters)

Yesterday we made retrieval smarter.

Today we make it **stronger**.

Single-vector search is not enough for production systems.

Modern RAG systems combine:

```
Dense Retrieval  (semantic)
+ Sparse Retrieval (keyword/BM25)
+ Metadata Filters
+ Optional Reranker
```

This is called **Hybrid Retrieval**.

---

# 1Ô∏è‚É£ Why Dense Alone Fails

Dense embeddings are good at:

‚úî Semantic similarity
‚úî Concept matching
‚úî Synonyms

But bad at:

‚úò Exact numbers
‚úò Rare keywords
‚úò IDs, codes, SKUs
‚úò Legal references
‚úò Short keyword queries

Example:

Query:

> ‚ÄúSOC2 Type II policy 2024 update‚Äù

Dense retrieval may ignore:

* "SOC2"
* "Type II"
* "2024"

But sparse retrieval (BM25) nails it.

---

# 2Ô∏è‚É£ Sparse Retrieval (BM25)

BM25 is a classic lexical ranking algorithm used in:

* Google Search
* Elasticsearch
* Apache Lucene

It scores documents using:

* Term frequency
* Inverse document frequency
* Length normalization

It doesn‚Äôt understand meaning ‚Äî but it understands **keywords extremely well**.

---

# 3Ô∏è‚É£ Dense Retrieval (Embeddings)

Used in:

* OpenAI embedding models
* Cohere
* Hugging Face

Captures:

* Semantic similarity
* Context
* Meaning

Fails on:

* Exact term importance
* Domain-specific tokens

---

# üß† Hybrid = Best of Both

## Formula (Simple Version)

```
Final Score = Œ± * Dense Score + Œ≤ * BM25 Score
```

Where:

* Œ±, Œ≤ tuned via validation
* Normalize both scores first!

---

# 4Ô∏è‚É£ Hybrid Architecture

```
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
User Query ‚Üí‚îÇ Embedding ‚îÇ‚Üí Dense Vector Search
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

User Query ‚Üí BM25 Search (Keyword)

        ‚Üì
Score Normalization
        ‚Üì
Weighted Merge
        ‚Üì
Top-k Results
```

---

# 5Ô∏è‚É£ Practical Implementation

### Option A ‚Äì Use Elasticsearch Hybrid

Elasticsearch supports:

* BM25
* Vector search
* Hybrid scoring
* Filters

Example query (conceptual):

```json
{
  "query": {
    "bool": {
      "must": [
        {"match": {"text": "SOC2 policy update"}},
        {"knn": {"embedding": {"vector": [...], "k": 10}}}
      ]
    }
  }
}
```

---

### Option B ‚Äì Manual Merge (FAISS + BM25)

```python
dense_results = dense_retriever(query)
sparse_results = bm25_retriever(query)

# normalize scores
# merge weighted
# deduplicate
```

---

# 6Ô∏è‚É£ Metadata Filtering (Underrated Power Move)

Before retrieval, filter by:

* Document type
* Date
* Customer
* Region
* Access permissions

Example:

> ‚ÄúShow Q4 revenue for Europe enterprise customers‚Äù

Filter first:

```
region = Europe
segment = enterprise
quarter = Q4
```

Then retrieve.

This dramatically improves precision.

---

# 7Ô∏è‚É£ Score Normalization (Critical)

Dense scores range differently than BM25.

Common methods:

* Min-Max normalization
* Z-score normalization
* Rank-based fusion (RRF)

---

## üèÜ Reciprocal Rank Fusion (RRF)

Used widely in IR systems.

Formula:

```
Score = 1 / (k + rank)
```

Very stable, no tuning needed.

Often beats weighted sums.

---

# 8Ô∏è‚É£ Real Production Pattern

Most serious RAG systems look like:

```
Query Rewrite
‚Üì
Metadata Filter
‚Üì
Hybrid Retrieve (Dense + BM25)
‚Üì
Rerank (Cross-Encoder)
‚Üì
Top 5 ‚Üí LLM
```

This is how search-quality systems are built.

---

# 9Ô∏è‚É£ When to Use Hybrid

Use Hybrid if:

* Legal docs
* Financial reports
* Codebases
* Technical manuals
* Enterprise search
* FAQs with IDs

If you're building toy chatbot ‚Üí dense is fine.

If you're building enterprise system ‚Üí hybrid is mandatory.

---

# üî¨ Experimental Insight

In benchmarks:

Dense only ‚Üí good semantic recall
Sparse only ‚Üí good keyword precision
Hybrid ‚Üí 10‚Äì25% retrieval boost

Especially for domain-specific corpora.

---

# üß™ Exercise for Today

Implement:

1. BM25 (use rank-bm25 or Elastic)
2. Dense retriever
3. RRF merge
4. Compare against dense-only

Evaluate:

* Recall@5
* MRR
* Latency
* Failure cases

---

# üß† Deep Thinking Question

Why does hybrid often outperform reranking-only approaches?

Think about this carefully.

---

# üéØ Tomorrow (Day 15)

We enter:

> ‚öñÔ∏è Reranking (Cross-Encoders vs Bi-Encoders)

This is where we trade latency for precision.

Your retrieval stack is starting to look production-grade now.
