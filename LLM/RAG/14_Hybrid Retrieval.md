# üöÄ RAG Mastery ‚Äì Day 14

# üß¨ Hybrid Retrieval (Dense + Sparse + Filters)


Single-vector search is not enough for production systems.

Modern RAG systems combine:

```
Dense Retrieval  (semantic)
+ Sparse Retrieval (keyword/BM25)
+ Metadata Filters
+ Optional Reranker
```

This is called **Hybrid Retrieval**.

---

# 1Ô∏è‚É£ Why Dense Alone Fails

Dense embeddings are good at:

‚úî Semantic similarity
‚úî Concept matching
‚úî Synonyms

But bad at:

‚úò Exact numbers
‚úò Rare keywords
‚úò IDs, codes, SKUs
‚úò Legal references
‚úò Short keyword queries

Example:

Query:

> ‚ÄúSOC2 Type II policy 2024 update‚Äù

Dense retrieval may ignore:

* "SOC2"
* "Type II"
* "2024"

But sparse retrieval (BM25) nails it.

---

# 2Ô∏è‚É£ Sparse Retrieval (BM25)

BM25 is a classic lexical ranking algorithm used in:

* Google Search
* Elasticsearch
* Apache Lucene

It scores documents using:

* Term frequency
* Inverse document frequency
* Length normalization

It doesn‚Äôt understand meaning ‚Äî but it understands **keywords extremely well**.

---

# 3Ô∏è‚É£ Dense Retrieval (Embeddings)

Used in:

* OpenAI embedding models
* Cohere
* Hugging Face

Captures:

* Semantic similarity
* Context
* Meaning

Fails on:

* Exact term importance
* Domain-specific tokens

---

# üß† Hybrid = Best of Both

## Formula (Simple Version)

```
Final Score = Œ± * Dense Score + Œ≤ * BM25 Score
```

Where:

* Œ±, Œ≤ tuned via validation
* Normalize both scores first!

---

# 4Ô∏è‚É£ Hybrid Architecture

```
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
User Query ‚Üí‚îÇ Embedding ‚îÇ‚Üí Dense Vector Search
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

User Query ‚Üí BM25 Search (Keyword)

        ‚Üì
Score Normalization
        ‚Üì
Weighted Merge
        ‚Üì
Top-k Results
```

---

# 5Ô∏è‚É£ Practical Implementation

### Option A ‚Äì Use Elasticsearch Hybrid

Elasticsearch supports:

* BM25
* Vector search
* Hybrid scoring
* Filters

Example query (conceptual):

```json
{
  "query": {
    "bool": {
      "must": $[
        {"match": {"text": "SOC2 policy update"}},
        {"knn": {"embedding": {"vector": $[...]$, "k": 10}}}
      ]$
    }
  }
}
```

---

### Option B ‚Äì Manual Merge (FAISS + BM25)

```python
dense_results = dense_retriever(query)
sparse_results = bm25_retriever(query)

# normalize scores
# merge weighted
# deduplicate
```

---

# 6Ô∏è‚É£ Metadata Filtering (Underrated Power Move)

Before retrieval, filter by:

* Document type
* Date
* Customer
* Region
* Access permissions

Example:

> ‚ÄúShow Q4 revenue for Europe enterprise customers‚Äù

Filter first:

```
region = Europe
segment = enterprise
quarter = Q4
```

Then retrieve.

This dramatically improves precision.

---

# 7Ô∏è‚É£ Score Normalization (Critical)

Dense scores range differently than BM25.

Common methods:

* Min-Max normalization
* Z-score normalization
* Rank-based fusion (RRF)

---

## üèÜ Reciprocal Rank Fusion (RRF)

Used widely in IR systems.

Formula:

```
Score = 1 / (k + rank)
```

Very stable, no tuning needed.

Often beats weighted sums.

---

# 8Ô∏è‚É£ Real Production Pattern

Most serious RAG systems look like:

```
Query Rewrite
‚Üì
Metadata Filter
‚Üì
Hybrid Retrieve (Dense + BM25)
‚Üì
Rerank (Cross-Encoder)
‚Üì
Top 5 ‚Üí LLM
```

This is how search-quality systems are built.

---

# 9Ô∏è‚É£ When to Use Hybrid

Use Hybrid if:

* Legal docs
* Financial reports
* Codebases
* Technical manuals
* Enterprise search
* FAQs with IDs

If you're building toy chatbot ‚Üí dense is fine.

If you're building enterprise system ‚Üí hybrid is mandatory.

---

# üî¨ Experimental Insight

In benchmarks:

Dense only ‚Üí good semantic recall
Sparse only ‚Üí good keyword precision
Hybrid ‚Üí 10‚Äì25% retrieval boost

Especially for domain-specific corpora.

---

# üß™ Exercise for Today

Implement:

1. BM25 (use rank-bm25 or Elastic)
2. Dense retriever
3. RRF merge
4. Compare against dense-only

Evaluate:

* Recall@5
* MRR
* Latency
* Failure cases

---


# PART 1 ‚Äî What is Hybrid Retrieval?

Hybrid retrieval = combining:

* **Sparse search (BM25 / keyword / lexical)**
* **Dense search (vector similarity)**

Because each solves different failure modes.

---

## Why Not Just Use Vectors?

Dense retrieval:

* Understands semantics
* Good for paraphrases
* Bad at exact keywords, rare terms, numbers

Sparse retrieval:

* Exact matching
* Good for:

  * IDs
  * Dates
  * Product names
  * Legal clauses
* Bad for semantic paraphrasing

---

## Example

Query:

> ‚ÄúWhat are penalties under Section 498A?‚Äù

Vector search might return:

* Domestic violence related documents

BM25 will correctly hit:

* Exact legal section references

Best result?
üëâ Combine both.

---

# PART 2 ‚Äî How Hybrid Retrieval Works

Architecture:

```
User Query
     ‚Üì
Sparse Retriever (BM25)
Dense Retriever (ANN)
     ‚Üì
Score Normalization
     ‚Üì
Fusion / Reranking
     ‚Üì
Top-K Final Results
```

---

# PART 3 ‚Äî Score Normalization (Critical)

Here‚Äôs the issue:

BM25 score range:

```
0 ‚Üí 20+
```

Cosine similarity:

```
-1 ‚Üí 1 (usually 0.3 ‚Üí 0.9)
```

You cannot directly add them.

So we normalize.

---

## üîπ Method 1: Min-Max Normalization

For each retriever:

$[
normalized = \frac{score - min}{max - min}
]$

Now both become:

```
0 ‚Üí 1
```

Then combine:

$[
final = \alpha \cdot dense + (1-\alpha) \cdot sparse
]$

Example:

| Doc | BM25 | Dense | Norm BM25 | Norm Dense | Final |
| --- | ---- | ----- | --------- | ---------- | ----- |
| A   | 10   | 0.8   | 0.7       | 0.85       | 0.79  |
| B   | 15   | 0.6   | 1.0       | 0.6        | 0.76  |

If Œ± = 0.6 ‚Üí favor dense

---

### When to Use?

* Small top-K lists
* Same query batch
* Quick fusion

---

## üîπ Method 2: Z-score Normalization

$[
z = \frac{score - \mu}{\sigma}
]$

Better when:

* Score distributions vary per query
* You want statistical scaling

Used in:

* Large production systems

---

## üîπ Method 3: Reciprocal Rank Fusion (RRF)

Instead of scores, use ranks:

$[
RRF = \sum \frac{1}{k + rank}
]$

If document ranks:

| Doc | Sparse Rank | Dense Rank |
| --- | ----------- | ---------- |
| A   | 1           | 5          |
| B   | 3           | 2          |

Then:

$[
score = 1/(k+rank1) + 1/(k+rank2)
]$

Advantages:

* No need for normalization
* Robust to score distribution
* Very popular in hybrid systems

---

### When to Use RRF?

* When sparse and dense scoring scales are very different
* When using heterogeneous models
* When simplicity > tuning

---

# PART 4 ‚Äî When to Use Which Strategy?

| Scenario               | Best Strategy          |
| ---------------------- | ---------------------- |
| Legal / Compliance     | RRF                    |
| E-commerce search      | Weighted normalization |
| Research search        | Z-score                |
| Low engineering effort | RRF                    |

---

# PART 5 ‚Äî Metadata Filtering (Very Important)

Now let‚Äôs discuss filtering.

Example:

```
Find AI articles
WHERE country = 'India'
AND date > 2024
```

How is this done internally?

---

## Strategy 1 ‚Äî Filter First, Then Vector Search

```
Filter ‚Üí Reduce candidate set ‚Üí ANN search
```

Works when:

* Filter reduces dataset significantly
* Few documents per partition

Used in:

* Qdrant
* Weaviate

Pros:

* Faster
* Efficient

Cons:

* If filter too broad ‚Üí no benefit

---

## Strategy 2 ‚Äî Search First, Then Filter

```
ANN top-100 ‚Üí Apply filter ‚Üí Return top-K
```

Used when:

* Filter not selective
* ANN search is cheap

Problem:
If filtering removes many results, you may get fewer than K outputs.

Solution:

* Over-fetch (top-200)

---

## Strategy 3 ‚Äî Hybrid Filtering (Index-Level Filtering)

Advanced systems build:

* Inverted index for metadata
* HNSW graph aware of payload

This allows:

* Search constrained within filtered subset

Used in:

* Pinecone
* Qdrant

---

# Example End-to-End Flow

User query:

> ‚ÄúAI startups in India after 2023‚Äù

System:

1. Metadata filter:

   * country=India
   * year>2023
2. Dense retrieval within subset
3. Sparse retrieval within subset
4. Normalize scores
5. RRF fusion
6. Optional cross-encoder reranker

---

# PART 6 ‚Äî Common Pitfalls

‚ùå Not normalizing scores
‚ùå Using fixed Œ± without validation
‚ùå Filtering after top-10 (too small candidate set)
‚ùå Not over-fetching
‚ùå Ignoring evaluation metrics (Recall@k, MRR)

---

# PART 7 ‚Äî When Hybrid Is Actually Necessary

You should use hybrid if:

* Queries contain numbers, IDs, product names
* Domain is legal / medical / financial
* You need high recall
* You see lexical mismatch failures

If your dataset is:

* Conversational knowledge base
* FAQ style

Dense-only might be enough.

---

# PART 8 ‚Äî Production-Grade Hybrid Stack

Typical enterprise RAG:

```
Elasticsearch (BM25)
+
Vector DB (HNSW)
+
Fusion Layer (RRF)
+
Cross-Encoder Re-ranker
+
LLM
```

---

# Final Mental Model

Sparse ‚Üí precision on exact tokens
Dense ‚Üí semantic recall
Fusion ‚Üí robustness
Metadata filter ‚Üí relevance constraints
Reranker ‚Üí final quality

---

Excellent. Now we design this like a **search infra engineer** building for 100M documents in production.

We‚Äôll split this into two major parts:

1. **System Architecture for 100M hybrid retrieval**
2. **Evaluation + tuning methodology**

No fluff ‚Äî real infra thinking.

---

# PART 1 ‚Äî Designing Hybrid Retrieval for 100M Documents

## üéØ Constraints (Assume)

* 100M documents
* 768-dim embeddings
* Metadata filtering required
* P95 latency target < 150ms
* Continuous updates
* Multi-tenant
* High recall requirement (RAG use case)

---

# 1Ô∏è‚É£ High-Level Architecture

```
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ   API Layer   ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ Query Router  ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚Üì                             ‚Üì
  Sparse Retrieval                Dense Retrieval
 (BM25 / inverted index)          (ANN - HNSW/IVF)
          ‚Üì                             ‚Üì
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
             Fusion Layer (RRF / Weighted)
                     ‚Üì
             Cross-Encoder Re-ranker
                     ‚Üì
                   Top-K
                     ‚Üì
                    LLM
```

---

# 2Ô∏è‚É£ Storage Layer Design (100M Scale)

### Document Storage

* Raw docs in object storage (S3)
* Metadata in distributed DB
* Embeddings stored in vector DB shards

---

## Dense Index Choice

At 100M scale:

| Index    | Use?                        | Why         |
| -------- | --------------------------- | ----------- |
| HNSW     | Yes (if enough RAM)         | High recall |
| IVF + PQ | Yes (if memory constrained) | Compression |
| Flat     | No                          | Too slow    |

Most likely production choice:

* **IVF + PQ for base**
* Optional HNSW refinement layer

Used in:

* Milvus
* Pinecone

---

# 3Ô∏è‚É£ Sharding Strategy

100M √ó 768 dims ‚âà 300GB raw float32

So we must shard.

### Shard by:

Option A ‚Äî Hash of document ID
Option B ‚Äî Semantic clustering (better pruning)

At 100M, use:

* 16‚Äì64 shards
* Replication factor 2‚Äì3

---

# 4Ô∏è‚É£ Metadata Filtering at Scale

You cannot filter na√Øvely.

### Correct Approach:

Build:

* Inverted index for metadata
* Vector index per shard

Execution:

```
Step 1: Apply metadata filter ‚Üí candidate docIDs
Step 2: Restrict ANN search to those IDs
```

Advanced systems integrate filter constraints inside ANN graph traversal.

Supported by:

* Qdrant
* Weaviate

---

# 5Ô∏è‚É£ Query Execution Strategy

We do **parallel retrieval**:

Each shard:

* Compute sparse top-200
* Compute dense top-200

Coordinator:

* Normalize scores
* Apply RRF
* Merge
* Send top-100 to reranker
* Return top-10

---

# 6Ô∏è‚É£ Why Over-Fetch?

Because:

Filtering + fusion reduces recall.

Typical values:

* Retrieve 5√ó or 10√ó final k
* If final k = 20 ‚Üí fetch 200 candidates

---

# 7Ô∏è‚É£ Reranking Layer

ANN ‚â† final answer.

Add:

* Cross-encoder (BERT-like)
* Rerank top 100
* Improves MRR drastically

Latency tradeoff:

* Add 20‚Äì50ms

---

# PART 2 ‚Äî Evaluation Metrics for Hybrid Systems

Now the real science begins.

You cannot tune hybrid systems without proper evaluation.

---

# 1Ô∏è‚É£ Core Retrieval Metrics

### Recall@k

$[
Recall@k = \frac{\text{relevant docs in top-k}}{\text{total relevant docs}}
]$

Most important for RAG.

Why?

If retrieval fails, LLM fails.

---

### Precision@k

$[
Precision@k = \frac{\text{relevant in top-k}}{k}
]$

Important for search systems.

---

### MRR (Mean Reciprocal Rank)

$[
MRR = \frac{1}{rank\ of\ first\ relevant}
]$

Critical when:

* Only one correct answer
* QA systems

---

### NDCG (Normalized Discounted Cumulative Gain)

Accounts for ranking quality:

$[
DCG = \sum \frac{rel_i}{\log_2(i+1)}
]$

Best metric for:

* Graded relevance
* Enterprise search

---

# 2Ô∏è‚É£ Hybrid-Specific Evaluation

Now things get interesting.

We must measure:

* Dense-only performance
* Sparse-only performance
* Hybrid performance

You want:

Hybrid > max(Dense, Sparse)

If not, fusion is broken.

---

# 3Ô∏è‚É£ Offline Evaluation Pipeline

Steps:

1. Build query set (1K‚Äì10K queries)
2. Label relevance
3. Compute metrics for:

   * BM25 only
   * Dense only
   * Hybrid Œ±=0.1 ‚Ä¶ 0.9
   * RRF
4. Plot curves

---

# 4Ô∏è‚É£ Online Metrics

Once deployed:

* Click-through rate (CTR)
* Dwell time
* Success rate
* Query reformulation rate

For RAG:

* LLM answer correctness
* Hallucination rate
* Answer grounding score

---

# 5Ô∏è‚É£ Tuning Strategy

### Step 1 ‚Äî Tune Dense Retriever

Adjust:

* HNSW ef_search
* IVF nprobe
* Top-k fetch size

Optimize Recall@100

---

### Step 2 ‚Äî Tune Sparse Retriever

Adjust:

* BM25 k1
* b parameter

Optimize Recall@100

---

### Step 3 ‚Äî Tune Fusion

Test:

* Weighted sum (Œ± sweep)
* RRF
* Z-score fusion

Pick best NDCG@10

---

### Step 4 ‚Äî Tune Reranker

Measure:

* MRR before reranker
* MRR after reranker

If reranker gives <5% gain ‚Üí something wrong.

---

# 6Ô∏è‚É£ Failure Mode Analysis

Check:

* Are long queries failing?
* Are numeric queries failing?
* Are entity queries failing?
* Is metadata filtering killing recall?

Manually inspect 100 failures.

This is where elite systems improve.

---

# 7Ô∏è‚É£ Scaling Concerns at 100M

Watch for:

* Memory pressure
* Shard imbalance
* Filter selectivity skew
* Hot partitions
* Embedding drift over time

You may need:

* Periodic re-indexing
* Embedding model versioning
* Shadow evaluation pipeline

---

# Final System Blueprint

For 100M documents:

* Distributed sparse index (Elasticsearch)
* Distributed vector index (HNSW/IVF)
* RRF fusion
* Cross-encoder reranker
* Metadata-aware ANN
* Evaluation pipeline with NDCG + Recall@k
* A/B testing infra

---

# Advanced Question for You

If:

* Dense Recall@100 = 82%
* Sparse Recall@100 = 75%
* Hybrid Recall@100 = 84%

Is your fusion working properly?

You gave:

* Dense Recall@100 = **82%**
* Sparse Recall@100 = **75%**
* Hybrid Recall@100 = **84%**

Now let‚Äôs reason like an infra engineer.

---

# Short Answer

Yes‚Ä¶ but **barely**.

It is technically working ‚Äî
but it is **not working optimally**.

Let‚Äôs break this down carefully.

---

# 1Ô∏è‚É£ What Should Hybrid Ideally Do?

In theory, hybrid retrieval should:

$[
Hybrid \ge \max(Dense, Sparse)
]$

That condition is satisfied:

$[
84% > 82%
]$

So it is improving recall.

But here‚Äôs the real question:

üëâ Is +2% meaningful?

---

# 2Ô∏è‚É£ How Much Gain Should We Expect?

At 100M scale:

* Dense and sparse often retrieve different failure cases.
* Proper hybrid systems often improve **3‚Äì8% recall@100**.
* In some domains (legal, finance), even **10%+**.

A +2% gain suggests one of these:

1. Dense and sparse are highly correlated (retrieving same docs)
2. Fusion weights not tuned properly
3. Candidate pool too small (under-fetching)
4. Metadata filtering limiting diversity
5. Sparse not strong enough (BM25 poorly tuned)

---

# 3Ô∏è‚É£ Let‚Äôs Think Mathematically

Let:

* D = dense retrieved set
* S = sparse retrieved set

Hybrid recall depends on:

$[
|D ‚à™ S|
]$

If overlap between D and S is very high:

$[
|D ‚à© S| \text{ is large}
]$

Then hybrid cannot improve much.

A +2% gain implies:

Dense and sparse results overlap heavily.

---

# 4Ô∏è‚É£ Diagnostic Questions

To know if it's ‚Äúreally‚Äù working, check:

### A. What is the overlap ratio?

$[
Overlap = \frac{|D ‚à© S|}{|D|}
]$

If overlap > 70%, sparse adds little diversity.

---

### B. What happens at higher K?

Check:

* Recall@200
* Recall@500

If hybrid gain increases at higher K ‚Üí fusion is helping but truncated early.

---

### C. What is NDCG@10?

Sometimes recall improves slightly,
but ranking improves significantly.

Hybrid might improve top ranking quality even if recall gain is small.

---

# 5Ô∏è‚É£ When 2% Is Actually Huge

At 100M scale:

* 2% recall gain could mean thousands of queries fixed.
* For RAG, 2% more grounding can reduce hallucinations noticeably.

In enterprise systems, even +1% recall is valuable.

So:

Technically working ‚úÖ
But probably under-optimized ‚ö†Ô∏è

---

# 6Ô∏è‚É£ How To Improve It

Here‚Äôs what I would test:

### 1Ô∏è‚É£ Increase candidate pool

If you fetch:

* Dense top 100
* Sparse top 100

Try 300‚Äì500.

Hybrid needs diversity.

---

### 2Ô∏è‚É£ Try RRF instead of weighted sum

If using Œ±-weighted scoring,
switch to Reciprocal Rank Fusion.

RRF often gives more robust gains.

---

### 3Ô∏è‚É£ Tune BM25 parameters

Adjust:

* k1 (term frequency saturation)
* b (length normalization)

Sparse might be underperforming.

---

### 4Ô∏è‚É£ Improve Dense Recall

Increase:

* HNSW ef_search
* IVF nprobe

Better base recall ‚Üí better hybrid.

---

# 7Ô∏è‚É£ The Real Answer

Is it working?

Yes.

Is it optimal?

Probably not.

Hybrid systems that are truly tuned usually:

* Show consistent gains across Recall@50, @100
* Improve NDCG@10 noticeably
* Reduce failure cases in numeric/entity queries

---

Now let me test your systems thinking.

If:

Dense = 82%
Sparse = 75%
Hybrid = 81%

What does that imply?

Think carefully before answering.
.

You gave:

* Dense Recall@100 = **82%**
* Sparse Recall@100 = **75%**
* Hybrid Recall@100 = **84%**

Now let‚Äôs reason like an infra engineer.

---

# Short Answer

Yes‚Ä¶ but **barely**.

It is technically working ‚Äî
but it is **not working optimally**.

Let‚Äôs break this down carefully.

---

# 1Ô∏è‚É£ What Should Hybrid Ideally Do?

In theory, hybrid retrieval should:

$[
Hybrid \ge \max(Dense, Sparse)
]$

That condition is satisfied:

$[
84% > 82%
]$

So it is improving recall.

But here‚Äôs the real question:

üëâ Is +2% meaningful?

---

# 2Ô∏è‚É£ How Much Gain Should We Expect?

At 100M scale:

* Dense and sparse often retrieve different failure cases.
* Proper hybrid systems often improve **3‚Äì8% recall@100**.
* In some domains (legal, finance), even **10%+**.

A +2% gain suggests one of these:

1. Dense and sparse are highly correlated (retrieving same docs)
2. Fusion weights not tuned properly
3. Candidate pool too small (under-fetching)
4. Metadata filtering limiting diversity
5. Sparse not strong enough (BM25 poorly tuned)

---

# 3Ô∏è‚É£ Let‚Äôs Think Mathematically

Let:

* D = dense retrieved set
* S = sparse retrieved set

Hybrid recall depends on:

$[
|D ‚à™ S|
]$

If overlap between D and S is very high:

$[
|D ‚à© S| \text{ is large}
]$

Then hybrid cannot improve much.

A +2% gain implies:

Dense and sparse results overlap heavily.

---

# 4Ô∏è‚É£ Diagnostic Questions

To know if it's ‚Äúreally‚Äù working, check:

### A. What is the overlap ratio?

$[
Overlap = \frac{|D ‚à© S|}{|D|}
]$

If overlap > 70%, sparse adds little diversity.

---

### B. What happens at higher K?

Check:

* Recall@200
* Recall@500

If hybrid gain increases at higher K ‚Üí fusion is helping but truncated early.

---

### C. What is NDCG@10?

Sometimes recall improves slightly,
but ranking improves significantly.

Hybrid might improve top ranking quality even if recall gain is small.

---

# 5Ô∏è‚É£ When 2% Is Actually Huge

At 100M scale:

* 2% recall gain could mean thousands of queries fixed.
* For RAG, 2% more grounding can reduce hallucinations noticeably.

In enterprise systems, even +1% recall is valuable.

So:

Technically working ‚úÖ
But probably under-optimized ‚ö†Ô∏è

---

# 6Ô∏è‚É£ How To Improve It

Here‚Äôs what I would test:

### 1Ô∏è‚É£ Increase candidate pool

If you fetch:

* Dense top 100
* Sparse top 100

Try 300‚Äì500.

Hybrid needs diversity.

---

### 2Ô∏è‚É£ Try RRF instead of weighted sum

If using Œ±-weighted scoring,
switch to Reciprocal Rank Fusion.

RRF often gives more robust gains.

---

### 3Ô∏è‚É£ Tune BM25 parameters

Adjust:

* k1 (term frequency saturation)
* b (length normalization)

Sparse might be underperforming.

---

### 4Ô∏è‚É£ Improve Dense Recall

Increase:

* HNSW ef_search
* IVF nprobe

Better base recall ‚Üí better hybrid.

---

# 7Ô∏è‚É£ The Real Answer

Is it working?

Yes.

Is it optimal?

Probably not.

Hybrid systems that are truly tuned usually:

* Show consistent gains across Recall@50, @100
* Improve NDCG@10 noticeably
* Reduce failure cases in numeric/entity queries

---

Now let me test your systems thinking.

If:

Dense = 82%
Sparse = 75%
Hybrid = 81%

What does that imply?

Think carefully before answering.


