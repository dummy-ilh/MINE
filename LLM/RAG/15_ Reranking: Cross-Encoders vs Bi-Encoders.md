# ğŸš€ RAG Mastery â€“ Day 15

# âš–ï¸ Reranking: Cross-Encoders vs Bi-Encoders

Up to now, your pipeline looks like:

```
Query â†’ Hybrid Retrieval â†’ Top-K Documents
```

But hereâ€™s the reality:

> Retrieval optimizes **recall**.
> Reranking optimizes **precision**.



---

# ğŸ§  Why Reranking Is Needed

Vector search is approximate:

* Embedding similarity â‰  true relevance
* Hybrid search still noisy
* Long documents dilute meaning
* Multi-topic chunks get high scores

So instead of sending noisy context to the LLM, we re-score more precisely.

---

# 1ï¸âƒ£ Bi-Encoder (What Youâ€™ve Been Using)

Dense retrievers use a **bi-encoder** architecture:

```
Query â†’ Encoder A â†’ Vector
Doc   â†’ Encoder B â†’ Vector

Similarity = cosine(query_vec, doc_vec)
```

Fast because:

* Documents are pre-embedded
* Only one query encoding needed
* Scales to millions of docs

But limitation:

* Query and doc encoded independently
* No deep token-level interaction

---

# 2ï¸âƒ£ Cross-Encoder (Precision Machine)

Cross-encoders take:

```
$[Query + Document]$ â†’ Transformer â†’ Relevance Score
```

Now the model sees full interaction:

* Token-by-token attention
* Phrase matching
* Negations
* Subtle context shifts

This dramatically improves ranking.

---

# ğŸ”¬ Example

Query:

> Does rate limiting reduce API latency?

Document A:

> Rate limiting protects systems from overload.

Document B:

> Rate limiting can increase latency under heavy load.

Dense retrieval may score both similarly.

Cross-encoder understands:

* â€œreduce latencyâ€ vs â€œincrease latencyâ€
* Directional meaning

---

# 3ï¸âƒ£ Architecture with Reranker

```
Query
   â†“
Hybrid Retrieve (top 50)
   â†“
Cross-Encoder Rerank
   â†“
Top 5
   â†“
LLM
```

This is modern production RAG.

Used by:

* Perplexity AI
* Google
* Microsoft search stacks

---

# 4ï¸âƒ£ Performance Tradeoff

| Model Type    | Speed       | Accuracy | Use Case          |
| ------------- | ----------- | -------- | ----------------- |
| Bi-Encoder    | âš¡ Very Fast | Medium   | Initial retrieval |
| Cross-Encoder | ğŸ¢ Slower   | High     | Final rerank      |

Cross-encoder complexity:

If you retrieve 50 docs â†’ you must run 50 forward passes.

So latency scales linearly.

---

# 5ï¸âƒ£ Popular Rerank Models

* Cohere rerank models
* SentenceTransformers cross-encoders
* BERT-based cross-attention models
* OpenAI reranking APIs

Many production systems use lightweight cross-encoders fine-tuned on relevance data.

---

# 6ï¸âƒ£ Code Example (SentenceTransformers)

```python
from sentence_transformers import CrossEncoder

model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

pairs = $[(query, doc) for doc in retrieved_docs]$

scores = model.predict(pairs)

reranked = sorted(zip(retrieved_docs, scores),
                  key=lambda x: x$[1]$,
                  reverse=True)

top_docs = $[doc for doc, score in reranked$[:5]$]$
```

---

# 7ï¸âƒ£ Latency Optimization Strategies

## âœ… Strategy 1: Limit candidate pool

Retrieve 20â€“50 docs only.

## âœ… Strategy 2: Use smaller cross-encoders

MiniLM-based models are efficient.

## âœ… Strategy 3: Distillation

Train a smaller reranker on outputs of a large reranker.

## âœ… Strategy 4: GPU batching

Batch query-doc pairs.

---

# 8ï¸âƒ£ When Reranking Helps Most

Reranking gives massive gains when:

* Documents are long
* Corpus is noisy
* Legal/financial domain
* Multi-hop questions
* Overlapping topics

Less useful when:

* Small clean dataset
* FAQ-style corpus
* Very short chunks

---

# 9ï¸âƒ£ Reranking vs Increasing K

You might think:

> Why not just send top 20 docs to LLM?

Because:

* Context window cost explodes
* LLM attention gets diluted
* Hallucination risk increases
* Token cost â†‘

Better to send:

* Top 3â€“7 high precision chunks

---

# ğŸ§  Deep Insight

Think in IR terms:

Dense Retrieval â†’ High Recall
Hybrid Retrieval â†’ Balanced Recall
Cross-Encoder â†’ High Precision

Together:

```
Recall first
Precision second
Generation last
```

This ordering is fundamental.

---

# ğŸ”¥ Advanced Insight

Many top-tier systems now use:

```
Hybrid Retrieve (k=100)
â†“
Cross-Encoder Rerank
â†“
LLM-based Rerank (optional)
â†“
Top 5
```

Yes â€” sometimes LLMs themselves are used as relevance judges.

---


Now weâ€™re moving into the **real quality engine** of retrieval systems.

ANN retrieval finds *candidates*.
**Reranking decides what actually matters.**

Letâ€™s go deep â€” architect level.

---

# ğŸ¯ Why Reranking Exists

Dense retrieval (HNSW / IVF) optimizes:

$[
\text{Vector similarity}
]$

But vector similarity â‰  true relevance.

ANN is:

* Approximate
* Bi-encoder based (query & doc encoded separately)
* Fast but shallow

Reranking is:

* Exact
* Cross-attention based
* Slow but powerful

---

# ğŸ§  Core Idea

Instead of:

```
score = cosine(query_emb, doc_emb)
```

We compute:

```
score = relevance(query, doc) using joint encoding
```

The key difference:

| Bi-encoder        | Cross-encoder   |
| ----------------- | --------------- |
| Encode separately | Encode together |
| Fast              | Expensive       |
| Approximate       | Precise         |

---

# ğŸ”¬ Under the Hood: Cross-Encoder Reranking

Letâ€™s say:

Query:

> â€œpenalties under section 498Aâ€

Document:

> â€œSection 498A IPC describes cruelty by husband and related punishmentâ€¦â€

---

## Step 1 â€” Concatenation

Model input:

```
$[CLS]$ query tokens $[SEP]$ document tokens $[SEP]$
```

Now the transformer sees BOTH together.

---

## Step 2 â€” Cross-Attention

Unlike bi-encoder:

* Query tokens attend to document tokens
* Document tokens attend to query tokens

This enables:

* Exact term matching
* Negation understanding
* Context sensitivity
* Numerical reasoning

---

## Step 3 â€” Output Score

Model outputs:

$[
P(relevant | query, document)
]$

Often a single scalar.

---

# âš™ï¸ Pipeline in Production

For 100M documents:

```
ANN â†’ top 200 candidates
        â†“
Cross-encoder rerank
        â†“
Top 10 returned
```

We never rerank all 100M.
Only small candidate set.

---

# ğŸ”¥ Why It Works So Well

Dense embeddings compress meaning into fixed vector.

But compression loses:

* Fine-grained word interactions
* Rare entity specificity
* Logical structure

Cross-encoder restores those interactions.

---

# ğŸ“Š Measurable Effect

Typical improvements:

| Metric  | Before | After |
| ------- | ------ | ----- |
| MRR@10  | 0.62   | 0.71  |
| NDCG@10 | 0.68   | 0.76  |

Huge gain at top ranks.

---

# ğŸ—ï¸ Architecture Variants

## 1ï¸âƒ£ MonoT5

Treat reranking as text-to-text:

```
Input: Query + Doc
Output: "relevant" or "not relevant"
```

---

## 2ï¸âƒ£ BERT Cross-Encoder

Output scalar relevance score.

Most common approach.

---

## 3ï¸âƒ£ Late Interaction Models (ColBERT-style)

Hybrid approach:

* Token-level embeddings
* MaxSim aggregation
* Faster than full cross-encoder

Used when:

* Want better accuracy than bi-encoder
* But cheaper than full cross-encoder

---

# â± Latency Consideration

If:

* 200 candidates
* Each inference 3ms on GPU

Total â‰ˆ 600ms âŒ too slow

So we:

* Batch process
* Use smaller model (MiniLM)
* Reduce candidates to 50â€“100

Target:

* 20â€“50ms reranking latency

---

# ğŸ¯ What Reranker Actually Learns

It learns:

* Query-document semantic alignment
* Entity match importance
* Field weighting
* Phrase importance
* Negation patterns
* Answer-bearing signals

It is trained on:

* Click logs
* Relevance labels
* Pairwise ranking loss
* Listwise ranking loss

---

# ğŸ§ª Training Objective (Behind the Scenes)

Common loss:

### Pairwise Loss

For relevant doc ( d^+ ) and irrelevant ( d^- ):

$[
L = \max(0, 1 - s(q,d^+) + s(q,d^-))
]$

Force relevant > irrelevant.

---

# ğŸ’¡ Why Not Use Cross-Encoder Directly on All Docs?

Complexity:

$[
O(N Ã— Transformer)
]$

Impossible at 100M scale.

ANN reduces to:

$[
O(200 Ã— Transformer)
]$

---

# ğŸ§  Mental Model

Think of retrieval as:

Stage 1 â€” Candidate Generator
Stage 2 â€” Precision Refiner

ANN maximizes recall.
Reranker maximizes precision.

---

# ğŸš¨ Common Mistakes

âŒ Reranking only top 10
âŒ Using too small candidate pool
âŒ Not batching GPU inference
âŒ Using reranker without evaluation
âŒ Training reranker on biased click data

---

# ğŸ” Reranking in Hybrid Systems

Hybrid gives broader candidate pool.

Reranker then:

* Resolves sparse vs dense conflicts
* Picks best ordering
* Fixes lexical noise

Hybrid + reranker together are multiplicative.

---

# ğŸ§  Deep Insight

Dense retrieval approximates:

$[
argmax_d f(q)^T f(d)
]$

Cross-encoder approximates:

$[
argmax_d g(q, d)
]$

Where:

* f = compressed representation
* g = full interaction function

g is strictly more expressive than f.

---

# ğŸ§ª How to Measure Reranker Quality

Evaluate:

* MRR@10
* NDCG@10
* Delta improvement over base retriever

If improvement < 3%:

* Candidate pool weak
* Model too small
* Domain mismatch

---

# ğŸ”¥ Production Blueprint

For 100M docs:

* ANN (Recall@200 â‰¥ 90%)
* Hybrid fusion
* Rerank top 100
* Return top 10
* Feed to LLM

---

Correct âœ… â€” itâ€™s **good**.

Now letâ€™s explain *why*, at a systems level.

---

# ğŸ¯ If MRR Improves but Recall@100 Stays the Same

That means:

* The correct documents were **already in the top 100**
* The reranker just moved them **higher**

So:

Recall measures:

> *Did we retrieve the relevant doc anywhere in top K?*

MRR measures:

> *How early did we retrieve it?*

Reranking **does not increase recall**
It improves **ordering quality**.

---

# ğŸ” Why Thatâ€™s Excellent

For:

* Search systems
* QA systems
* RAG pipelines

Users care about:

* Top 1â€“10 results
* Not top 100

If the relevant doc moves:

From rank 42 â†’ rank 2

Thatâ€™s a massive improvement in user experience.

---

# ğŸ§  In RAG Context

If relevant doc is:

* Rank 75 â†’ LLM might not see it
* Rank 3 â†’ LLM definitely sees it

Even though Recall@100 unchanged.

So reranker improves:

* Grounding
* Faithfulness
* Answer accuracy
* Hallucination reduction

---

# ğŸ“Š Concrete Example

Before reranking:

| Rank | Relevant? |
| ---- | --------- |
| 1    | âŒ         |
| 2    | âŒ         |
| 3    | âŒ         |
| 4    | âŒ         |
| 5    | âŒ         |
| 42   | âœ…         |

MRR = 1/42 â‰ˆ 0.023

After reranking:

| Rank | Relevant? |
| ---- | --------- |
| 1    | âŒ         |
| 2    | âœ…         |

MRR = 1/2 = 0.5

Huge gain.

Recall@100 = same.

---

# ğŸš€ What This Tells You About Your System

It means:

1. Retriever has decent recall
2. Candidate generation working
3. Reranker doing its job

If reranker improves MRR significantly,
your stage-1 retrieval is good enough.

---

# ğŸ”¥ When It Would Be Bad

If:

Recall@100 = low (say 60%)
Reranker improves MRR

That means:
You are polishing bad candidates.

No reranker can fix missing recall.

---

# ğŸ§  The Core Principle

Stage 1 â†’ maximize recall
Stage 2 â†’ maximize precision

If reranker improves MRR but recall unchanged,
your pipeline architecture is correct.

---



