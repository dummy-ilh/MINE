# ğŸš€ RAG Mastery â€“ Day 15

# âš–ï¸ Reranking: Cross-Encoders vs Bi-Encoders

Up to now, your pipeline looks like:

```
Query â†’ Hybrid Retrieval â†’ Top-K Documents
```

But hereâ€™s the reality:

> Retrieval optimizes **recall**.
> Reranking optimizes **precision**.

Day 15 is about making your top 20 results become the **correct top 5**.

---

# ğŸ§  Why Reranking Is Needed

Vector search is approximate:

* Embedding similarity â‰  true relevance
* Hybrid search still noisy
* Long documents dilute meaning
* Multi-topic chunks get high scores

So instead of sending noisy context to the LLM, we re-score more precisely.

---

# 1ï¸âƒ£ Bi-Encoder (What Youâ€™ve Been Using)

Dense retrievers use a **bi-encoder** architecture:

```
Query â†’ Encoder A â†’ Vector
Doc   â†’ Encoder B â†’ Vector

Similarity = cosine(query_vec, doc_vec)
```

Fast because:

* Documents are pre-embedded
* Only one query encoding needed
* Scales to millions of docs

But limitation:

* Query and doc encoded independently
* No deep token-level interaction

---

# 2ï¸âƒ£ Cross-Encoder (Precision Machine)

Cross-encoders take:

```
[Query + Document] â†’ Transformer â†’ Relevance Score
```

Now the model sees full interaction:

* Token-by-token attention
* Phrase matching
* Negations
* Subtle context shifts

This dramatically improves ranking.

---

# ğŸ”¬ Example

Query:

> Does rate limiting reduce API latency?

Document A:

> Rate limiting protects systems from overload.

Document B:

> Rate limiting can increase latency under heavy load.

Dense retrieval may score both similarly.

Cross-encoder understands:

* â€œreduce latencyâ€ vs â€œincrease latencyâ€
* Directional meaning

---

# 3ï¸âƒ£ Architecture with Reranker

```
Query
   â†“
Hybrid Retrieve (top 50)
   â†“
Cross-Encoder Rerank
   â†“
Top 5
   â†“
LLM
```

This is modern production RAG.

Used by:

* Perplexity AI
* Google
* Microsoft search stacks

---

# 4ï¸âƒ£ Performance Tradeoff

| Model Type    | Speed       | Accuracy | Use Case          |
| ------------- | ----------- | -------- | ----------------- |
| Bi-Encoder    | âš¡ Very Fast | Medium   | Initial retrieval |
| Cross-Encoder | ğŸ¢ Slower   | High     | Final rerank      |

Cross-encoder complexity:

If you retrieve 50 docs â†’ you must run 50 forward passes.

So latency scales linearly.

---

# 5ï¸âƒ£ Popular Rerank Models

* Cohere rerank models
* SentenceTransformers cross-encoders
* BERT-based cross-attention models
* OpenAI reranking APIs

Many production systems use lightweight cross-encoders fine-tuned on relevance data.

---

# 6ï¸âƒ£ Code Example (SentenceTransformers)

```python
from sentence_transformers import CrossEncoder

model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

pairs = [(query, doc) for doc in retrieved_docs]

scores = model.predict(pairs)

reranked = sorted(zip(retrieved_docs, scores),
                  key=lambda x: x[1],
                  reverse=True)

top_docs = [doc for doc, score in reranked[:5]]
```

---

# 7ï¸âƒ£ Latency Optimization Strategies

## âœ… Strategy 1: Limit candidate pool

Retrieve 20â€“50 docs only.

## âœ… Strategy 2: Use smaller cross-encoders

MiniLM-based models are efficient.

## âœ… Strategy 3: Distillation

Train a smaller reranker on outputs of a large reranker.

## âœ… Strategy 4: GPU batching

Batch query-doc pairs.

---

# 8ï¸âƒ£ When Reranking Helps Most

Reranking gives massive gains when:

* Documents are long
* Corpus is noisy
* Legal/financial domain
* Multi-hop questions
* Overlapping topics

Less useful when:

* Small clean dataset
* FAQ-style corpus
* Very short chunks

---

# 9ï¸âƒ£ Reranking vs Increasing K

You might think:

> Why not just send top 20 docs to LLM?

Because:

* Context window cost explodes
* LLM attention gets diluted
* Hallucination risk increases
* Token cost â†‘

Better to send:

* Top 3â€“7 high precision chunks

---

# ğŸ§  Deep Insight

Think in IR terms:

Dense Retrieval â†’ High Recall
Hybrid Retrieval â†’ Balanced Recall
Cross-Encoder â†’ High Precision

Together:

```
Recall first
Precision second
Generation last
```

This ordering is fundamental.

---

# ğŸ”¥ Advanced Insight

Many top-tier systems now use:

```
Hybrid Retrieve (k=100)
â†“
Cross-Encoder Rerank
â†“
LLM-based Rerank (optional)
â†“
Top 5
```

Yes â€” sometimes LLMs themselves are used as relevance judges.

---

# ğŸ§ª Exercise for Today

1. Add cross-encoder reranking.
2. Compare:

   * Dense only
   * Hybrid only
   * Hybrid + rerank
3. Measure:

   * Recall@5
   * MRR
   * Latency
4. Analyze failure cases.

---

# ğŸ§  Critical Thinking

Why not replace dense retrieval entirely with cross-encoders?

Answer carefully â€” this question separates beginners from system designers.

---

# ğŸ¯ Tomorrow (Day 16)

We go into:

> ğŸ“ RAG Evaluation â€” Measuring Groundedness, Faithfulness, and Retrieval Quality

This is where engineers become researchers.

Youâ€™re now building serious search systems.
