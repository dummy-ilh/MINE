Good. Weâ€™re entering **Week 2** now.

Week 1 = mental models.
Week 2 = you build the machine.

Today:

> **Day 8 â€” Build a Minimal RAG From Scratch (No Framework Magic)**

No LangChain abstractions.
No â€œvector store wrappers.â€
Just the core pipeline so you actually understand whatâ€™s happening.

---

# ğŸ“˜ Day 8 â€” Minimal RAG (Raw, Clean Architecture)

---

# 1ï¸âƒ£ The Full Pipeline (What Weâ€™re Building)

```
Documents
   â†“
Chunking
   â†“
Embeddings
   â†“
Vector Index (FAISS)
   â†“
Query Embedding
   â†“
Similarity Search
   â†“
Prompt Construction
   â†“
LLM Answer
```

Thatâ€™s it.

If you master this, frameworks become optional tools â€” not crutches.

---

# 2ï¸âƒ£ Step 1 â€” Install Dependencies

```bash
pip install openai faiss-cpu tiktoken numpy
```

(You can swap embedding model later.)

---

# 3ï¸âƒ£ Step 2 â€” Prepare Documents

Letâ€™s simulate:

```python
documents = [
    "Refunds requested after 30 days require manual approval.",
    "Customers in Germany are subject to additional VAT penalties.",
    "Refunds are processed within 5â€“7 business days."
]
```

In production, these come from:

* PDFs
* Markdown
* HTML
* Databases

We keep it simple today.

---

# 4ï¸âƒ£ Step 3 â€” Chunking (Minimal Version)

For now, assume:

* Each document = one chunk

Later (Day 9+), weâ€™ll build a real ingestion pipeline.

---

# 5ï¸âƒ£ Step 4 â€” Create Embeddings

```python
from openai import OpenAI
import numpy as np

client = OpenAI()

def embed(texts):
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    return np.array([e.embedding for e in response.data])
```

Generate document embeddings:

```python
doc_embeddings = embed(documents)
```

Shape:

```
(num_docs, embedding_dim)
```

---

# 6ï¸âƒ£ Step 5 â€” Build Vector Index (FAISS)

```python
import faiss

dimension = doc_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)

index.add(doc_embeddings)
```

Now FAISS holds your semantic space.

---

# 7ï¸âƒ£ Step 6 â€” Query-Time Retrieval

```python
query = "What happens if refund is late in Germany?"

query_embedding = embed([query])

k = 2
distances, indices = index.search(query_embedding, k)

retrieved_docs = [documents[i] for i in indices[0]]
```

Now you have top-k relevant chunks.

---

# 8ï¸âƒ£ Step 7 â€” Construct the Prompt

```python
context = "\n\n".join(retrieved_docs)

prompt = f"""
Answer the question using ONLY the context below.
If the answer is not in the context, say you don't know.

Context:
{context}

Question:
{query}
"""
```

---

# 9ï¸âƒ£ Step 8 â€” Generate Answer

```python
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": prompt}],
    temperature=0
)

print(response.choices[0].message.content)
```

Temperature = 0
â†’ deterministic
â†’ better for grounded answers

---

# ğŸ”Ÿ What You Just Built

You built:

* Embedding model
* Vector index
* Similarity search
* Prompt grounding
* Generation

This is 90% of every RAG system.

Frameworks just add:

* Orchestration
* Memory
* Observability
* Tools

---

# ğŸ§  Important Observations

### 1ï¸âƒ£ Retrieval quality matters more than generation

Even GPT-4 cannot fix wrong retrieval.

### 2ï¸âƒ£ You didnâ€™t normalize embeddings

If using cosine similarity:

* Normalize vectors first

FAISS L2 â‰  cosine unless normalized.

### 3ï¸âƒ£ No metadata yet

Production systems need:

* IDs
* Versioning
* Filtering

Weâ€™ll add that soon.

---

# 11ï¸âƒ£ Where This Minimal System Breaks

* No chunking strategy
* No hybrid search
* No re-ranking
* No metadata filtering
* No evaluation metrics
* No scaling

But this skeleton is correct.

---

# ğŸ¯ Interview Angle

If asked:

> â€œCan you build RAG without LangChain?â€

You should confidently say:

> â€œYes, itâ€™s just embeddings, vector search, and prompt construction. Frameworks add orchestration, but the core logic is straightforward.â€

That shows true understanding.

---

# ğŸ“… Day 9 Preview â€” Real Document Ingestion

Tomorrow we upgrade:

* Load PDFs
* Clean text
* Proper chunking
* Deduplication
* Metadata storage

We move from toy â†’ realistic.

Say **â€œDay 9â€** when ready.
