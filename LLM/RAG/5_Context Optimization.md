

# ðŸ§© Context Optimization: Chunking, Windowing & Compression

By now youâ€™ve built:

```
Rewrite â†’ Hybrid Retrieve â†’ Rerank â†’ Generate
```

But hereâ€™s a brutal truth:

> **Most RAG systems fail because of bad chunking.**

Even perfect retrieval cannot fix poorly structured context.

Today we optimize the *atomic unit* of RAG:
ðŸ‘‰ **The chunk**

---

# 1ï¸âƒ£ Why Chunking Is Critical

Imagine a 20-page document.

If you:

* Embed the whole thing â†’ semantic dilution
* Split randomly every 500 tokens â†’ broken meaning
* Split too small â†’ context fragmentation

Chunking directly affects:

* Retrieval recall
* Ranking precision
* Hallucination risk
* Token cost

---

# ðŸ§  Core Principle

> A chunk should represent one coherent idea.

Not:

* Half a paragraph
* Two unrelated sections
* An entire chapter

Think in terms of **semantic atomicity**.

---

# 2ï¸âƒ£ Fixed-Size Chunking (Baseline)

Example:

```
Split every 512 tokens with 50-token overlap
```

Pros:

* Simple
* Fast
* Works okay for uniform text

Cons:

* Cuts meaning arbitrarily
* Breaks tables and lists
* Hurts legal/technical docs

Good for:

* Blog content
* FAQs
* Short documents

---

# 3ï¸âƒ£ Semantic Chunking (Better)

Instead of token length, split by:

* Paragraph boundaries
* Headings
* Section markers
* Sentence clustering

Approach:

1. Split by paragraphs
2. Merge small ones
3. Keep chunks 300â€“800 tokens

This preserves meaning structure.

---

# 4ï¸âƒ£ Overlap Strategy

Overlap prevents boundary information loss.

Example:

```
Chunk 1: Tokens 0â€“500
Chunk 2: Tokens 450â€“950
```

Why overlap works:

* Questions often span chunk boundaries
* It increases recall
* Slightly increases storage cost

Typical overlap:

* 10â€“20%

Too much overlap â†’ duplication noise
Too little â†’ recall drops

---

# 5ï¸âƒ£ Sliding Window Retrieval

Instead of static chunks:

1. Retrieve candidate doc
2. Slide window across doc
3. Select best matching window

This reduces semantic dilution.

Used in advanced search systems like:

* Google search
* Microsoft enterprise search

---

# 6ï¸âƒ£ Hierarchical Retrieval

Very powerful pattern.

```
Level 1: Retrieve section
Level 2: Retrieve chunk inside section
```

Instead of indexing tiny chunks directly:

* Index sections
* Then refine inside

This reduces false positives.

---

# 7ï¸âƒ£ Context Compression (Advanced)

Even after reranking, context can be noisy.

We compress before sending to LLM.

## Method A: Extractive Compression

Use smaller model to extract:

* Only relevant sentences
* Remove boilerplate

## Method B: LLM Compression

Prompt:

> â€œExtract only information relevant to the query.â€

This reduces:

* Token cost
* Attention dilution
* Hallucination risk

---

# 8ï¸âƒ£ Lost-in-the-Middle Problem

Large context windows suffer from:

> Middle content gets less attention.

Even long-context models struggle here.

Solution:

* Put most relevant chunk at top
* Order by relevance score
* Keep context small and precise

---

# 9ï¸âƒ£ Chunk Size Tradeoff

| Small Chunks            | Large Chunks     |
| ----------------------- | ---------------- |
| High precision          | High recall      |
| Lower semantic dilution | More noise       |
| More embeddings         | Fewer embeddings |

Empirically:

* 300â€“800 tokens often best
* Domain dependent

---

# ðŸ”¬ Experimental Insight

Research shows:

Better chunking can improve retrieval more than switching embedding models.

Yes â€” chunking matters more than model choice sometimes.

---

# ðŸ— Ideal Production Pattern

```
Ingestion:
- Semantic split
- 500 tokens
- 15% overlap
- Metadata tagging

Retrieval:
- Hybrid search
- Rerank

Pre-Generation:
- Compress to relevant spans
- Order by relevance

Generation:
- Strong grounding prompt
```

This is near state-of-the-art practical RAG.

---

# ðŸ§  Deep Insight

Think of chunking like:

> Database schema design for vector search.

If your schema is bad, queries will suffer.

---

# ðŸ§ª Todayâ€™s Exercise

1. Compare:

   * 200-token chunks
   * 500-token chunks
   * 1000-token chunks
2. Measure:

   * Recall@5
   * MRR
   * Latency
3. Try semantic splitting instead of fixed-size.
4. Observe which failure cases disappear.

---

# ðŸ”¥ Critical Thinking

Why do extremely large context windows (like 100k tokens) NOT eliminate the need for good chunking?

Think carefully â€” this is subtle and important.

---


