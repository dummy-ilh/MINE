Since probability distributions are defined and characterized by parameters, models that are based on them are known as **parametric models**. The core idea behind a parametric model is that the data being analyzed are generated by an underlying **probability distribution function (PDF)**.

If a sample of data is truly representative of the overall population, then the sample data will follow the same PDF as the population data and share the same (though initially unknown) parameter values. These parameters specify the mean or location (shape) and possibly the scale of the PDF that best represents the population data, as well as the distribution of a random sample drawn from that population. A **statistical model** therefore represents the relationship between the parameters of the population’s underlying PDF and the analyst’s estimates of those parameters.

**Regression analysis** is one of the most common methods used to estimate these true parameters in as unbiased a way as possible. In other words, regression helps establish an accurate model of the population data. However, measurement errors can occur at nearly every step, and the random sample being analyzed may not perfectly reflect the true population or its parameters. The regression modeling process is thus a systematic approach to understanding and managing the uncertainty involved in estimating the true parameters of the population’s underlying distribution. This is crucial because predictions made from the model are assumed to represent that same population.

In practice, analysts typically rely on a limited set of PDFs to describe population data from which the sample is assumed to be drawn. If the response variable (( y )) is **binary** (0 or 1), a **Bernoulli distribution** is generally used to model the data, since it represents a sequence of 1s and 0s. If the variable is **continuous** and appears to follow a normal pattern, it is commonly modeled using a **Gaussian (normal) distribution**. This represents a straightforward modeling relationship.

Other commonly used distributions in modeling include the **lognormal**, **binomial**, **exponential**, **Poisson**, **negative binomial**, **gamma**, **inverse Gaussian**, and **beta** PDFs. Additionally, **mixtures of distributions** can be created to describe more complex data. For instance, the **lognormal**, **negative binomial**, and **beta binomial** distributions are examples of such mixture distributions. Despite being mixtures, these are still valid PDFs and operate under the same basic assumptions as other probability distributions.

It is also important to note that not all probability distributions share the same number of parameters. The **Bernoulli**, **exponential**, and **Poisson** distributions are **single-parameter** distributions, and models derived from them are likewise single-parameter models. This parameter typically represents the mean or location parameter. In contrast, the **normal**, **lognormal**, **gamma**, **inverse Gaussian**, **beta**, **beta binomial**, **binomial**, and **negative binomial** distributions are **two-parameter** models. The first four of these are **continuous distributions** characterized by a mean (or shape) and a scale (or variability) parameter. The **binomial**, **beta**, and **beta binomial** distributions will be discussed later in the context of **grouped logistic regression**.



