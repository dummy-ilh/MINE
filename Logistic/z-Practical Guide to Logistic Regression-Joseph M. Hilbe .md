Since probability distributions are defined and characterized by parameters, models that are based on them are known as **parametric models**. The core idea behind a parametric model is that the data being analyzed are generated by an underlying **probability distribution function (PDF)**.

If a sample of data is truly representative of the overall population, then the sample data will follow the same PDF as the population data and share the same (though initially unknown) parameter values. These parameters specify the mean or location (shape) and possibly the scale of the PDF that best represents the population data, as well as the distribution of a random sample drawn from that population. A **statistical model** therefore represents the relationship between the parameters of the population’s underlying PDF and the analyst’s estimates of those parameters.

**Regression analysis** is one of the most common methods used to estimate these true parameters in as unbiased a way as possible. In other words, regression helps establish an accurate model of the population data. However, measurement errors can occur at nearly every step, and the random sample being analyzed may not perfectly reflect the true population or its parameters. The regression modeling process is thus a systematic approach to understanding and managing the uncertainty involved in estimating the true parameters of the population’s underlying distribution. This is crucial because predictions made from the model are assumed to represent that same population.

In practice, analysts typically rely on a limited set of PDFs to describe population data from which the sample is assumed to be drawn. If the response variable (( y )) is **binary** (0 or 1), a **Bernoulli distribution** is generally used to model the data, since it represents a sequence of 1s and 0s. If the variable is **continuous** and appears to follow a normal pattern, it is commonly modeled using a **Gaussian (normal) distribution**. This represents a straightforward modeling relationship.

Other commonly used distributions in modeling include the **lognormal**, **binomial**, **exponential**, **Poisson**, **negative binomial**, **gamma**, **inverse Gaussian**, and **beta** PDFs. Additionally, **mixtures of distributions** can be created to describe more complex data. For instance, the **lognormal**, **negative binomial**, and **beta binomial** distributions are examples of such mixture distributions. Despite being mixtures, these are still valid PDFs and operate under the same basic assumptions as other probability distributions.

It is also important to note that not all probability distributions share the same number of parameters. The **Bernoulli**, **exponential**, and **Poisson** distributions are **single-parameter** distributions, and models derived from them are likewise single-parameter models. This parameter typically represents the mean or location parameter. In contrast, the **normal**, **lognormal**, **gamma**, **inverse Gaussian**, **beta**, **beta binomial**, **binomial**, and **negative binomial** distributions are **two-parameter** models. The first four of these are **continuous distributions** characterized by a mean (or shape) and a scale (or variability) parameter. The **binomial**, **beta**, and **beta binomial** distributions will be discussed later in the context of **grouped logistic regression**.


The **probability function** for a random sample can be expressed as:

$f(y; p) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}$

**(Equation 1.1)**

Here, the **joint probability distribution function (PDF)** is the product (Π) of the probabilities of each independent observation in the dataset, denoted by subscript ( i ). Typically, the product symbol is omitted for simplicity, since joint PDFs are understood to represent the product of independent components.

The **Bernoulli distribution** for a single observation can therefore be written as:

$
f(y_i; p_i) = p_i^{y_i} (1 - p_i)^{1 - y_i}
$
**(Equation 1.2)**

In this expression, ( y_i ) represents the response variable being modeled, and ( p_i ) is the probability that ( y_i = 1 ). The value ( y_i = 1 ) generally indicates a **success** or the **occurrence of the event of interest**, while ( y_i = 0 ) indicates **failure** or **non-occurrence**.

A **probability function** generates data based on known parameters—this is the meaning of ( f(y; p) ). However, in real applications, we typically have **known data** and wish to **estimate the unknown parameters**. To achieve this, we reverse the relationship between ( y ) and ( p ) in the PDF and attempt to calculate ( p ) based on ( y ). This inverted relationship is known as the **likelihood function**.

The **likelihood function** is expressed as:

[
L(p; y) = \prod_{i=1}^{n} p_i^{y_i} (1 - p_i)^{1 - y_i}
]
**(Equation 1.3)**

This structure is often rewritten in **exponential family form**, which is mathematically equivalent to Equation 1.3:

$
L(p; y) = \exp \left[ \sum_i y_i \ln(p_i) + (1 - y_i) \ln(1 - p_i) \right]
$

One advantage of expressing the **log-likelihood function** in this exponential form is that it allows us to easily derive the **link function**, as well as the **mean** and **variance** functions of the underlying Bernoulli distribution.

The **link function** corresponds to the term following ( y ) in the first part of the exponential form. For the Bernoulli distribution, this is:

[
\text{Link function: } \log\left(\frac{p}{1 - p}\right)
]

The **mean** of the distribution is obtained as the derivative of the negative of the second term with respect to the link function, while the **variance** is given by the second derivative. For the Bernoulli distribution:

$
\text{Mean: } \mu = p
$
$
\text{Variance: } V(p) = p(1 - p) = \mu(1 - \mu)
$

The **linear predictor** of the logistic regression model is given by:

$
x\beta = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
$
**(Equation 1.7)**

The **fitted (predicted) value** of the logistic regression model is determined through the **link function**, (\log\left(\frac{\mu}{1 - \mu}\right)), creating a linear relationship between the predicted probability ((\mu)) and the linear predictor ((x\beta)):

$
\ln\left(\frac{\mu_i}{1 - \mu_i}\right) = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_p x_{pi}
$
**(Equation 1.8)**

Here, (\mu_i) (like (p_i)) represents the probability that the response (y_i = 1). It can be interpreted as the probability of **presence** or **occurrence** of a characteristic, while (1 - \mu_i) represents the probability of its **absence**.

The **odds** of occurrence are defined as the ratio of success probability to failure probability:

$
\text{Odds} = \frac{\mu}{1 - \mu} = \frac{p}{1 - p}
$

If (\mu = 0.7), then (1 - \mu = 0.3), and (\mu + (1 - \mu) = 1). The logarithm of the odds, (\log\left(\frac{\mu}{1 - \mu}\right)), is called the **logit function**, which gives **logistic regression** its name.

To determine (\mu) from the linear predictor ((x\beta)), we solve the logit function for (\mu):

$
\mu = \frac{\exp(x\beta)}{1 + \exp(x\beta)} = \frac{1}{1 + \exp(-x\beta)}
$

This expression defines the **logistic function**, which maps any real-valued input into the interval (0, 1), making it ideal for modeling probabilities.


-------------
The **relationship between the logistic odds ratio and the regression coefficient** is given by:

[
\ln(\text{Odds Ratio}) = \text{Coefficient}
]
[
\exp(\text{Coefficient}) = \text{Odds Ratio}
]

---

### Comparison of Linear and Logistic Regression Models

| Model Type                    | Functional Form               | Expression for Mean (μ)                                                         |
| ----------------------------- | ----------------------------- | ------------------------------------------------------------------------------- |
| **Linear Regression**         | μ = x′β                       | The predicted value is a linear combination of predictors.                      |
| **Logistic Regression**       | μ = exp(x′β) / [1 + exp(x′β)] | The predicted probability is a nonlinear function bounded between 0 and 1.      |
| **Alternative Logistic Form** | μ = 1 / [1 + exp(−x′β)]       | Equivalent to the above, representing the standard logistic (sigmoid) function. |

---

### Standard Errors of Coefficients and Odds Ratios

**Standard errors (SEs)** measure the variability or uncertainty associated with estimated coefficients. They help analysts determine how much an estimated coefficient might vary if the study were repeated with different samples.

However, **the standard errors of odds ratios** cannot be directly extracted from the variance-covariance matrix of the estimated coefficients. Instead, they are computed using the **Delta Method** — a statistical approximation technique for estimating the variance of a nonlinear function of a random variable (see Hilbe, 2009; 2016).

When applying the **Delta Method** to odds ratios (or to risk or rate ratios), the calculation is straightforward:

[
SE_{OR} = \exp(\beta) \times SE_{\text{coef}}
]

where:

* ( \beta ) is the estimated coefficient,
* ( SE_{\text{coef}} ) is the standard error of the coefficient.

---

### Hypothesis Testing: z-score and p-value

The **z-score** for a coefficient is calculated as:

[
z = \frac{\text{coef}}{SE}
]

The **p-value** (for a two-tailed test) is computed as:

[
p = 2 \times P(Z > |z|) = 2 \times \text{pnorm}(|z|, \text{lower.tail} = FALSE)
]

In R, these calculations can be performed as:

```r
zscore <- coef / se
pvalue <- 2 * pnorm(abs(zscore), lower.tail = FALSE)
```

---

### Confidence Intervals (Wald Method)

The **Wald Confidence Interval** provides an approximate range within which the true coefficient is expected to fall with a specified probability (typically 95%).

Lower and upper confidence limits are calculated as:

[
\text{Lower CI} = \text{coef} - z_{0.975} \times SE
]
[
\text{Upper CI} = \text{coef} + z_{0.975} \times SE
]

In R, this can be written as:

```r
loci <- coef - qnorm(.975) * se
upci <- coef + qnorm(.975) * se
```

where `qnorm(0.975)` corresponds to the cutoff for the upper 2.5% tail of the standard normal distribution (approximately 1.96).

The **interpretation** of the 95% confidence interval is that if the modeling process were repeated many times, the true parameter value would fall within the calculated interval **95 times out of 100**.

---

### Likelihood Ratio Test (LRT)

The **Likelihood Ratio Test** compares two nested models — a **full model** (with all predictors) and a **reduced model** (with some predictors removed). It evaluates whether the additional predictors in the full model significantly improve model fit.

The test statistic is:

[
\text{LRT} = -2 \left[ \ln(L_{\text{reduced}}) - \ln(L_{\text{full}}) \right]
]

This statistic approximately follows a **chi-square (χ²) distribution**, with degrees of freedom equal to the difference in the number of parameters between the two models. A large LRT value (and small p-value) indicates that the full model provides a significantly better fit.


Here’s the rewritten and polished version of your passage while keeping all technical content and meaning intact:

---

```r
> exp(coef(logit3))
  (Intercept) factor(type)2 factor(type)3
    0.4727273     1.3664596     1.8665158
```

**Interpretation:**

* **Urgent admission** patients have approximately **37% greater odds** of dying in the hospital than **elective admissions**.
* **Emergency admission** patients have approximately **87% greater odds** of dying in the hospital than **elective admissions**.

---

```r
> medpar$type <- factor(medpar$type)
> medpar$type <- relevel(medpar$type, ref=3)
> logit4 <- glm(died ~ factor(type), family = binomial, data = medpar)
> exp(coef(logit4))
  (Intercept) factor(type)1 factor(type)2
    0.8823529     0.5357576     0.7320911
```

**Interpretation:**

* **Elective patients** have about **half the odds** of dying in the hospital compared with **emergency patients**.
* **Urgent patients** have about **three-quarters of the odds** of dying in the hospital compared with **emergency patients**.

---

As mentioned earlier, **indicator (dummy) variables** can be created manually, and levels can be **merged** if appropriate. This is often done when a particular category’s coefficient (or corresponding odds ratio) is **not statistically significant** compared to the reference level.

For instance, in the model where **type = 3 (emergency)** is the reference level, it appears that the **level 2 coefficient** (urgent admissions) may not be significantly different from the **level 1 coefficient** (elective admissions). This suggests that, based on the data, levels 1 and 2 might be **combined or treated as a single category** for a more parsimonious model.

---

Here’s your passage rewritten clearly and professionally, with all the original meaning and technical content preserved — just reformatted for readability and smoother flow:

---

A **continuous predictor** can take negative, zero, and positive numeric values. However, continuous predictors tend to pose more challenges for analysts than **discrete predictors** (such as binary or categorical variables). This is because the **distribution or shape** of a continuous variable may not be suitable for inclusion in a logistic model unless it is **transformed** appropriately.

A key concept to remember is that a continuous predictor must be **linear in the logit**. This means that the predictor should have a **linear relationship** with the logistic link function, ( \log(\mu / (1 - \mu)) ). If a continuous variable exhibits strong curvature (for example, a parabolic relationship), it will generally **not** be linear in the logit.

Another aspect to examine is the **range** of the continuous predictor. For example, if the predictor represents **age** and ranges from 21 to 65, it is often better to **center** or **standardize** it. These operations, along with their rationale, will be discussed later in the chapter. These two issues—linearity in the logit and range adjustment—are common sources of difficulty when dealing with continuous predictors.

---

### The Challenge of a Single Coefficient

One of the main challenges in handling continuous predictors in regression models is that a **single coefficient** represents the entire range of predictor values. Recall that a regression coefficient is essentially a **slope**—it measures the **rate of change** in the response for a one-unit change in the predictor. Logistic regression assumes that this rate of change is **constant** across the entire range of the variable. This assumption holds because the logistic **link function** linearizes the relationship between the linear predictor and the fitted value.

But what happens when the predictor is **curved**, such as a **parabolic** relationship? In such cases, analysts often **transform** the variable by including both the original and the squared term (e.g., ( x ) and ( x^2 )) in the model. Other common transformations include the **square root**, **inverse**, **inverse square**, and **logarithmic** transformations—the **log transform** being one of the most widely used.

---

### The Trade-Off: Transformation vs. Interpretability

While transformations can improve model fit, they often reduce **interpretability**. Any transformation applied to a variable must be incorporated into how we interpret its coefficient. For instance, when applying a log transformation, the interpretation changes—now, a one-unit change in the predictor corresponds to a **logarithmic change** in the response.

Analysts sometimes apply complex transformations to achieve linearity between the logit and the predictor, but later find it difficult to interpret what the resulting coefficient actually means.

---

### Assessing Linearity and Curvature

Linearizing a predictor in **logistic regression** is more challenging than in **linear regression**, where the linear predictor and fitted value are identical. Moreover, other predictors in the model may influence this relationship.

To evaluate the linearity of a continuous predictor, analysts often use:

* **Partial residual plots**, or
* **Generalized Additive Models (GAMs)**

These tools help determine the best transformation for a continuous predictor. We will discuss these diagnostic methods further in **Chapter 3**.

---

### Interpretation of a Continuous Predictor

The interpretation logic for continuous predictors is the same as for binary or categorical predictors.

* For a **binary predictor**, the **odds ratio (OR)** compares the odds when ( x = 1 ) to the odds when ( x = 0 ).
* For a **categorical predictor**, the reference level corresponds to ( x = 0 ).
* For a **continuous predictor**, the lower of two adjacent values serves as the reference (( x = 0 )), and the higher as ( x = 1 ).

For example, if **age** is the predictor, then the **odds ratio** compares the odds of death at age 21 to those at age 20:

[
\text{OR} = \frac{\text{odds}(21)}{\text{odds}(20)}
]

This ratio remains constant for all adjacent pairs of predictor values.

If the odds ratio for **age** is 1.01 and the response variable is *died*, we can conclude that the **odds of death increase by 1% for each additional year of age**.

---

### A Simple Example: Using a Generalized Additive Model (GAM)

As mentioned earlier, when including a continuous predictor in a logistic model, we assume that the **slope (rate of change)** of the response for a one-unit change in the predictor remains constant across the entire predictor range. However, if the slope changes substantially at different points, it may be preferable to **categorize (factor)** the continuous predictor at those change points.

While this approach reduces information, it can increase interpretive accuracy.

**Generalized Additive Models (GAMs)** are a widely used tool for examining the **underlying shape** of a continuous predictor, adjusted by other variables, within a **GLM family**—such as logistic regression.

Let’s take the variable **LOS** (Length of Stay, measured in nights) from the `medpar` dataset as an example. The variable ranges from 1 to 116. A **cubic spline** can be used to smooth the distribution of LOS, allowing us to visualize and assess its shape.

In R, this is done using the `s()` operator within the **mgcv** package:

```r
> summary(medpar$los)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  1.000   4.000   8.000   9.854  13.000 116.000

> library(mgcv)
> diedgam <- gam(died ~ s(los), family = binomial, data = medpar)
> summary(diedgam)
```

This GAM model smooths the effect of **length of stay (LOS)** on the probability of death, allowing us to determine whether the relationship is **linear in the logit** or if a **transformation** (or categorization) might be necessary.

---
Here’s a **succinct, clear rewrite** of your passage — preserving all the key ideas and technical content while removing redundancy and improving flow:

---

### 2.5.3 Centering

A **continuous predictor** whose minimum value is far from zero should typically be **centered** (subtracting its mean or another reference value). For example, in the `badhealth` dataset from the **COUNT** package, centering helps reduce multicollinearity and improves interpretability of the model intercept.

---

### 2.5.4 Standardization

**Standardization** is important when continuous predictors are measured on **different scales**. It is done by dividing the **centered variable** by its **standard deviation**, easily achieved in R with the `scale()` function.

Standardizing allows predictors to be compared directly, as all are expressed in **standard deviation units**. However, this makes coefficients harder to interpret in real-world terms.

The choice to standardize depends on modeling goals:

* **Interpretation-focused models** (e.g., understanding odds ratios): avoid unnecessary transformations.
* **Prediction-focused models** (e.g., forecasting probabilities): prioritize optimal transformations for best fit.

---

### 2.6.1 Basics of Model Prediction

Prediction in logistic regression works the same regardless of the number of predictors. Each predictor’s effect is adjusted for the others.

Example: predicting **death (`died`)** using the binary predictor **race (`white`)**.

```r
logit7 <- glm(died ~ white, family = binomial, data = medpar)
summary(logit7)
exp(coef(logit7))
```

Output interpretation:

* Intercept OR = 0.396
* White OR = 1.35

→ White patients have **35% higher odds of death** than nonwhite patients.
Predicted probabilities (`fitb`) show 0.283 for nonwhite and 0.349 for white patients — only two values since the predictor is binary.

Now using a **continuous predictor**, `los` (Length of Stay):

```r
logit8 <- glm(died ~ los, family = binomial, data = medpar)
summary(logit8)
exp(coef(logit8))
```

* Intercept OR = 0.696
* LOS OR = 0.97

Thus, **each additional hospital day reduces death odds by about 3%**.
Predicted probabilities (`fitc`) range from 0.02 to 0.40.

To find the death probability for a **20-day stay**:
[
\eta = -0.3617 - 0.0305(20) = -0.9713 \
\mu = \frac{1}{1 + e^{-(-0.9713)}} = 0.275
]
→ A patient hospitalized for 20 days has a **27.5% chance of death**.

---

### 2.6.2 Prediction Confidence Intervals

To compute **confidence intervals (CIs)** for predictions:

```r
lpred <- predict(logit8, type="link", se.fit=TRUE)
up <- lpred$fit + qnorm(.975) * lpred$se.fit
lo <- lpred$fit - qnorm(.975) * lpred$se.fit
```

Convert to probability scale using the **inverse logit link**:

```r
upci <- logit8$family$linkinv(up)
mu   <- logit8$family$linkinv(lpred$fit)
loci <- logit8$family$linkinv(lo)
```

Typical summaries show:

* Lower 95% CI mean ≈ 0.313
* Mean probability ≈ 0.343
* Upper 95% CI mean ≈ 0.375

Visualizing predicted probability vs. LOS:

```r
plot(medpar$los, mu)
lines(medpar$los, loci, col=2, type='p')
lines(medpar$los, upci, col=3, type='p')
```

→ The plot displays the fitted logistic curve with **95% confidence bands** showing how the probability of death changes with hospital stay length.

---
Here’s a **clear and concise explanation** of **AIC** and **BIC**, focusing on how they are used in model evaluation — especially in logistic regression and other statistical models.

---

## **AIC and BIC: Model Selection Criteria**

### **1. Akaike Information Criterion (AIC)**

**Formula:**
[
\text{AIC} = -2 \ln(L) + 2k
]

Where:

* ( L ) = maximum likelihood of the model (how well the model fits the data)
* ( k ) = number of estimated parameters in the model

**Interpretation:**

* AIC measures the **relative quality of models** by balancing **fit** and **complexity**.
* The **first term** (–2 log-likelihood) rewards good fit (larger ( L ) = smaller –2ln(L)),
  while the **second term** (+2k) penalizes for too many parameters (to discourage overfitting).
* **Lower AIC → better model**, among models fit to the same dataset.

**Usage in logistic regression:**
Used to compare multiple models (e.g., different predictors, transformations, or interaction terms).
[
\text{AIC}*{model1} < \text{AIC}*{model2} \implies model1 \text{ fits better (with parsimony)}
]

---

### **2. Bayesian Information Criterion (BIC)**

**Formula:**
[
\text{BIC} = -2 \ln(L) + k \ln(n)
]

Where:

* ( n ) = sample size
* ( k ) = number of model parameters
* ( L ) = model likelihood

**Interpretation:**

* Like AIC, BIC balances fit and complexity, but penalizes complexity **more strongly** (especially for large ( n )) because of the ( \ln(n) ) factor.
* **Lower BIC → preferred model.**
* BIC tends to favor **simpler models** more aggressively than AIC.

---

### **Comparison**

| Criterion | Penalty Term | Tends to Favor                         | Typical Use                                   |
| --------- | ------------ | -------------------------------------- | --------------------------------------------- |
| **AIC**   | ( 2k )       | Models with better predictive accuracy | When prediction/generalization is the goal    |
| **BIC**   | ( k \ln(n) ) | Simpler models (fewer predictors)      | When identifying the *true* model is the goal |

---

### **Example in R:**

```r
model1 <- glm(died ~ los, family = binomial, data = medpar)
model2 <- glm(died ~ los + age, family = binomial, data = medpar)

AIC(model1, model2)
BIC(model1, model2)
```

**Output interpretation:**

* The model with the **lower AIC/BIC** is statistically preferred.
* However, differences should be **interpreted relatively**:

  * ΔAIC < 2 → models are essentially equivalent
  * ΔAIC 4–7 → less support for higher AIC model
  * ΔAIC > 10 → model with higher AIC is not supported

---

### **Summary**

* **AIC**: Focuses on *predictive performance* (used in most applied modeling).
* **BIC**: Focuses on *model simplicity* and likelihood of being the true model.
* In logistic regression output (e.g., `summary(glm(...))`), both AIC and BIC help assess **goodness of fit** while discouraging overfitting.

---

Here’s a **succinct and structured rewrite** of your content on Pearson Chi², residuals, and related goodness-of-fit diagnostics in logistic regression:

---

## **4.1 Goodness-of-Fit in Logistic Regression**

### **4.1.1 Pearson Chi² Test**

The **Pearson Chi² statistic** is used to assess **model fit** in logistic regression:

[
\text{Pearson Chi² / Residual df} \approx 1
]

* A **well-fitted model** will have the **Pearson Chi² statistic** close to its **residual degrees of freedom (df)**.
* Values **much greater than 1** indicate potential **overdispersion** or **lack of fit**.
* Values **much less than 1** indicate underdispersion.

**Interpretation:**
[
\chi^2_{\text{Pearson}} / \text{Residual df} \approx 1 \implies \text{Good fit}
]

---

### **4.1.2 Likelihood Ratio Test (LRT)**

The **Likelihood Ratio Test** compares a **full model** with a **reduced model** (nested):

[
\text{LRT} = -2 \left( \ln L_{\text{reduced}} - \ln L_{\text{full}} \right)
]

* Large LRT values indicate that the **full model significantly improves fit** over the reduced model.
* The statistic approximately follows a **chi-square distribution** with degrees of freedom equal to the difference in the number of parameters.

---

### **4.1.3 Residual Analysis**

Residuals in logistic regression help identify **lack of fit** and **influential observations**. Common residuals include:

| Residual Type                       | Formula                                                                | Notes                                                     |
| ----------------------------------- | ---------------------------------------------------------------------- | --------------------------------------------------------- |
| **Raw** ( r )                       | ( y - \mu )                                                            | Simple difference between observed and predicted response |
| **Pearson** ( r_p )                 | ( \frac{y - \mu}{\sqrt{\mu(1-\mu)}} )                                  | Standardized by variance of Bernoulli distribution        |
| **Deviance** ( r_d )                | ( \text{sign}(y-\mu) \sqrt{2[y\ln(y/\mu) + (1-y)\ln((1-y)/(1-\mu))]} ) | Measures contribution to likelihood ratio                 |
| **Standardized Pearson / Deviance** | ( r_s = r / \sqrt{1-h} )                                               | Adjusted for **leverage** ( h )                           |
| **Anscombe** ( r_A )                | Transformation-based residual                                          | Reduces skewness for small counts                         |
| **Cook’s Distance** ( r_{CD} )      | Measures influence of an observation on all fitted values              | Uses leverage ( h ) and residuals                         |
| **Delta Pearson / Deviance / Beta** | ( \Delta \chi^2, \Delta \text{Dev}, \Delta \beta )                     | Change in statistic when a single observation is omitted  |

* Here, ( h ) = **leverage** from the hat matrix, ( p ) = number of coefficients, and ( n ) = sample size.

---

### **4.1.4 Hosmer–Lemeshow Test**

* The **Hosmer–Lemeshow (H-L) statistic** assesses **fit for grouped binary outcomes**.
* Observations are grouped into deciles of predicted probabilities.
* Compares **observed vs. expected** counts in each group using a **Chi² test**.
* **Large p-values** indicate good fit; **small p-values** suggest poor fit.

---

**Summary of Goodness-of-Fit Checks in Logistic Regression:**

1. **Pearson Chi² / Residual df ≈ 1** → Good fit
2. **Likelihood Ratio Test** → Compare nested models
3. **Residuals** → Identify misfit or influential points
4. **Hosmer–Lemeshow Test** → Assess calibration of predicted probabilities

---

Here’s a **succinct, clear rewrite** of your section on the **Binomial Probability Distribution Function (PDF)** while keeping the key formulas and concepts intact:

---

## **5.1 The Binomial Probability Distribution Function**

The **binomial probability distribution** describes the probability of ( y ) successes in ( n ) trials, each with success probability ( p ). Its probability mass function (PMF) is:

[
f(y; n, p) = \binom{n}{y} p^y (1-p)^{,n-y} \quad \text{for } y = 0, 1, \dots, n
]

In **exponential family form**, this can be expressed as:

[
f(y; n, p) = \exp \Bigg[ y \ln\frac{p}{1-p} + n \ln(1-p) + \ln \binom{n}{y} \Bigg]
]

Here:

* ( n ) = number of trials (or observations in a **covariate pattern**)
* ( y ) = observed successes
* ( p ) = probability of success

---

### **Mean and Variance**

For the binomial distribution:

[
\text{Mean: } \mu = E[Y] = np
]

[
\text{Variance: } V(Y) = np(1-p) = \mu \left(1 - \frac{\mu}{n}\right)
]

---

### **Link Function and Inverse Link**

The **canonical link function** (logit) is:

[
\eta = \ln \frac{\mu / n}{1 - \mu / n} = \ln \frac{p}{1-p}
]

The **inverse link** (expressing the mean in terms of the linear predictor ( \eta = X\beta )):

[
\mu = n \cdot \frac{e^{\eta}}{1 + e^{\eta}}
\quad \text{or} \quad
p = \frac{e^{X\beta}}{1 + e^{X\beta}}
]

---

### **Log-Likelihood Function**

For a set of observations indexed by ( i ):

[
\ell(\mu; y, n) = \sum_{i=1}^m \left[ y_i \ln\frac{\mu_i}{n_i} + (n_i - y_i) \ln\left(1 - \frac{\mu_i}{n_i}\right) + \ln \binom{n_i}{y_i} \right]
]

---

### **Deviance Statistic**

The **deviance** measures the difference between the **full model** and a **saturated model**:

[
D = 2 \sum_{i=1}^m \left[ y_i \ln\frac{y_i}{\hat{\mu}_i} + (n_i - y_i) \ln\frac{n_i - y_i}{n_i - \hat{\mu}_i} \right]
]

* Small deviance → model fits well
* Large deviance → poor fit

---

This summarizes the **binomial distribution, link functions, log-likelihood, and deviance** in a compact, practical form for logistic regression modeling.

---

Here’s a **succinct, structured rewrite** of your section on **overdispersion in binary and grouped logistic regression**:

---

## **5.2 Overdispersion in Binary and Grouped Logistic Regression**

### **Concept of Overdispersion**

* In logistic regression, the **binomial model assumes independent observations**.
* When data are **clustered or collected in panels**, correlation within clusters can exceed what is allowed by the binomial assumption.
* This extra correlation is called **overdispersion**.

> Binary response models (Bernoulli) are often thought **not to have overdispersion** because variance is fully determined by the mean:
> [
> \text{Var}(Y) = \mu(1-\mu)
> ]
> However, correlations within observations can still create extra variability.

* Analogous to **Poisson overdispersion**, which occurs when the variance exceeds the mean, **binary data can also show extra dispersion**.

---

### **Detecting and Adjusting for Overdispersion**

1. **Quasibinomial Family (R)**:

   * Adjusts standard errors for extra dispersion post hoc by scaling with the square root of the **dispersion statistic**.
   * Syntax: `glm(..., family = quasibinomial)`.

2. **Grouped Logistic Models**:

   * Binomial denominator > 1, e.g., counts of successes per cluster.
   * If dispersion > 1, extra correlation exists, similar to overdispersion in Bernoulli observations.
   * **Implicit overdispersion**: correlation within binary observations even when each observation has a binomial denominator of 1.

3. **Extra-parameter Models**:

   * To explicitly account for overdispersion, models with **additional parameters** are used:

     * **Negative binomial** (Poisson counts)
     * **Beta-binomial** (binary/grouped data)
   * These models add a **heterogeneity parameter** to account for correlation.

---

### **Identifying Overdispersion**

* **Pearson Dispersion Statistic**:

[
\hat{\phi} = \frac{\chi^2_{\text{Pearson}}}{\text{Residual df}}
]

* Interpretation:

  * ( \hat{\phi} > 1 ) → extra variability (overdispersion)
  * ( \hat{\phi} \approx 1 ) → good fit
  * ( \hat{\phi} < 1 ) → underdispersion (rare)

**Example in R:**

```r
P__disp(bin)
# Pearson Chi2 = 6.63
# Dispersion   = 3.32  # indicates overdispersion
```

* Standard errors in a quasibinomial or robust/sandwich approach are adjusted by ( \sqrt{\hat{\phi}} ).

---

### **Apparent Overdispersion**

Sometimes a dispersion > 1 is **not true overdispersion**, but due to model issues:

1. Missing predictor(s)
2. Needed interaction terms
3. Predictor transformations (e.g., log(x))
4. Misspecified link function (probit, cloglog)
5. Outliers in the data

**Guideline:**

* Check these 5 factors when ( \hat{\phi} > 1 ).

* If addressing them reduces ( \hat{\phi} \approx 1 ), the data are **not truly overdispersed**.

* Underdispersion (( \hat{\phi} < 1 )) is rare and usually handled by **scaling or robust SEs**.

---

This summary captures the **mechanics of overdispersion**, **detection**, and **adjustments** in binary and grouped logistic regression in a compact, practical way.

---
Here’s a **succinct and structured summary** of your section on **beta-binomial regression**:

---

## **5.5 Beta-Binomial Regression**

### **Concept**

* **Overdispersed binomial data** occurs when variability exceeds what the standard binomial model allows.
* **Beta-binomial regression** addresses this by assuming the binomial mean parameter, ( \mu ), is itself **beta distributed**.

  * The beta distribution has **two parameters**, allowing the model to account for **extra-binomial correlation**.
  * This makes the beta-binomial a **mixture of beta and binomial distributions**.

### **Key Formulas**

1. **Binomial PDF** (in terms of (\mu)):

[
f(y; n, \mu) = \binom{n}{y} \mu^y (1-\mu)^{n-y}
]

2. **Beta PDF** (parameters (a, b)):

[
f(y; a, b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} y^{a-1} (1-y)^{b-1}, \quad 0 < y < 1
]

3. **Beta-binomial mixture**:

[
f(y; n, a, b) = f(y|n,\mu) \cdot f(\mu|a,b)
]

4. **Mean and variance** of beta-binomial:

[
E[Y] = n\mu, \quad \text{Var}(Y) = n\mu(1-\mu)\left[1 + (n-1)\sigma\right]
]

* Here, (\sigma) is the **overdispersion/heterogeneity parameter**.

---

### **Advantages**

* Explicitly models **overdispersion** by introducing (\sigma).
* Conjugacy of **beta and binomial** simplifies estimation.
* Useful for **Bayesian and frequentist approaches**.

---

### **Example: Titanic Data (Grouped Format)**

* **Predictors**: age (adult/child), sex (male/female), class (1st, 2nd, 3rd).
* **Response**: number survived vs. number died.

#### **Grouped Logistic Regression**

```r
jhlogit <- glm(cbind(survive,died) ~ age + sex + class03, 
               data=titanicgrp, family=binomial)
```

* Dispersion statistic: **14.41 → severe overdispersion**
* Robust SEs applied with `sandwich` package to adjust standard errors.

#### **Beta-Binomial Regression (gamlss)**

```r
mybb <- gamlss(cbind(survive,died) ~ age + sex + class03, 
               data=titanicgrp, family=BB)
```

* **Improved fit**: AIC reduced from 157.77 → 85.80
* **Estimated sigma**: 0.165 (log link: `exp(-1.801)`)
* Odds ratios generally **inflated** compared to grouped logistic, but p-values similar.

| Predictor   | OR (Beta-binomial) |
| ----------- | ------------------ |
| Intercept   | 4.47               |
| Age (adult) | 0.11               |
| Sex (male)  | 0.11               |
| Class (2nd) | 7.53               |
| Class (1st) | 15.80              |

---

### **Takeaways**

* Beta-binomial regression is ideal for **grouped binary data with overdispersion**.
* Provides **more accurate standard errors** and **better model fit** than standard grouped logistic regression.
* Sigma informs how much **extra correlation/heterogeneity** exists in the data.

---
Here’s a **succinct overview of Bayesian Logistic Regression** and the methodology:

---

## **6. Bayesian Logistic Regression**

### **6.1 Bayesian Methodology Overview**

**Bayesian statistics** provides a framework for updating beliefs about parameters based on observed data. Unlike classical (frequentist) methods, Bayesian methods treat parameters as **random variables** rather than fixed but unknown quantities.

---

### **Key Concepts**

1. **Prior Distribution ((P(\theta)))**

   * Represents **beliefs about parameters** before seeing the data.
   * Can be informative (based on previous knowledge) or non-informative (flat/uniform).

2. **Likelihood ((P(D|\theta)))**

   * Probability of observing the data (D) given the parameter (\theta).
   * For logistic regression, the likelihood is the **binomial likelihood**:
     [
     L(\beta) = \prod_{i=1}^{n} \pi_i^{y_i} (1-\pi_i)^{1-y_i}, \quad
     \pi_i = \frac{1}{1 + e^{-x_i'\beta}}
     ]

3. **Posterior Distribution ((P(\theta|D)))**

   * Updated beliefs about parameters after observing data:
     [
     P(\theta|D) \propto P(D|\theta) \cdot P(\theta)
     ]
   * Combines prior and likelihood using **Bayes’ theorem**.

4. **Predictive Distribution**

   * Probability of future or unseen observations:
     [
     P(y_{\text{new}}|D) = \int P(y_{\text{new}}|\theta) P(\theta|D) d\theta
     ]

---

### **Bayesian Logistic Regression Model**

* **Model**:
  [
  y_i \sim \text{Bernoulli}(\pi_i), \quad
  \pi_i = \frac{1}{1 + e^{-x_i'\beta}}
  ]

* **Prior example**:
  (\beta_j \sim N(0, \sigma^2))

* **Posterior**:
  [
  P(\beta|y,X) \propto \prod_{i=1}^{n} \pi_i^{y_i} (1-\pi_i)^{1-y_i} \cdot \prod_{j=1}^{p} P(\beta_j)
  ]

---

### **Advantages**

1. Can include **prior knowledge**.
2. Handles **small samples or sparse data** better.
3. Provides **full posterior distributions** of coefficients → allows credible intervals.
4. Naturally accounts for **parameter uncertainty** in predictions.

---

### **Computation**

* Analytical solutions for posteriors are usually **not available**.
* Use **numerical methods** such as:

  * **Markov Chain Monte Carlo (MCMC)**
  * **Hamiltonian Monte Carlo (HMC, e.g., Stan)**
  * **Variational Inference** for approximation

---

### **Interpretation**

* Posterior mean or median → estimate of coefficient.
* **Credible interval** (e.g., 95%) → interval in which the parameter lies with 95% probability.
* Odds ratios can be computed as ( \exp(\beta) ) from posterior samples.

---

If you want, I can create a **visual step-by-step Bayesian logistic regression diagram** showing how **prior + likelihood → posterior → predictive probability**, which is extremely helpful for intuition.

Do you want me to make that diagram?


