

## ACF vs. PACF
ACF and PACF plots allow you to determine the AR and MA components of an ARIMA model. Both the Seasonal and the non-Seasonal AR and MA components can be determined from the ACF and PACF plots.
Both ACF and PACF plots show correlation coefficients at different lags, but what they measure is fundamentally different.

* **Lag:** In time series, a "lag" refers to a past time point. Lag 1 means the observation immediately preceding the current one ($Y_{t-1}$), Lag 2 means the observation two time periods ago ($Y_{t-2}$), and so on.

### 1. Autocorrelation Function (ACF)

* **What it Measures:** The ACF measures the **linear relationship (correlation)** between an observation and its lagged values, **including both direct and indirect effects**.
    * **Direct effect:** The correlation directly between $Y_t$ and $Y_{t-k}$.
    * **Indirect effect:** The correlation that is *passed through* intermediate lags. For example, the correlation between $Y_t$ and $Y_{t-2}$ might be partly due to the correlation between $Y_t$ and $Y_{t-1}$, and then $Y_{t-1}$ and $Y_{t-2}$. The ACF captures all of this.
* **Intuition:** "How much does the current value correlate with a value from $k$ periods ago, considering all the 'steps' in between?"
* **Appearance on Plot:**
    * **AR (Autoregressive) Process:** For an AR(p) process, the ACF typically **decays gradually** (either exponentially or as a dampened sine wave) as the lag increases. This is because the indirect effects from earlier lags propagate through the series.
    * **MA (Moving Average) Process:** For an MA(q) process, the ACF will typically have **significant spikes only up to lag 'q'**, and then "cut off" (drop to non-significant levels) beyond that lag. This is because an MA process is only directly dependent on a finite number of past error terms.
* **Primary Use:**
    * Identifying the order of a **Moving Average (MA)** component (q).
    * Detecting **seasonality** (spikes at seasonal lags, e.g., lag 12 for monthly data).
    * Checking for **non-stationarity** (slow decay suggests a trend).

### 2. Partial Autocorrelation Function (PACF)

* **What it Measures:** The PACF measures the **direct linear relationship (correlation)** between an observation and its lagged values, **after removing the influence of all the intermediate lags**.
    * **Intuition:** "How much does the current value correlate with a value from $k$ periods ago, *if we remove the effects of all the values in between*?"
* **Appearance on Plot:**
    * **AR (Autoregressive) Process:** For an AR(p) process, the PACF will typically have **significant spikes only up to lag 'p'**, and then "cut off" (drop to non-significant levels) beyond that lag. This is because an AR process directly depends on a finite number of past observations.
    * **MA (Moving Average) Process:** For an MA(q) process, the PACF typically **decays gradually** (either exponentially or as a dampened sine wave) as the lag increases. This is because the indirect effects from past error terms extend through the series.
* **Primary Use:**
    * Identifying the order of an **Autoregressive (AR)** component (p).

### Summary Table: ACF vs. PACF

| Feature               | Autocorrelation Function (ACF)                                  | Partial Autocorrelation Function (PACF)                                |
| :-------------------- | :-------------------------------------------------------------- | :--------------------------------------------------------------------- |
| **What it shows** | Total correlation between $Y_t$ and $Y_{t-k}$ (direct + indirect) | Direct correlation between $Y_t$ and $Y_{t-k}$ (after removing intermediate effects) |
| **AR(p) Process** | Decays gradually                                                | Cuts off after lag 'p'                                                 |
| **MA(q) Process** | Cuts off after lag 'q'                                          | Decays gradually                                                       |
| **Primary for AR/MA** | Used to identify **q** (MA order)                               | Used to identify **p** (AR order)                                      |

**How to use them for ARIMA Model Identification:**

* **If ACF cuts off and PACF decays gradually:** Suggests an MA(q) model, where q is the lag where ACF cuts off.
* **If PACF cuts off and ACF decays gradually:** Suggests an AR(p) model, where p is the lag where PACF cuts off.
* **If both ACF and PACF decay gradually:** Suggests an ARMA(p,q) model, where you might need to use information criteria (AIC, BIC) to determine p and q.
* **If both ACF and PACF show significant spikes at seasonal lags:** Indicates the presence of seasonality, suggesting a SARIMA model.

![ACF & PACF cheatsheet](https://linkedin.com/path/to/image.jpg)
https://www.linkedin.com/pulse/reading-acf-pacf-plots-missing-manual-cheatsheet-saqib-ali

Class, let's get down to the mathematical definitions behind the ACF and PACF. While software packages usually calculate these for us, understanding the formulas provides a deeper insight into what they represent.

---

## ACF and PACF Formulas

For a time series $Y_t$, we generally assume it is **weakly stationary** when calculating these functions. This means its mean ($\mu$) and variance ($\sigma^2$) are constant over time, and the covariance between two points depends only on the lag between them.

### 1. Autocorrelation Function (ACF) - $\rho_k$

The autocorrelation function at lag $k$, denoted as $\rho_k$, measures the linear correlation between a value in the series at time $t$ ($Y_t$) and a value at time $t-k$ ($Y_{t-k}$).

The formula for the theoretical ACF at lag $k$ for a **stationary** time series is:

$$\rho_k = \frac{Cov(Y_t, Y_{t-k})}{\sqrt{Var(Y_t)Var(Y_{t-k})}}$$

Since for a stationary series, $Var(Y_t) = Var(Y_{t-k}) = \sigma^2$, and $Cov(Y_t, Y_{t-k})$ is the autocovariance at lag $k$, denoted as $\gamma_k$, the formula simplifies to:

$$\rho_k = \frac{\gamma_k}{\gamma_0}$$

Where:
* $\gamma_k = Cov(Y_t, Y_{t-k}) = E[(Y_t - \mu)(Y_{t-k} - \mu)]$ is the autocovariance at lag $k$.
* $\gamma_0 = Var(Y_t) = E[(Y_t - \mu)^2]$ is the variance of the series (autocovariance at lag 0).
* $\mu = E(Y_t)$ is the mean of the time series.

**Sample ACF:** In practice, we don't have the true population mean and covariances. Instead, we estimate them from our observed data $y_1, y_2, ..., y_n$. The sample mean is $\bar{y} = \frac{1}{n} \sum_{t=1}^{n} y_t$.

The **sample autocorrelation function (ACF)** at lag $k$ is estimated as:

$$\hat{\rho}_k = \frac{\sum_{t=k+1}^{n} (y_t - \bar{y})(y_{t-k} - \bar{y})}{\sum_{t=1}^{n} (y_t - \bar{y})^2}$$

This formula calculates the sample covariance between $y_t$ and $y_{t-k}$ (numerator) and normalizes it by the total sample variance (denominator).

### 2. Partial Autocorrelation Function (PACF) - $\phi_{kk}$ (or $\phi_k$)

The PACF at lag $k$, denoted as $\phi_{kk}$ (or simply $\phi_k$), measures the correlation between $Y_t$ and $Y_{t-k}$ **after removing the linear effects of the intermediate observations** $Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1}$.

The theoretical PACF is more complex to derive directly with a single closed-form expression like the ACF. Instead, it's often defined recursively or through regression concepts.

**Intuitive Definition (using regression):**
The PACF at lag $k$, $\phi_{kk}$, can be thought of as the last coefficient in an autoregressive model of order $k$, $AR(k)$, where $Y_t$ is regressed on its $k$ lagged values:

$$Y_t = \alpha_0 + \phi_{k1} Y_{t-1} + \phi_{k2} Y_{t-2} + ... + \phi_{kk} Y_{t-k} + \epsilon_t$$

Here, $\phi_{kk}$ is the partial autocorrelation at lag $k$. For example:
* $\phi_{11} = \rho_1$ (PACF at lag 1 is just the ACF at lag 1, as there are no intermediate lags to remove).
* $\phi_{22}$ is the correlation between $Y_t$ and $Y_{t-2}$ after removing the linear effect of $Y_{t-1}$.
* $\phi_{33}$ is the correlation between $Y_t$ and $Y_{t-3}$ after removing the linear effect of $Y_{t-1}$ and $Y_{t-2}$.

**Recursive Calculation (Durbin-Levinson Algorithm):**
The PACF values can be computed recursively using the Durbin-Levinson algorithm, which relates them to the ACF values:

For $k=1$:
$$\phi_{11} = \rho_1$$

For $k > 1$:
$$\phi_{kk} = \frac{\rho_k - \sum_{j=1}^{k-1} \phi_{(k-1),j} \rho_{k-j}}{1 - \sum_{j=1}^{k-1} \phi_{(k-1),j} \rho_j}$$

Where $\phi_{k,j} = \phi_{(k-1),j} - \phi_{kk} \phi_{(k-1),(k-j)}$. This recursive definition highlights how the PACF "partials out" the effects of shorter lags.

**Sample PACF:**
Similarly, sample PACF values ($\hat{\phi}_{kk}$) are calculated by plugging in the sample ACF values ($\hat{\rho}_k$) into the Durbin-Levinson algorithm or by fitting successive AR models to the data.

While the exact calculation of PACF is more involved, the key takeaway is its conceptual meaning: it isolates the *direct* correlation at a specific lag.





---

## What is a Stationary Series?

A **stationary time series** is one whose statistical properties (such as mean, variance, and autocorrelation) **do not change over time**. In simpler terms, if you take different segments of the series, they should statistically look the same.

There are different levels of stationarity, but the most commonly referred to in time series modeling is **weak-sense stationarity** (also known as second-order stationarity). A time series is weakly stationary if it satisfies these three conditions:

1.  **Constant Mean:** The mean value of the series remains constant over time. There's no upward or downward trend.
    * $E(Y_t) = \mu$ for all $t$
2.  **Constant Variance:** The variance (and thus standard deviation) of the series remains constant over time. The spread of the data points around the mean doesn't change significantly.
    * $Var(Y_t) = \sigma^2$ for all $t$
3.  **Constant Autocovariance (or Autocorrelation):** The covariance between any two observations depends only on the time lag between them, not on the specific time at which the observations are made.
    * $Cov(Y_t, Y_{t-k}) = \gamma_k$ for all $t$ and any lag $k$.
    * This implies that the ACF and PACF plots will quickly decay to zero.

**Examples of Non-Stationary Series:**

* **Trend:** A series that consistently increases or decreases over time (e.g., population growth, stock prices). The mean is not constant.
* **Seasonality:** A series with repeating patterns at fixed intervals (e.g., retail sales peaking every December). The mean and sometimes variance change seasonally.
* **Changing Variance:** A series where the variability of the data changes over time (e.g., stock market volatility often increases during crises). The variance is not constant.

---

## Is Stationarity Mandatory?

**Yes, for many traditional time series models, stationarity is a fundamental assumption and is often mandatory for reliable analysis and forecasting.**

Here's why it's so important:

1.  **Model Assumptions:**
    * Many classical time series models, such as ARIMA (Autoregressive Integrated Moving Average) and its variants (AR, MA, ARMA, SARIMA), explicitly assume that the underlying process generating the data is stationary.
    * If you apply these models to non-stationary data, the coefficients you estimate might be biased, the standard errors incorrect, and your forecasts unreliable and misleading. The models rely on the past statistical properties holding true for the future.

2.  **Reliable Inferences:**
    * When a series is stationary, you can use past data to estimate its statistical properties (mean, variance, autocorrelations) and be confident that these estimates are representative of the process in the future.
    * With non-stationary data, the statistical properties are constantly changing, so any statistics you calculate (e.g., sample mean) will only be valid for the specific period you're observing and won't generalize to other periods or the future.

3.  **Simplified Modeling:**
    * Stationary series are much easier to model because their behavior is consistent over time. You don't have to worry about the mean or variance drifting, which simplifies the task of identifying patterns and building predictive models.

4.  **Avoiding Spurious Regressions:**
    * A significant danger of using non-stationary time series in regression analysis (even if you're not doing classical time series forecasting) is the risk of **spurious regressions**. This is when you find a statistically significant relationship between two non-stationary variables, but that relationship is purely coincidental and has no real underlying economic or causal meaning.

**Deviation Allowed?**

While strict stationarity (where the entire probability distribution remains constant) is rarely seen in real-world data, **weak-sense stationarity is the practical goal.**

Deviations from stationarity (like trends or seasonality) *are not inherently "allowed"* for most models. Instead, these deviations must be **removed or accounted for** through transformations to achieve stationarity. This process is called **"stationarization."**

**Common Techniques to Achieve Stationarity:**

* **Differencing:** This is the most common technique. It involves subtracting the previous observation from the current observation.
    * **First-order differencing:** $Y'_t = Y_t - Y_{t-1}$ (removes linear trends).
    * **Seasonal differencing:** $Y'_t = Y_t - Y_{t-L}$ (removes seasonality of period L, e.g., $L=12$ for monthly data).
* **Logarithmic Transformation:** Used to stabilize variance when the variability of the series increases with its level.
* **Detrending:** Removing a deterministic trend (e.g., fitting a regression line to the data and using the residuals). This is for "trend-stationary" series.
* **De-seasonalizing:** Removing the seasonal component (e.g., using seasonal decomposition).

The "Integrated" (I) component in ARIMA models refers precisely to the differencing required to make the series stationary. An ARIMA(p,d,q) model applies 'd' orders of differencing to the series to achieve stationarity before fitting AR and MA components.

---

## Interview Questions and Answers on Stationarity

Here are some common interview questions related to stationarity, along with concise answers you can provide:

1.  **Q: What is stationarity in the context of time series analysis, and why is it important?**
    * **A:** Stationarity means that the statistical properties of a time series (mean, variance, and autocorrelation structure) remain constant over time. It's important because many traditional time series models (like ARIMA) assume stationarity to ensure reliable parameter estimation, valid statistical inference, and accurate forecasting. Non-stationary data can lead to misleading results.

2.  **Q: What are the key characteristics of a stationary time series?**
    * **A:** A weakly stationary series has a constant mean, constant variance, and an autocorrelation structure that depends only on the lag, not on the specific time point. This means no clear trends, no seasonality, and no changes in the spread of the data over time.

3.  **Q: How can you check for stationarity in a time series?**
    * **A:**
        * **Visual Inspection:** Plotting the series and looking for trends, seasonality, or changing variance.
        * **ACF/PACF Plots:** A non-stationary series often has a slow decay in its ACF plot (indicating a trend) or significant spikes at seasonal lags.
        * **Statistical Tests (Unit Root Tests):**
            * **Augmented Dickey-Fuller (ADF) Test:** Null Hypothesis: The series has a unit root (is non-stationary). A small p-value (e.g., < 0.05) suggests rejecting the null, meaning the series is likely stationary.
            * **Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test:** Null Hypothesis: The series is stationary. A small p-value suggests rejecting the null, meaning the series is likely non-stationary. (Note: These two tests are complementary and sometimes give conflicting results, requiring careful interpretation).

4.  **Q: What are common ways to make a non-stationary series stationary?**
    * **A:** The most common method is **differencing**, which involves subtracting a lagged value from the current observation (e.g., $Y_t - Y_{t-1}$ for first-order differencing, or $Y_t - Y_{t-L}$ for seasonal differencing). Other methods include **logarithmic transformations** (to stabilize variance) or **detrending/deseasonalizing** (removing deterministic components).

5.  **Q: Explain the concept of "integrated" (I) in ARIMA models.**
    * **A:** The "I" stands for "Integrated" and refers to the number of times the raw observations are differenced to make the time series stationary. An ARIMA(p,d,q) model means the series was differenced 'd' times before the Autoregressive (AR) and Moving Average (MA) components were applied.

6.  **Q: Can you apply regression models directly to non-stationary time series? What are the risks?**
    * **A:** While you *can* technically run a regression, it's generally ill-advised due to the risk of **spurious regressions**. This means you might find a strong, statistically significant relationship between two non-stationary variables that is purely coincidental and has no real-world meaning. The standard assumptions of OLS regression (like independent errors) are often violated with non-stationary data, leading to invalid inferences.

Alright class, let's get into the nuances of stationarity. While we've discussed weak-sense stationarity extensively, it's important to understand that there's a stronger, more theoretical concept called **strict stationarity**.

---

## Strict vs. Weak Stationarity

The distinction between strict and weak stationarity lies in how comprehensive their conditions are regarding the statistical properties of a time series.

### 1. Weak-Sense Stationarity (or Wide-Sense Stationarity, Covariance Stationarity, Second-Order Stationarity)

As we've discussed, this is the practical definition of stationarity used in most time series modeling.

* **Conditions:** A time series $Y_t$ is weakly stationary if it satisfies:
    1.  **Constant Mean:** $E(Y_t) = \mu$, for all $t$. The expected value of the series is constant over time.
    2.  **Constant Variance:** $Var(Y_t) = \sigma^2 < \infty$, for all $t$. The variance of the series is finite and constant over time.
    3.  **Covariance Depends Only on Lag:** $Cov(Y_t, Y_{t-k}) = \gamma_k$, for all $t$ and any lag $k$. The covariance between any two observations depends only on the time difference ($k$) between them, not on the absolute time ($t$). This implies constant autocorrelation.

* **Focus:** This definition focuses only on the **first two moments** (mean and variance) and the second-order property of the covariance.

* **Practicality:** It's a much more **tractable and testable** assumption for real-world data. It's the standard assumption for models like ARIMA.

### 2. Strict-Sense Stationarity (or Strong Stationarity)

This is a much more rigorous and theoretical definition.

* **Condition:** A time series $Y_t$ is strictly stationary if the **joint probability distribution** of any set of observations remains unchanged by a shift in time.
    * Formally, for any collection of time points $t_1, t_2, ..., t_K$ and for any time shift $\tau$, the joint distribution of $(Y_{t_1}, Y_{t_2}, ..., Y_{t_K})$ is the same as the joint distribution of $(Y_{t_1+\tau}, Y_{t_2+\tau}, ..., Y_{t_K+\tau})$.

* **Focus:** This definition covers **all moments** of the distribution (mean, variance, skewness, kurtosis, and all higher-order moments) and their relationships. It means the entire stochastic process is statistically identical over any time window of the same length.

* **Implication:** If a time series is strictly stationary, it implies that it is also weakly stationary, *provided that its first two moments (mean and variance) are finite*. The converse is not necessarily true: a weakly stationary series is not always strictly stationary.

* **Real-world Applicability:** Strict stationarity is a very **strong and often unrealistic assumption** for real-world time series. It's difficult to verify in practice because it requires knowing the full joint probability distribution, which is rarely possible.

### Key Differences Summarized:

| Feature           | Weak-Sense Stationarity                                           | Strict-Sense Stationarity                                            |
| :---------------- | :---------------------------------------------------------------- | :------------------------------------------------------------------- |
| **Conditions** | Constant mean, constant finite variance, covariance depends only on lag. | Joint probability distribution is invariant to time shifts.          |
| **Focus** | First and second moments (mean, variance, covariance).            | All moments and the entire probability distribution.                 |
| **Implication** | Does *not* necessarily imply strict stationarity.                 | Implies weak stationarity (if mean and variance are finite).         |
| **Practicality** | **Most commonly used and testable** in applied time series analysis. | **Highly theoretical**, rarely perfectly met by real-world data.     |
| **Modeling Basis**| Basis for ARIMA, AR, MA, ARMA models.                           | Less direct application in model selection for most standard models. |

**Think of it this way:**

* **Weak stationarity** is like saying "the average behavior and spread of the data are consistent over time."
* **Strict stationarity** is like saying "the entire *fingerprint* of the data's behavior, in every statistical aspect, is identical at any point in time."

In the vast majority of time series applications, especially in econometrics and practical forecasting, when we talk about "stationarity," we are referring to **weak-sense stationarity**. The goal of transformations like differencing is to achieve weak-sense stationarity, as this is sufficient for the validity of most widely used time series models.


https://online.stat.psu.edu/stat510/Lesson01 - ploys
