import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import make_scorer, recall_score, confusion_matrix

# Define your data and split it into features (X) and target labels (y).
# X, y = ...

# Create a function to calculate False Positive % (Noise Reduction) from the confusion matrix.
def calculate_false_positive_rate(confusion_matrix):
    tn, fp, fn, tp = confusion_matrix.ravel()
    false_positive_rate = fp / (fp + tn)
    return false_positive_rate

# Define the parameters you want to tune and their respective values in a dictionary format.
param_grid = {
    'max_depth': [10, 20, 30, 40],
    'n_estimators': [50, 100, 150, 200],
}

# Create a custom scoring function to optimize for recall while minimizing false positives.
scoring = {
    'Recall': make_scorer(recall_score),
    'FalsePositiveRate': make_scorer(calculate_false_positive_rate, greater_is_better=False)
}

# Create a Random Forest classifier and use GridSearchCV to perform hyperparameter tuning.
rf_classifier = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(
    rf_classifier, param_grid, scoring=scoring, refit='Recall', cv=5
)

grid_search.fit(X, y)

# Extract the results and store them in a DataFrame.
results_df = pd.DataFrame(grid_search.cv_results_)[['param_max_depth', 'param_n_estimators', 'mean_test_Recall', 'mean_test_FalsePositiveRate']]
results_df.rename(columns={'param_max_depth': 'max_depth', 'param_n_estimators': 'n_est', 'mean_test_Recall': 'Recall', 'mean_test_FalsePositiveRate': 'FalsePositiveRate'}, inplace=True)

# Now, results_df contains the hyperparameter combinations, Recall, and False Positive %.
