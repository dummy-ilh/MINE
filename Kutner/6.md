Chapter 6, "Multiple Regression I," marks a significant step forward from simple linear regression by allowing the inclusion of multiple predictor variables in the model. It heavily leverages the matrix algebra introduced in Chapter 5 to express and analyze these more complex relationships compactly and efficiently.

---

# Chapter 6: Multiple Regression I

## 6.1 Multiple Regression Models (Page 214)

### Need for Several Predictor Variables (Page 214)
In many real-world scenarios, a single independent variable is insufficient to explain or predict the variation in a dependent variable. The "need for several predictor variables" arises when the phenomenon under study is influenced by multiple factors working simultaneously. For instance, predicting house prices often requires considering square footage, number of bedrooms, location, age, etc., rather than just square footage alone. Multiple regression allows for a more comprehensive and realistic modeling of such complex relationships.

### First-Order Model with Two Predictor Variables (Page 215)
This is the simplest form of multiple linear regression. A first-order model implies that the relationship between the response variable and each predictor variable is linear.
The model equation is:
$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i$
Where:
* $Y_i$: The dependent variable for the $i$-th observation.
* $X_{i1}$, $X_{i2}$: The two predictor (independent) variables for the $i$-th observation.
* $\beta_0$: The intercept (mean of $Y$ when $X_1=0$ and $X_2=0$).
* $\beta_1$: The partial regression coefficient for $X_1$. It represents the change in the mean of $Y$ for a one-unit increase in $X_1$, *holding $X_2$ constant*.
* $\beta_2$: The partial regression coefficient for $X_2$. It represents the change in the mean of $Y$ for a one-unit increase in $X_2$, *holding $X_1$ constant*.
* $\epsilon_i$: The random error term for the $i$-th observation.

### First-Order Model with More than Two Predictor Variables (Page 217)
The concept extends directly to $p-1$ predictor variables.
The model equation becomes:
$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \dots + \beta_{p-1} X_{i,p-1} + \epsilon_i$
Here, $p-1$ represents the number of predictor variables, and $p$ represents the total number of parameters (including the intercept $\beta_0$). Each $\beta_k$ (for $k=1, \dots, p-1$) is a partial regression coefficient, indicating the change in $E\{Y\}$ for a one-unit increase in $X_k$, holding all other predictor variables constant.

### General Linear Regression Model (Page 217)
The term "General Linear Regression Model" is crucial. It describes any model that is **linear in its parameters**, even if it includes non-linear transformations of the predictor variables or interaction terms. This means it can represent a wide variety of relationships.
Examples of models falling under the general linear regression model:
* **Polynomial regression:** $Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \epsilon_i$ (Here $X_1 = X$ and $X_2 = X^2$, still linear in $\beta$'s).
* **Interaction terms:** $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1}X_{i2} + \epsilon_i$.
* **Dummy variables:** For categorical predictors (e.g., $Y_i = \beta_0 + \beta_1 \text{Gender}_i + \beta_2 \text{Age}_i + \epsilon_i$, where Gender is 0 or 1).
* **Transformed variables:** $Y_i = \beta_0 + \beta_1 \log(X_{i1}) + \beta_2 \sqrt{X_{i2}} + \epsilon_i$.

The key is that the $\beta$ coefficients are multiplied by known constants or functions of $X$, and the terms are added.

## 6.2 General Linear Regression Model in Matrix Terms (Page 222)

The elegance of matrix algebra truly shines here. The general linear regression model, regardless of the number of predictors or the form of the predictor variables (as long as it's linear in parameters), can be written compactly as:
$\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$

Where for $n$ observations and $p$ parameters ($\beta_0, \beta_1, \dots, \beta_{p-1}$):
* $\mathbf{Y}$ is an $n \times 1$ vector of observed response values:
    $\mathbf{Y} = \begin{pmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{pmatrix}$
* $\mathbf{X}$ is an $n \times p$ **design matrix** (or model matrix). The first column is typically all ones (for the intercept $\beta_0$). Subsequent columns correspond to the predictor variables (or transformations/interactions of them).
    $\mathbf{X} = \begin{pmatrix} 1 & X_{11} & X_{12} & \dots & X_{1,p-1} \\ 1 & X_{21} & X_{22} & \dots & X_{2,p-1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & X_{n1} & X_{n2} & \dots & X_{n,p-1} \end{pmatrix}$
* $\boldsymbol{\beta}$ is a $p \times 1$ vector of unknown regression parameters:
    $\boldsymbol{\beta} = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_{p-1} \end{pmatrix}$
* $\boldsymbol{\epsilon}$ is an $n \times 1$ vector of random error terms:
    $\boldsymbol{\epsilon} = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{pmatrix}$

**Assumptions in Matrix Terms (for inference):**
1.  $E\{\boldsymbol{\epsilon}\} = \mathbf{0}$ (mean of errors is zero).
2.  $Var\{\boldsymbol{\epsilon}\} = \sigma^2 \mathbf{I}_{n \times n}$ (errors have constant variance $\sigma^2$ and are uncorrelated, where $\mathbf{I}$ is the identity matrix). This implies homoscedasticity and independence.
3.  $\boldsymbol{\epsilon}$ follows a multivariate normal distribution.
4.  The $\mathbf{X}$ matrix is fixed and of full column rank (no perfect multicollinearity).

## 6.3 Estimation of Regression Coefficients (Page 223)

The method of **least squares** is used to estimate the regression parameters $\boldsymbol{\beta}$. The objective is to find the vector $\mathbf{b}$ that minimizes the sum of squared errors, $SSE = \boldsymbol{\epsilon}^T \boldsymbol{\epsilon} = (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})^T (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})$.

The least squares estimator for the regression coefficients $\boldsymbol{\beta}$ is given by:
$\mathbf{b} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$
This formula directly generalizes from simple linear regression. The term $(\mathbf{X}^T \mathbf{X})^{-1}$ requires that the matrix $\mathbf{X}^T \mathbf{X}$ is invertible, which means there's no perfect multicollinearity (no exact linear relationships between the predictor variables).

## 6.4 Fitted Values and Residuals (Page 224)

Once $\mathbf{b}$ is estimated, we can calculate the fitted values and residuals for each observation.

* **Fitted Values ($\hat{\mathbf{Y}}$):** The vector of predicted response values for the observed data points.
    $\hat{\mathbf{Y}} = \mathbf{X}\mathbf{b}$
    Substituting the formula for $\mathbf{b}$:
    $\hat{\mathbf{Y}} = \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$
    The matrix $\mathbf{H} = \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T$ is known as the **Hat Matrix**. It projects the observed $\mathbf{Y}$ onto the column space of $\mathbf{X}$, giving the fitted values. So, $\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}$. The hat matrix is symmetric and idempotent ($\mathbf{H}^2 = \mathbf{H}$).

* **Residuals ($\mathbf{e}$):** The vector of differences between the observed and fitted values.
    $\mathbf{e} = \mathbf{Y} - \hat{\mathbf{Y}}$
    Using the Hat Matrix:
    $\mathbf{e} = \mathbf{Y} - \mathbf{H}\mathbf{Y} = (\mathbf{I} - \mathbf{H})\mathbf{Y}$
    The matrix $(\mathbf{I} - \mathbf{H})$ is also symmetric and idempotent. The sum of residuals ($\sum e_i$) is 0 when the model includes an intercept.

## 6.5 Analysis of Variance Results (Page 225)

The ANOVA framework for partitioning total variability remains the same as in simple linear regression, but the calculations are generalized using matrix terms.

### Sums of Squares and Mean Squares (Page 225)
* **Total Sum of Squares (SST):** Measures the total variation in the dependent variable.
    $SST = \mathbf{Y}^T \mathbf{Y} - \frac{1}{n}(\mathbf{1}^T \mathbf{Y})^2$
    Degrees of freedom: $n-1$.
* **Regression Sum of Squares (SSR):** Measures the variation in $Y$ explained by the regression model.
    $SSR = \mathbf{b}^T \mathbf{X}^T \mathbf{Y} - \frac{1}{n}(\mathbf{1}^T \mathbf{Y})^2$
    Degrees of freedom: $p-1$ (number of predictor variables).
* **Error Sum of Squares (SSE):** Measures the unexplained variation (residual variation).
    $SSE = \mathbf{e}^T \mathbf{e} = \mathbf{Y}^T \mathbf{Y} - \mathbf{b}^T \mathbf{X}^T \mathbf{Y}$
    Degrees of freedom: $n-p$ (number of observations minus number of parameters).
* Relationship: $SST = SSR + SSE$.
* **Mean Squares:**
    * Mean Square Regression (MSR) = $SSR / (p-1)$
    * Mean Square Error (MSE) = $SSE / (n-p) = s^2$ (This is the unbiased estimator of the error variance $\sigma^2$).

### F Test for Regression Relation (Page 226)
This overall F-test assesses whether there is a significant linear relationship between the dependent variable $Y$ and *at least one* of the predictor variables.
* **Null Hypothesis ($H_0$):** All regression coefficients (excluding the intercept) are zero ($\beta_1 = \beta_2 = \dots = \beta_{p-1} = 0$). This implies none of the predictor variables linearly explain variation in $Y$.
* **Alternative Hypothesis ($H_a$):** At least one $\beta_k \neq 0$ (for $k=1, \dots, p-1$). This implies at least one predictor variable is useful.
* **Test Statistic:** $F^* = MSR / MSE$.
* **Distribution:** Under $H_0$, $F^*$ follows an F-distribution with $(p-1)$ numerator and $(n-p)$ denominator degrees of freedom: $F(p-1, n-p)$.
* **Decision:** If $F^*$ is greater than the critical F-value (or p-value is small), reject $H_0$, concluding that the regression model is statistically significant.

### Coefficient of Multiple Determination ($R^2$) (Page 226)
* **Definition:** $R^2 = SSR / SST = 1 - (SSE / SST)$.
* **Interpretation:** It represents the proportion of the total variation in the dependent variable $Y$ that is explained by the set of all predictor variables in the regression model. $0 \le R^2 \le 1$. A higher $R^2$ indicates a better fit of the model to the data.
* **Adjusted R-squared:** A common refinement, especially in multiple regression, is the adjusted $R^2$. It penalizes the inclusion of additional predictor variables that do not significantly improve the model's fit, providing a more honest assessment of explanatory power.

### Coefficient of Multiple Correlation ($R$) (Page 227)
* **Definition:** $R = \sqrt{R^2}$.
* **Interpretation:** It is the correlation coefficient between the observed $Y$ values and the fitted $\hat{Y}$ values from the multiple regression model. It indicates the strength of the linear association between the dependent variable and the entire set of predictor variables. $0 \le R \le 1$.

## 6.6 Inferences about Regression Parameters (Page 227)

Once the overall model significance is established, we often want to examine the significance of individual predictor variables.

### Interval Estimation of $\beta_k$ (Page 228)
A confidence interval for an individual regression coefficient $\beta_k$ is constructed as:
$b_k \pm t(1-\alpha/2; n-p) s\{b_k\}$
* $b_k$: The estimated coefficient for $X_k$.
* $s\{b_k\}$: The estimated standard error of $b_k$. This value is derived from the diagonal elements of the estimated variance-covariance matrix of the regression coefficients: $s^2(\mathbf{X}^T \mathbf{X})^{-1}$. Specifically, $s\{b_k\} = \sqrt{[s^2(\mathbf{X}^T \mathbf{X})^{-1}]_{kk}}$, where $[s^2(\mathbf{X}^T \mathbf{X})^{-1}]_{kk}$ is the $k$-th diagonal element.
* $t(1-\alpha/2; n-p)$: The critical value from the t-distribution with $n-p$ degrees of freedom.

### Tests for $\beta_k$ (Page 228)
To test if an individual predictor variable $X_k$ is linearly related to $Y$ (after accounting for other predictors in the model):
* **Null Hypothesis ($H_0$):** $\beta_k = 0$ (The predictor $X_k$ has no linear effect on $Y$, holding other predictors constant).
* **Alternative Hypothesis ($H_a$):** $\beta_k \neq 0$.
* **Test Statistic:** $t^* = b_k / s\{b_k\}$.
* **Distribution:** Under $H_0$, $t^*$ follows a t-distribution with $n-p$ degrees of freedom.

### Joint Inferences (Page 228)
Similar to Chapter 4, joint inferences are necessary when making statements about *multiple* parameters simultaneously.
* **Joint Confidence Intervals:** For example, using the Bonferroni procedure, we can construct simultaneous confidence intervals for several $\beta_k$ parameters. Each interval would use a critical t-value based on $1-\alpha/(2g)$ (where $g$ is the number of simultaneous intervals).
* **Joint Tests:** More complex F-tests can be used to test hypotheses about subsets of regression coefficients (e.g., $H_0: \beta_1 = \beta_2 = 0$). This allows testing if a group of predictors is jointly significant. (More extensively covered in Chapter 7).

## 6.7 Estimation of Mean Response and Prediction of New Observation (Page 229)

These concepts also generalize from simple linear regression, but now involve a vector of predictor values for the new observation.

Let $\mathbf{X}_h$ be a $1 \times p$ row vector of new predictor values (including the 1 for the intercept): $\mathbf{X}_h = \begin{pmatrix} 1 & X_{h1} & X_{h2} & \dots & X_{h,p-1} \end{pmatrix}$.

* **Estimated Mean Response ($\hat{Y}_h$):**
    $\hat{Y}_h = \mathbf{X}_h \mathbf{b}$

### Interval Estimation of $E\{Y_h\}$ (Page 229)
A confidence interval for the mean response $E\{Y_h\}$ at $\mathbf{X}_h$:
$\hat{Y}_h \pm t(1-\alpha/2; n-p) s\{\hat{Y}_h\}$
* $s\{\hat{Y}_h\} = \sqrt{s^2 \mathbf{X}_h (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}_h^T}$ (Note: $\mathbf{X}_h$ is a row vector, so $\mathbf{X}_h^T$ is a column vector).

### Confidence Region for Regression Surface (Page 229)
This is the multivariate extension of the Working-Hotelling confidence band. It provides a region that contains the entire true regression surface with a specified overall confidence level. It's an elliptical (or hyper-elliptical) region in the parameter space.

### Simultaneous Confidence Intervals for Several Mean Responses (Page 230)
Similar to $\beta_k$s, if you need simultaneous confidence intervals for $E\{Y_h\}$ at multiple $\mathbf{X}_h$ points, the Bonferroni procedure can be used:
$\hat{Y}_h \pm t(1-\alpha/(2g); n-p) s\{\hat{Y}_h\}$
Where $g$ is the number of mean responses being estimated simultaneously.

### Prediction of New Observation $Y_{h(new)}$ (Page 230)
A prediction interval for a single new observation $Y_{h(new)}$ at $\mathbf{X}_h$:
$\hat{Y}_h \pm t(1-\alpha/2; n-p) s_{pred}$
* $s_{pred} = \sqrt{s^2 [1 + \mathbf{X}_h (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}_h^T]}$

### Prediction of Mean of $m$ New Observations at $\mathbf{X}_h$ (Page 230)
If predicting the mean of $m$ new observations at $\mathbf{X}_h$:
$\hat{Y}_h \pm t(1-\alpha/2; n-p) s_{pred, m}$
* $s_{pred, m} = \sqrt{s^2 [\frac{1}{m} + \mathbf{X}_h (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}_h^T]}$ (As $m \to \infty$, this approaches the confidence interval for the mean response).

### Predictions of $g$ New Observations (Page 231)
For $g$ simultaneous prediction intervals for new observations, the Bonferroni procedure is typically used, similar to simultaneous mean responses.

### Caution about Hidden Extrapolations (Page 231)
In multiple regression, it's easy to make a "hidden extrapolation." An $\mathbf{X}_h$ vector might have individual $X_{hj}$ values within the range of observed values for each respective predictor, but the *combination* of these $X_{hj}$ values might be far outside the range of observed combinations in the original data. This means you're extrapolating in a multi-dimensional space, and predictions can be highly unreliable. This is visually harder to detect than in simple regression.

## 6.8 Diagnostics and Remedial Measures (Page 232)

The principles of diagnostics from Chapter 3 are equally, if not more, crucial in multiple regression. Violations of assumptions can be harder to spot and have more complex consequences.

* **Scatter Plot Matrix:** A matrix of scatter plots showing the relationship between each pair of predictor variables, and between each predictor and the response variable. Helps visualize pairwise correlations and detect gross non-linearity.
* **Three-Dimensional Scatter Plots:** Useful for visualizing relationships with two predictors and one response, but limited to three dimensions.
* **Residual Plots:**
    * **Residuals vs. Fitted Values ($\hat{Y}$):** Essential for checking constant variance and linearity.
    * **Residuals vs. Individual Predictors ($X_k$):** Helpful for detecting specific non-linearity or heteroscedasticity related to a single predictor.
    * **Residuals vs. Omitted Variables:** If you suspect an important variable is missing, plotting residuals against it can show if a pattern exists, indicating its importance.
    * **Residuals vs. Time Order:** For detecting autocorrelation.
* **Formal Tests:**
    * **Correlation Test for Normality (e.g., Shapiro-Wilk, Anderson-Darling):** Applied to the residuals to check the normality assumption.
    * **Brown-Forsythe Test for Constancy of Error Variance:** Robust test for heteroscedasticity across groups of residuals (e.g., grouped by predictor values).
    * **Breusch-Pagan Test for Constancy of Error Variance:** A more general test for heteroscedasticity, checking if variance is related to predictor variables.
    * **F Test for Lack of Fit:** Applicable if there are replicated observations in the multi-dimensional predictor space. Tests the appropriateness of the chosen functional form.
* **Remedial Measures:** The same remedial measures apply as in simple linear regression:
    * **Transformations:** Of $Y$ (for linearity, variance, normality) or $X$'s (for linearity).
    * **Weighted Least Squares (WLS):** To address heteroscedasticity.
    * **Robust Standard Errors:** To provide valid inferences in the presence of heteroscedasticity without changing point estimates.
    * **Dealing with Outliers and Influential Points:** Using measures like Cook's distance, DFFITS, DFBETAS to identify and potentially address unusual observations.

## 6.9 An Example - Multiple Regression with Two Predictor Variables (Page 236)

This section provides a practical, step-by-step example to illustrate the concepts discussed in the chapter. It walks through the process of:
* **Setting:** Defining the problem and collecting data.
* **Basic Calculations:** Performing the matrix multiplications $(\mathbf{X}^T \mathbf{X})$, $(\mathbf{X}^T \mathbf{X})^{-1}$, and $\mathbf{X}^T \mathbf{Y}$ to prepare for parameter estimation.
* **Estimated Regression Function:** Calculating $\mathbf{b} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$ to get the specific regression equation.
* **Fitted Values and Residuals:** Calculating $\hat{\mathbf{Y}}$ and $\mathbf{e}$.
* **Analysis of Appropriateness of Model:** Performing the diagnostic checks (residual plots, formal tests).
* **Analysis of Variance:** Constructing the ANOVA table, calculating SSR, SSE, SST, MSR, MSE, $R^2$, and performing the overall F-test.
* **Estimation of Regression Parameters:** Calculating standard errors for $b_k$ and constructing confidence intervals and t-tests for individual coefficients.
* **Estimation of Mean Response:** Calculating $\hat{Y}_h$ and its confidence interval for specific $\mathbf{X}_h$ values.
* **Prediction Limits for New Observations:** Calculating prediction intervals for new $Y$ values at specified $\mathbf{X}_h$ values.

This practical example consolidates the theoretical concepts by showing how they are applied in a real-world scenario.
