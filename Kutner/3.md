This chapter is crucial for applying regression effectively, as it focuses on verifying the assumptions of the linear model and addressing issues when those assumptions are violated.

---

# Chapter 3: Diagnostics and Remedial Measures

The validity of inferences (confidence intervals, hypothesis tests) in linear regression depends critically on the underlying model assumptions (linearity, constant variance, independence, normality of errors). **Diagnostics** are techniques used to detect departures from these assumptions, while **remedial measures** are actions taken to correct or mitigate the impact of such departures.

## 3.1 Diagnostics for Predictor Variable (Page 100)

Before even looking at residuals, it's good practice to examine the independent variable(s) ($X$) themselves.
* **Histograms or Box Plots of X:** Reveal the distribution of $X$. Are there any extreme values (outliers) in the $X$ space? Is the distribution skewed?
* **Scatter Plot of Y vs. X:** This is the most fundamental diagnostic for simple linear regression. It immediately helps visualize the nature of the relationship (linear, non-linear), the spread of $Y$ for different $X$ values, and potential outliers.

**Importance:** Outliers in $X$ (known as high-leverage points) can exert undue influence on the regression line, even if their corresponding $Y$ value is not an outlier.

## 3.2 Residuals (Page 102)

**Residuals ($e_i$)** are the differences between the observed values $Y_i$ and the fitted (predicted) values $\hat{Y}_i$:
$e_i = Y_i - \hat{Y}_i$

Residuals are the empirical counterparts of the unobservable error terms ($\epsilon_i$). Analysis of residuals helps diagnose whether the assumptions about the error terms are met.

### Properties of Residuals

Least squares residuals have some mathematical properties:
* **Sum of Residuals is Zero:** $\sum_{i=1}^{n} e_i = 0$. This is a direct consequence of the least squares estimation process.
* **Sum of Residuals Weighted by X is Zero:** $\sum_{i=1}^{n} X_i e_i = 0$. This means the residuals are uncorrelated with the independent variable.
* **Sum of Residuals Weighted by Fitted Values is Zero:** $\sum_{i=1}^{n} \hat{Y}_i e_i = 0$. This means the residuals are uncorrelated with the fitted values.

These properties are mathematical consequences of the least squares fitting procedure and are not diagnostics in themselves.

### Semistudentized Residuals (Page 103)

Raw residuals ($e_i$) can be difficult to interpret directly because their magnitude depends on the units of $Y$ and the overall scale of variation. **Semistudentized residuals** standardize the raw residuals, making them easier to compare and interpret.

**Formula:** $e_i^* = \frac{e_i}{s}$
Where $s = \sqrt{MSE}$ is the estimated standard deviation of the error terms.

**Purpose:** Semistudentized residuals give a rough indication of how many estimated standard deviations an observation is away from the fitted regression line. Values typically falling outside $\pm 2$ or $\pm 3$ warrant closer inspection as potential outliers.

### Departures from Model to Be Studied by Residuals (Page 103)

Residual analysis is the primary tool for detecting the following departures from the simple linear regression model assumptions:
* Nonlinearity of the regression function
* Nonconstancy of error variance (heteroscedasticity)
* Presence of outliers
* Nonindependence of error terms (autocorrelation)
* Nonnormality of error terms
* Omission of important predictor variables

## 3.3 Diagnostics for Residuals (Page 103)

This section focuses on using various plots involving residuals to visually detect model departures.

### Nonlinearity of Regression Function (Page 104)

* **Plot:** **Residuals ($e_i$) vs. Fitted Values ($\hat{Y}_i$)** or **Residuals ($e_i$) vs. Independent Variable ($X_i$)**.
* **Pattern indicating nonlinearity:** If the linear model is appropriate, residuals should scatter randomly around zero with no discernible pattern. A distinct **curvilinear pattern** (e.g., U-shape, inverted U-shape, S-shape) indicates that the relationship between $X$ and $Y$ is not linear and a linear model is misspecified.
* **Implication:** The linear model does not capture the true functional form of the relationship. Coefficient estimates may be biased.

### Nonconstancy of Error Variance (Heteroscedasticity) (Page 107)

* **Plot:** **Residuals ($e_i$) vs. Fitted Values ($\hat{Y}_i$)** or **Residuals ($e_i$) vs. Independent Variable ($X_i$)**.
* **Pattern indicating heteroscedasticity:** The spread (variance) of residuals should be constant across all levels of $\hat{Y}$ or $X$. A **funnel shape** (residuals "fanning out" or "fanning in") or other systematic changes in spread indicate nonconstant variance.
* **Implication:** Standard errors of the regression coefficients are biased (usually underestimated), leading to incorrect confidence intervals and p-values. Inferences about the significance of predictors become unreliable.

### Presence of Outliers (Page 108)

* **Plot:** **Semistudentized Residuals ($e_i^*$) vs. Fitted Values ($\hat{Y}_i$)** or **Index Plot of Residuals** (residuals plotted against their observation number).
* **Pattern indicating outliers:** One or a few residuals that are exceptionally large in magnitude (e.g., beyond $\pm 2$ or $\pm 3$) compared to the others.
* **Implication:** Outliers can disproportionately influence the estimated regression line, potentially pulling the line away from the majority of the data points and distorting coefficient estimates and inferences.

### Nonindependence of Error Terms (Autocorrelation) (Page 108)

* **Plot:** **Residuals ($e_i$) vs. Time Order of Observations** (if data is collected sequentially or over time).
* **Pattern indicating nonindependence:** Residuals should be randomly scattered without any discernible pattern related to their order. A **cyclical, wavy, or trending pattern** suggests autocorrelation (residuals are correlated with each other).
    * **Positive autocorrelation:** Consecutive residuals tend to have the same sign (e.g., positive residual followed by positive).
    * **Negative autocorrelation:** Consecutive residuals tend to alternate signs (e.g., positive residual followed by negative).
* **Implication:** Standard errors are typically underestimated, leading to artificially narrow confidence intervals and inflated t-statistics/F-statistics. This makes it more likely to declare predictors significant when they are not.

### Nonnormality of Error Terms (Page 110)

* **Plot:** **Normal Probability Plot (or Q-Q Plot) of Residuals**. A **Histogram of Residuals** can also be used as a supplementary diagnostic.
* **Pattern indicating nonnormality:** On a normal probability plot, if the errors are normally distributed, the plotted points should fall approximately along a straight line. Departures from a straight line (e.g., an S-shape, a heavy tail, a light tail) indicate nonnormality. A skewed or non-bell-shaped histogram also suggests nonnormality.
* **Implication:** While least squares estimates ($b_0, b_1$) are still unbiased, inferences (t-tests, F-tests, confidence intervals) based on the t- and F-distributions rely on normality. For small sample sizes, severe nonnormality can make these inferences unreliable. For large samples, the Central Limit Theorem helps mitigate this.

### Omission of Important Predictor Variables (Page 112)

* **Plot:** **Residuals ($e_i$) vs. an Omitted Predictor Variable** (if data on the omitted variable is available). Sometimes, also patterns in **Residuals vs. Included Predictor** can suggest this, if the included predictor is a proxy for the omitted one.
* **Pattern:** If a systematic pattern (e.g., linear trend, curve) appears when residuals are plotted against a variable not included in the model, it suggests that the omitted variable might be an important predictor.
* **Implication:** Omitting important correlated predictors can lead to **omitted variable bias**, where the estimated coefficients of the included variables are biased and inconsistent.

---

## 3.4 Overview of Tests Involving Residuals (Page 114)

While visual plots are powerful, formal statistical tests can provide objective measures of assumption violations.

* **Tests for Randomness:**
    * **Durbin-Watson Test:** Commonly used for detecting first-order autocorrelation (correlation between an error term and the previous error term), particularly in time series data.
* **Tests for Constancy of Variance:**
    * **Brown-Forsythe Test:** A robust test comparing the variability of residuals in different groups (e.g., low, medium, high $X$ values).
    * **Breusch-Pagan Test / White Test:** More general tests that check if the squared residuals are systematically related to the predictor variables.
* **Tests for Outliers:**
    * **Bonferroni Outlier Test:** Uses the maximum studentized residual to test if the most extreme observation is an outlier.
* **Tests for Normality:**
    * **Shapiro-Wilk Test:** A powerful general test for normality.
    * **Anderson-Darling Test:** Another popular goodness-of-fit test for normality.
    * **Kolmogorov-Smirnov Test:** A general test for distribution fit.

## 3.5 Correlation Test for Nonnormality (Page 115)

This is a specific type of normality test often used in conjunction with normal probability plots.
* **Concept:** It assesses the linearity of points on a normal probability plot by calculating the correlation coefficient between the ordered residuals ($e_{(i)}$) and their expected values under a normal distribution (normal scores or normal quantiles).
* **Interpretation:** A high correlation coefficient (close to 1) suggests that the residuals are approximately normally distributed. A low correlation indicates departure from normality. The hypothesis test is usually for $H_0$: data are normal vs. $H_a$: data are not normal. A low p-value (e.g., <0.05) would lead to rejection of normality.

## 3.6 Tests for Constancy of Error Variance (Page 116)

### Brown-Forsythe Test (Page 116)

* **Nature:** This is a modified Levene's test, which is robust to non-normality.
* **Procedure:**
    1.  Divide the data into several groups based on the values of the independent variable $X$ (e.g., low $X$, medium $X$, high $X$).
    2.  For each group, calculate the median of the absolute residuals (or squared residuals).
    3.  Perform a one-way ANOVA on these absolute (or squared) residuals using the groups as factor levels.
* **Hypotheses:** $H_0$: The error variances are constant across groups (homoscedasticity). $H_a$: The error variances are not constant (heteroscedasticity).
* **Logic:** If the variances are constant, the medians of the absolute residuals across groups should be similar. A significant p-value from the ANOVA suggests heteroscedasticity.

### Breusch-Pagan Test (Page 118)

* **Nature:** A more general and often more powerful test for heteroscedasticity.
* **Procedure:**
    1.  Fit the original regression model and obtain the squared residuals ($e_i^2$).
    2.  Regress these squared residuals on the original predictor variables (or other variables suspected of influencing the variance).
    3.  Calculate a test statistic (often related to $n \times R^2$ from this auxiliary regression of $e_i^2$).
* **Hypotheses:** $H_0$: Constant error variance. $H_a$: Error variance is a function of the predictors.
* **Logic:** If the squared residuals can be predicted by the independent variables, it indicates nonconstant variance. The test statistic follows a chi-squared distribution.

---

## 3.7 F Test for Lack of Fit (Page 119)

The F-test for lack of fit is a formal test to assess if the chosen functional form of the regression model (e.g., a linear relationship) is adequate. It requires **replicated observations** (i.e., multiple $Y$ observations for at least some identical $X$ values).

### Assumptions (Page 119)

* **Replicated Observations:** There must be at least one $X$ level for which there are two or more $Y$ observations.
* The error terms $\epsilon_i$ are independent and normally distributed with constant variance $\sigma^2$.

### Notation (Page 121)

* Let $c$ be the number of distinct $X$ values.
* Let $n_j$ be the number of observations at the $j$-th distinct $X$ value. So, $\sum n_j = n$.
* $Y_{jl}$ is the $l$-th observation at the $j$-th distinct $X$ value ($l=1, \dots, n_j$).
* $\bar{Y}_j$ is the mean of the $Y$ observations at the $j$-th distinct $X$ value.

The total sum of squares ($SST$) is partitioned into two components:
* **Pure Error (SSE_PE):** Variation within the groups of replicated observations. It measures the inherent variability of $Y$ at fixed $X$ levels, independent of the model's form.
    $SSE_{PE} = \sum_{j=1}^{c} \sum_{l=1}^{n_j} (Y_{jl} - \bar{Y}_j)^2$
    Degrees of freedom: $df_{PE} = \sum_{j=1}^{c} (n_j - 1) = n - c$
* **Lack of Fit (SSE_LF):** Variation of the group means ($\bar{Y}_j$) around the fitted regression line. This component reflects how well the chosen regression function fits these means.
    $SSE_{LF} = SSE - SSE_{PE}$
    Degrees of freedom: $df_{LF} = df_{Error} - df_{PE} = (n-2) - (n-c) = c-2$ (for simple linear regression).

So, the Error Sum of Squares ($SSE$) from the full linear model can be further partitioned:
$SSE = SSE_{LF} + SSE_{PE}$

### Full Model (Page 121)

The "full model" here is a saturated model that perfectly fits the mean of $Y$ for each distinct $X$ value. For each group $j$, the estimated mean is simply the observed mean $\bar{Y}_j$. This model always fits the data perfectly at the group level.

### Reduced Model (Page 123)

The "reduced model" is the specific regression function you are testing (e.g., the simple linear regression model).

### Test Statistic (Page 123)

The test statistic for lack of fit compares the variability due to lack of fit to the pure error variability:

$F^* = \frac{SSE_{LF} / df_{LF}}{SSE_{PE} / df_{PE}} = \frac{MS_{LF}}{MS_{PE}}$

Under the null hypothesis that the regression function is correct (no lack of fit), $F^*$ follows an $F$-distribution with $df_{LF}$ and $df_{PE}$ degrees of freedom.

* **Hypotheses:**
    * $H_0$: The chosen regression function (e.g., linear) is an adequate model for the relationship. (No lack of fit)
    * $H_a$: The chosen regression function is not adequate. (Lack of fit exists)

* **Decision:** If $F^*$ is large (p-value $\le \alpha$), reject $H_0$, indicating significant lack of fit and suggesting the model form is incorrect.

### ANOVA Table (Page 124)

The ANOVA table is expanded to show the partitioning of SSE:

| Source of Variation | Sum of Squares (SS) | Degrees of Freedom (df) | Mean Square (MS) | F Statistic | P-value |
| :------------------ | :------------------ | :---------------------- | :--------------- | :---------- | :------ |
| Regression          | SSR                 | 1                       | MSR              | $F^* = MSR/MSE$ |         |
| **Error** | **SSE** | **n-2** | **MSE** |             |         |
| $\quad$Lack of Fit  | $SSE_{LF}$          | $c-2$                   | $MS_{LF}$        | $F^* = MS_{LF}/MS_{PE}$ |         |
| $\quad$Pure Error   | $SSE_{PE}$          | $n-c$                   | $MS_{PE}$        |             |         |
| Total               | SST                 | n-1                     |                  |             |         |

## 3.8 Overview of Remedial Measures (Page 127)

Once departures are detected, remedial measures are employed to address them. These often involve **transformations** of variables or using different modeling approaches.

### Nonlinearity of Regression Function (Page 128)

* **Add polynomial terms:** Include $X^2$, $X^3$, etc., in the model ($\hat{Y} = b_0 + b_1 X + b_2 X^2$).
* **Transform independent variable:** Use $\log(X)$, $\sqrt{X}$, $1/X$.
* **Transform dependent variable:** Use $\log(Y)$, $\sqrt{Y}$, $1/Y$ (can also help with nonconstant variance and nonnormality).
* **Use non-linear regression models:** If the underlying relationship is known to be inherently non-linear (e.g., exponential, logistic).

### Nonconstancy of Error Variance (Page 128)

* **Transform dependent variable:** $\log(Y)$, $\sqrt{Y}$, $1/Y$ are often effective in stabilizing variance.
* **Weighted Least Squares (WLS):** Assign different weights to observations based on their estimated variances, giving less weight to observations with larger variances.
* **Robust standard errors:** Use heteroscedasticity-consistent standard errors (e.g., White's standard errors) which correct for the biased standard errors without changing the coefficient estimates.

### Nonindependence of Error Terms (Page 128)

* **Include omitted variables:** If the nonindependence is due to a time-dependent variable not in the model.
* **Time series models:** For time-ordered data, use models that explicitly account for autocorrelation (e.g., ARIMA models for the error terms).
* **Generalized Least Squares (GLS):** A more general approach that accounts for structured correlation in the errors.

### Nonnormality of Error Terms (Page 128)

* **Transform dependent variable:** $\log(Y)$, $\sqrt{Y}$, etc. (often works in conjunction with nonconstant variance).
* **Use non-parametric regression methods:** If strong nonnormality persists and sample size is small.
* **Rely on Central Limit Theorem:** For large sample sizes, parameter estimates will be approximately normal even if errors are not.

### Omission of Important Predictor Variables (Page 129)

* **Collect data on and include the omitted variables:** The most direct solution.
* **Use proxy variables:** If direct measurement is not possible, use variables that are highly correlated with the omitted variable.

### Outlying Observations (Page 129)

* **Investigate and Correct:** Check for data entry errors or measurement errors. If found, correct or remove.
* **Robust Regression:** Use regression methods that are less sensitive to outliers.
* **Transformations:** Transformations can sometimes mitigate the effect of outliers by pulling in extreme values.
* **Remove (with justification):** If an outlier is clearly an anomaly not representative of the population and influencing results, it might be removed, but this must be strongly justified and clearly stated.

## 3.9 Transformations (Page 129)

Transformations are powerful tools for addressing model departures. They can be applied to the dependent variable ($Y$), independent variable(s) ($X$), or both.

### Transformations for Nonlinear Relation Only (Page 129)

These transformations are applied to the $X$ variable(s) to linearize a non-linear relationship while keeping the error structure (constant variance, normality) intact.
* **Polynomial terms:** $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon$ (quadratic model).
* **Logarithmic transformation of X:** $Y = \beta_0 + \beta_1 \ln(X) + \epsilon$. Useful for relationships where $Y$ changes by a constant amount for a proportional change in $X$.
* **Reciprocal transformation of X:** $Y = \beta_0 + \beta_1 (1/X) + \epsilon$.
* **Square root transformation of X:** $Y = \beta_0 + \beta_1 \sqrt{X} + \epsilon$.

### Transformations for Nonnormality and Unequal Error Variances (Page 132)

These transformations are primarily applied to the $Y$ variable. They often have the beneficial side effect of simultaneously stabilizing variance, improving normality, and sometimes even linearizing the relationship.
* **Logarithmic transformation of Y:** $\ln(Y)$ (or $\log_{10}(Y)$). Common when the error variance increases with the mean of $Y$, or when the distribution of $Y$ is right-skewed.
* **Square root transformation of Y:** $\sqrt{Y}$. Useful for count data or when variance is proportional to the mean.
* **Reciprocal transformation of Y:** $1/Y$. Useful when variance increases rapidly with the mean.

**Choosing a transformation:** Often guided by residual plots. For example, if residuals show a "fan" shape (heteroscedasticity) and the $Y$ values are skewed, a log transform of $Y$ might be suitable.

### Box-Cox Transformations (Page 134)

The Box-Cox transformation is a family of power transformations applied to the dependent variable $Y$. It provides a systematic way to choose an optimal power $\lambda$ for transforming $Y$ from the data itself.

**Formula:**
$Y^{(\lambda)} = \begin{cases} \frac{Y^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0 \\ \ln(Y) & \text{if } \lambda = 0 \end{cases}$

* **How it works:** The method involves fitting the regression model for various values of $\lambda$ and selecting the $\lambda$ that maximizes the likelihood function (or minimizes the SSE) for the transformed data.
* **Benefits:** It helps stabilize variance, improve normality, and sometimes linearize the relationship simultaneously, without requiring subjective judgment for the power.
* **Limitations:** The interpretation of coefficients on the transformed scale can be less intuitive. Also, if $\lambda$ is far from 1, it significantly alters the scale of $Y$.

## 3.10 Exploration of Shape of Regression Function (Page 137)

These methods are exploratory tools to help visualize the underlying relationship between $X$ and $Y$ without assuming a specific functional form.

### Lowess Method (Locally Weighted Scatterplot Smoothing) (Page 138)

* **Purpose:** Lowess (or LOESS) is a non-parametric method used to fit a smooth curve to a scatter plot. It doesn't assume a predefined functional form (like linear or quadratic) but rather estimates the regression surface at each point using local weighted regression.
* **How it works:** For each point on the X-axis where a smoothed value is desired, a weighted linear (or polynomial) regression is performed on a local subset of the data points. Points closer to the target point receive higher weights.
* **Benefit:** Provides a visual representation of the underlying relationship, helping to identify whether a linear model is appropriate or if a more complex (e.g., curvilinear) function is needed.

### Use of Smoothed Curves to Confirm Fitted Regression Function (Page 139)

* After fitting a linear regression model, plotting the Lowess curve on the same scatter plot can help confirm if the linear fit is reasonable.
* If the Lowess curve closely follows the straight regression line, it supports the linearity assumption.
* If the Lowess curve shows a clear bend or pattern that the linear line doesn't capture, it suggests that a linear model is not appropriate and indicates the need for transformation or a more complex functional form.

---
Okay, let's elaborate on each of the statistical tests mentioned in the context of regression diagnostics, explaining what they do, how they generally work, and why they are important.

---

## Statistical Tests for Regression Diagnostics

These tests provide formal statistical evidence to support or refute the visual insights gained from residual plots.

### 1. Tests for Randomness / Independence of Error Terms

The assumption is that error terms ($\epsilon_i$) are independent of each other. Violations, especially in time-series data, lead to **autocorrelation**.

#### a. Durbin-Watson Test

* **What it tests:** Primarily detects **first-order autocorrelation** (correlation between an error term and the immediately preceding one).
    * $H_0$: No positive first-order autocorrelation ($\rho \le 0$).
    * $H_1$: Positive first-order autocorrelation ($\rho > 0$).
    * It can also be used for negative autocorrelation with a modified alternative hypothesis.

* **How it works:** It calculates a test statistic ($d$) based on the differences between consecutive residuals.
    $d = \frac{\sum_{i=2}^{n} (e_i - e_{i-1})^2}{\sum_{i=1}^{n} e_i^2}$
    * A value of $d \approx 2$ suggests no autocorrelation.
    * A value of $d < 2$ suggests positive autocorrelation.
    * A value of $d > 2$ suggests negative autocorrelation.
    The test involves comparing $d$ to lower ($d_L$) and upper ($d_U$) critical values from a Durbin-Watson table. There's a region of indecision.

* **Why it's important:** Autocorrelation violates the independence assumption, leading to **underestimated standard errors** of regression coefficients. This makes your t-statistics and F-statistics inflated, leading to a higher chance of incorrectly concluding that predictors are significant. If autocorrelation is present, standard OLS inferences are invalid.

* **Typical Distribution:** The test statistic $d$ does not follow a standard distribution directly, but its critical values are derived and tabulated.

### 2. Tests for Constancy of Error Variance (Homoscedasticity)

The assumption is that the variance of the error terms ($\sigma^2$) is constant across all levels of the independent variables. Violation is called **heteroscedasticity**.

#### a. Brown-Forsythe Test (Modified Levene's Test)

* **What it tests:** Whether the variances of the residuals are constant across different groups of data (usually defined by the range of $X$ values).
    * $H_0$: The error variances are constant (homoscedasticity).
    * $H_a$: The error variances are not constant (heteroscedasticity).

* **How it works:**
    1.  Divide the residuals into several groups (e.g., based on low, medium, high values of the independent variable or fitted values).
    2.  For each group, calculate the **absolute deviations of residuals from their group median** (this is the key modification from original Levene's, making it more robust to non-normality).
    3.  Perform a one-way ANOVA on these absolute deviations.

* **Why it's important:** Heteroscedasticity leads to **biased (usually underestimated) standard errors** of the regression coefficients, making confidence intervals too narrow and p-values too small. This can lead to false conclusions about predictor significance. The Brown-Forsythe test is robust to non-normality, making it a good choice if normality is also questionable.

* **Typical Distribution:** F-distribution (from the ANOVA).

#### b. Breusch-Pagan Test

* **What it tests:** A more general test for heteroscedasticity, checking if the variance of the errors is systematically related to the independent variables (or other specified variables).
    * $H_0$: The error variance is constant (homoscedasticity).
    * $H_a$: The error variance is a function of the independent variables.

* **How it works:**
    1.  Fit the original regression model and obtain the squared residuals ($e_i^2$).
    2.  Perform an auxiliary regression: Regress these squared residuals ($e_i^2$) on the independent variables of the original model (or any other variables suspected of influencing the variance).
    3.  Calculate a test statistic based on the $R^2$ from this auxiliary regression (e.g., $n \times R^2_{auxiliary}$). A significant $R^2$ in this auxiliary regression suggests that the independent variables explain some variance in the squared residuals, indicating heteroscedasticity.

* **Why it's important:** Same reasons as the Brown-Forsythe test. This test is generally more powerful than graphical methods and can detect more complex forms of heteroscedasticity.

* **Typical Distribution:** Chi-squared distribution with degrees of freedom equal to the number of independent variables in the auxiliary regression.

### 3. Tests for Outliers

Outliers are observations with unusually large residuals, suggesting they don't fit the model well.

#### a. Bonferroni Outlier Test

* **What it tests:** Whether the *most extreme* residual in the sample is an outlier. It addresses the multiple testing problem that arises from visually inspecting many residuals to find the largest one.
    * $H_0$: The most extreme observation is not an outlier.
    * $H_a$: The most extreme observation is an outlier.

* **How it works:**
    1.  Calculate studentized (or externally studentized) residuals for all observations. These residuals account for the fact that observations with higher leverage have smaller residual variances.
    2.  Find the observation with the largest absolute studentized residual ($t_{max}^*$).
    3.  Compare $t_{max}^*$ to a critical value from a t-distribution adjusted using the Bonferroni correction for multiple comparisons. This correction ensures that the overall probability of incorrectly identifying *any* outlier in the sample (Type I error) remains at the desired $\alpha$ level.

* **Why it's important:** Outliers can disproportionately influence the estimated regression coefficients and inflate the error variance (MSE), leading to inaccurate inferences. This test provides a formal statistical basis to identify if an extreme point is truly an anomaly.

* **Typical Distribution:** t-distribution (with Bonferroni adjustment).

### 4. Tests for Normality of Error Terms

The assumption is that the error terms are normally distributed.

#### a. Correlation Test for Normality

* **What it tests:** Whether the empirical cumulative distribution function of the residuals is sufficiently close to a theoretical normal distribution.
    * $H_0$: Residuals are normally distributed.
    * $H_a$: Residuals are not normally distributed.

* **How it works:**
    1.  Order the calculated residuals from smallest to largest.
    2.  Determine the expected values of these ordered residuals if they truly came from a normal distribution (these are called normal scores or normal quantiles).
    3.  Calculate the Pearson correlation coefficient between the ordered residuals and their corresponding normal scores.
    4.  Compare this correlation coefficient to tabulated critical values. A correlation close to 1 supports normality.

* **Why it's important:** Normality of errors is crucial for the validity of t-tests and F-tests in small samples. While OLS estimates are still unbiased without normality, the accuracy of confidence intervals and p-values relies on this assumption.

* **Typical Distribution:** Special tables for the correlation coefficient.

#### b. Shapiro-Wilk Test

* **What it tests:** A highly regarded and powerful omnibus test for normality.
    * $H_0$: The sample data (residuals) comes from a normal distribution.
    * $H_a$: The sample data does not come from a normal distribution.

* **How it works:** It calculates a test statistic ($W$) that essentially measures how well the sorted residuals conform to a straight line on a normal Q-Q plot. It's a ratio of two estimators of variance. A value of $W$ close to 1 indicates normality.

* **Why it's important:** It's one of the most powerful tests for normality, particularly good for smaller to moderate sample sizes.

* **Typical Distribution:** Special tables for $W$ or its associated p-value.

#### c. Anderson-Darling Test

* **What it tests:** Another powerful goodness-of-fit test for normality, which gives more weight to the tails of the distribution.
    * $H_0$: The sample data (residuals) comes from a normal distribution.
    * $H_a$: The sample data does not come from a normal distribution.

* **How it works:** It's a type of weighted Cramer-von Mises statistic that calculates the difference between the empirical cumulative distribution function of the residuals and the cumulative distribution function of a theoretical normal distribution. The weighting in the tails makes it more sensitive to deviations there.

* **Why it's important:** It's known to be more sensitive to deviations in the tails of the distribution compared to the Kolmogorov-Smirnov test.

* **Typical Distribution:** Special tables for the Anderson-Darling statistic ($A^2$).

#### d. Kolmogorov-Smirnov (Lilliefors Test)

* **What it tests:** A general non-parametric goodness-of-fit test that can be used to compare a sample distribution to a theoretical distribution (like normal, exponential, etc.). Lilliefors' test is a modification specifically for testing normality when the mean and variance are estimated from the data.
    * $H_0$: The sample data (residuals) comes from a specified distribution (e.g., normal).
    * $H_a$: The sample data does not come from that distribution.

* **How it works:** It calculates the maximum absolute difference between the empirical cumulative distribution function (ECDF) of the residuals and the cumulative distribution function (CDF) of a theoretical normal distribution.

* **Why it's important:** While general, it's often considered less powerful than Shapiro-Wilk or Anderson-Darling for specifically testing normality.

* **Typical Distribution:** Kolmogorov-Smirnov distribution (or Lilliefors' modified K-S distribution).

### 5. F Test for Lack of Fit

This test is distinct from the F-test for overall model significance. It specifically evaluates the appropriateness of the chosen **functional form** of the regression model.

* **What it tests:** Whether the specified regression function (e.g., a straight line) adequately describes the relationship between $X$ and $Y$. It requires **replicated observations** (multiple $Y$ values for at least some $X$ values).
    * $H_0$: The chosen regression function (e.g., linear) is adequate. (No lack of fit)
    * $H_a$: The chosen regression function is not adequate. (Significant lack of fit)

* **How it works:** It partitions the Error Sum of Squares (SSE) into two components:
    * **Pure Error (SSE_PE):** The variability within the groups of repeated observations. This is the "noise" that *cannot* be explained by *any* model.
    * **Lack of Fit (SSE_LF):** The variability of the group means around the fitted regression line. This is the portion of the error that *could* be explained if a more appropriate functional form were used.
    The test statistic is the ratio of Mean Square Lack of Fit ($MS_{LF} = SSE_{LF} / df_{LF}$) to Mean Square Pure Error ($MS_{PE} = SSE_{PE} / df_{PE}$).

* **Why it's important:** If the test for lack of fit is significant, it means your chosen model (e.g., linear) is a poor representation of the true relationship. This indicates that a different functional form (e.g., quadratic, logarithmic) is likely needed. It's a powerful objective check on the linearity assumption.

* **Typical Distribution:** F-distribution with $df_{LF}$ and $df_{PE}$ degrees of freedom.

---
