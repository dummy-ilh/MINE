Sure, here are the notes in Markdown format, as requested:

# Chapter 1: What is Regression? Why the Name?

---

### What is Regression?

**Regression analysis** is a powerful statistical technique used to model the relationship between a **dependent variable** (also called response variable, outcome variable) and one or more **independent variables** (also called predictor variables, explanatory variables, or covariates). The primary goals of regression analysis are:
1.  **Prediction:** To predict the value of the dependent variable based on the values of the independent variables.
2.  **Inference:** To understand the nature and strength of the relationship between the variables, and to assess the effect of independent variables on the dependent variable.
3.  **Modeling:** To construct a mathematical equation that describes the relationship.

### Why the Name?

The term "regression" was first used by Sir Francis Galton in the late 19th century. He observed that while tall parents tended to have tall children, the children's heights, on average, "**regressed**" or "moved back" towards the average height of the population. This phenomenon is known as "regression to the mean." Although modern regression analysis is used for much more than this specific biological phenomenon, the name "regression" stuck. It now broadly refers to statistical methods that model relationships between variables, predicting one from others.

---

### Statistical Relation Between Two Variables

In statistics, we often deal with two types of relationships:
* **Functional (Deterministic) Relationship:** In this relationship, one variable can be perfectly predicted from another (e.g., $F = 1.8C + 32$). There's no random error.
* **Statistical Relationship:** This is common in real-world data. While variables might tend to move together, the relationship isn't perfect. There's inherent variability, meaning that for a given independent variable, the dependent variable can take on a range of values due to unobserved factors, measurement errors, or inherent randomness. Regression analysis deals with statistical relationships, aiming to capture the systematic part while modeling random variation.

---

### Basic Concepts

* **Dependent Variable (Y):** The variable we're trying to predict or explain (response variable, outcome variable).
* **Independent Variable(s) (X):** The variable(s) used to predict or explain the dependent variable (predictor variable, explanatory variable, covariate).
* **Regression Function:** The mathematical equation describing the relationship between the **mean of the dependent variable** and the independent variable(s). It represents the systematic part of the relationship.
* **Error Term ($\epsilon$):** Represents the random deviation of an observed value of the dependent variable from its mean (as predicted by the regression function). It accounts for all factors not included in the model, measurement error, and inherent randomness.

---

### Construction of Regression Models

The process of constructing a regression model typically involves:
1.  **Stating the problem:** Clearly defining the research question and variables.
2.  **Collecting data:** Obtaining relevant data.
3.  **Specifying the model:** Choosing the form of the regression function (e.g., linear) and identifying independent variables.
4.  **Fitting the model:** Estimating the parameters of the regression function from observed data.
5.  **Evaluating the model:** Assessing how well the model fits the data and meets assumptions.
6.  **Using the model:** Making predictions or inferences.

---

### Regression and Causality

It's crucial to understand that **correlation does not imply causation**. While regression analysis can identify and quantify relationships, it cannot, by itself, prove causality.

* **Association:** Regression shows a strong statistical association.
* **Causation:** To infer causation, one needs to consider:
    * **Temporal precedence:** X must precede Y.
    * **Plausible mechanism:** A logical reason for X to cause Y.
    * **Elimination of confounding variables:** Other variables influencing both X and Y must be controlled for.
    Controlled experiments are often needed to establish causality, whereas observational studies are less conclusive.

---

### Formal Statement of Model

For **simple linear regression** (one independent variable), the formal model is:

$Y_i = E[Y_i | X_i] + \epsilon_i$

Where:
* $Y_i$ is the $i$-th observed value of the dependent variable.
* $X_i$ is the $i$-th observed value of the independent variable.
* $E[Y_i | X_i]$ is the conditional mean of $Y$ for a given $X_i$, known as the **regression function**.
* $\epsilon_i$ is the random error term for the $i$-th observation.

Assuming a linear relationship for the regression function:

$E[Y_i | X_i] = \beta_0 + \beta_1 X_i$

Thus, the full simple linear regression model is:

$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$

Where:
* $\beta_0$ (beta-nought) is the **Y-intercept**. It represents the mean value of $Y$ when $X$ is zero.
* $\beta_1$ (beta-one) is the **slope parameter**. It represents the change in the mean value of $Y$ for a one-unit increase in $X$.
* $\epsilon_i$ are typically assumed to be independent and identically distributed (i.i.d.) random variables following a normal distribution with mean zero and constant variance: $\epsilon_i \sim N(0, \sigma^2)$.

---

### Important Features of Model

Key features and assumptions of the simple linear regression model ($Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$) often include:

1.  **Linearity:** The relationship between $X$ and the mean of $Y$ ($E[Y|X]$) is linear.
2.  **Normality of Errors:** The error terms $\epsilon_i$ are normally distributed.
3.  **Constant Variance (Homoscedasticity):** The variance of the error terms ($\sigma^2$) is constant across all levels of $X$. $\text{Var}(\epsilon_i) = \sigma^2$.
4.  **Independence of Errors:** The error terms $\epsilon_i$ are independent of each other.
5.  **X is Fixed or Measured Without Error:** $X$ values are either fixed constants or measured without significant error.

---

### Meaning of Regression Parameters

Given the model $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$:

* **$\beta_0$ (Intercept):**
    * Represents the mean response $E[Y]$ when $X = 0$.
    * Its interpretation is only meaningful if $X=0$ is within the range of observed $X$ values and makes practical sense.
* **$\beta_1$ (Slope):**
    * Represents the change in the mean response $E[Y]$ for a one-unit increase in $X$.
    * $\beta_1 > 0$ indicates a positive relationship.
    * $\beta_1 < 0$ indicates a negative relationship.
    * $\beta_1 = 0$ indicates no linear relationship.

---

### Estimation of Regression Function

Since the true parameters $\beta_0$ and $\beta_1$ are unknown, we estimate them from sample data $(X_1, Y_1), \dots, (X_n, Y_n)$ using estimates $b_0$ and $b_1$.

The **estimated regression function** (or estimated regression line) is:

$\hat{Y}_i = b_0 + b_1 X_i$

Where:
* $\hat{Y}_i$ is the **predicted value** or **estimated mean response** for a given $X_i$.
* $b_0$ is the estimate of $\beta_0$.
* $b_1$ is the estimate of $\beta_1$.

---

### Method of Least Squares

The **Method of Least Squares** is the most common way to estimate $b_0$ and $b_1$. It finds the line that minimizes the sum of the squared differences between the observed $Y$ values and the predicted $\hat{Y}$ values.

The difference between an observed $Y_i$ and its predicted $\hat{Y}_i$ is the **residual**, $e_i$:

$e_i = Y_i - \hat{Y}_i = Y_i - (b_0 + b_1 X_i)$

The objective is to minimize the **Sum of Squared Errors (SSE)** or **Sum of Squared Residuals (SSR)**:

Minimize: $\sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (Y_i - (b_0 + b_1 X_i))^2$

The least squares estimators are:

$b_1 = \frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n} (X_i - \bar{X})^2}$

This can also be expressed as:

$b_1 = \frac{s_{XY}}{s_X^2}$

And:

$b_0 = \bar{Y} - b_1 \bar{X}$

Where $\bar{X}$ and $\bar{Y}$ are the sample means of $X$ and $Y$.

---

### Point Estimation of Mean Response

Once the least squares regression line $\hat{Y} = b_0 + b_1 X$ is determined, we can estimate the mean response for any given $X_h$:

$\hat{Y}_h = b_0 + b_1 X_h$

This $\hat{Y}_h$ is the best unbiased estimator of $E[Y_h | X_h]$.

---

### Residuals

A **residual** $e_i$ is the difference between the observed $Y_i$ and the predicted $\hat{Y}_i$:

$e_i = Y_i - \hat{Y}_i$

Residuals are crucial for:
* **Assessing model fit:** Large residuals indicate poor fit.
* **Checking assumptions:** Patterns in residuals can reveal violations of assumptions.
* **Identifying outliers:** Observations with very large residuals might be outliers.

---

### Properties of Fitted Regression Line

The least squares regression line $\hat{Y}_i = b_0 + b_1 X_i$ has several important properties:

1.  **The sum of the residuals is zero:** $\sum_{i=1}^{n} e_i = 0$.
2.  **The sum of the squared residuals is a minimum:** This is its defining property.
3.  **The regression line always passes through the point of means ($\bar{X}, \bar{Y}$):**
    * This is evident from the formula for $b_0$: $b_0 = \bar{Y} - b_1 \bar{X}$.
4.  **The sum of the observed values of $Y$ equals the sum of the fitted values:** $\sum_{i=1}^{n} Y_i = \sum_{i=1}^{n} \hat{Y}_i$.
5.  **The sum of the cross-products of the $X$ values and the residuals is zero:** $\sum_{i=1}^{n} X_i e_i = 0$.
6.  **The sum of the cross-products of the fitted values and the residuals is zero:** $\sum_{i=1}^{n} \hat{Y}_i e_i = 0$.




##  Estimation of Regression Function

Since the true parameters $\beta_0$ and $\beta_1$ are unknown population values, we must estimate them from sample data. The estimated regression function, also known as the **fitted regression line**, is denoted as:

$\hat{Y}_i = b_0 + b_1 X_i$

Where:
* $\hat{Y}_i$ (read "Y-hat sub i") is the **predicted value** or **estimated mean response** for a given $X_i$. This is the value of $Y$ that the fitted line predicts for $X_i$.
* $b_0$ is the estimate of the population intercept $\beta_0$.
* $b_1$ is the estimate of the population slope $\beta_1$.

Our goal is to find the "best" estimates $b_0$ and $b_1$ from the sample data.

---

## Method of Least Squares

The most widely used method for estimating the parameters $b_0$ and $b_1$ is the **Method of Least Squares**. This method finds the line that minimizes the sum of the squared differences between the observed values of $Y$ and the values predicted by the regression line.

The difference between an observed value $Y_i$ and its corresponding predicted value $\hat{Y}_i$ is called a **residual**, denoted as $e_i$:

$e_i = Y_i - \hat{Y}_i = Y_i - (b_0 + b_1 X_i)$

The **sum of squared errors (SSE)**, also known as the **sum of squared residuals (SSR)**, is the quantity we wish to minimize:

$Q = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (Y_i - (b_0 + b_1 X_i))^2$

To find the values of $b_0$ and $b_1$ that minimize $Q$, we use calculus (take partial derivatives with respect to $b_0$ and $b_1$ and set them to zero). This leads to a system of two linear equations, called the **normal equations**:

1.  $\sum Y_i = n b_0 + b_1 \sum X_i$
2.  $\sum X_i Y_i = b_0 \sum X_i + b_1 \sum X_i^2$

Solving these normal equations for $b_0$ and $b_1$ yields the least squares estimators:

$b_1 = \frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n} (X_i - \bar{X})^2}$

This formula can also be expressed as:

$b_1 = \frac{n \sum X_i Y_i - (\sum X_i)(\sum Y_i)}{n \sum X_i^2 - (\sum X_i)^2}$

And the intercept estimator is:

$b_0 = \bar{Y} - b_1 \bar{X}$

Where $\bar{X}$ and $\bar{Y}$ are the sample means of $X$ and $Y$, respectively. These $b_0$ and $b_1$ values are unique and represent the best-fitting line in the least squares sense.

---

## Point Estimation of Mean Response

Once the least squares estimators $b_0$ and $b_1$ are obtained, the fitted regression line $\hat{Y} = b_0 + b_1 X$ can be used to estimate the mean response for any given value of $X$.

If we are interested in the mean response at a specific value of the independent variable, say $X_h$, the point estimate for the mean response $E[Y_h|X_h]$ is:

$\hat{Y}_h = b_0 + b_1 X_h$

This $\hat{Y}_h$ provides the best single estimate of the average value of $Y$ for observations with independent variable value $X_h$, under the assumption that the linear model is correct.

---

##  Residuals

A **residual** $e_i$ is the vertical distance between an observed data point $Y_i$ and the corresponding point on the fitted regression line $\hat{Y}_i$:

$e_i = Y_i - \hat{Y}_i$

Residuals are essential for:
* **Assessing Model Fit:** Large residuals indicate that the model is not fitting those particular observations well.
* **Checking Model Assumptions:** Patterns in the residuals (e.g., a fanning-out pattern, a curve) can signal violations of the linearity, constant variance, or independence assumptions.
* **Identifying Outliers:** Observations with unusually large residuals (in magnitude) may be outliers or influential data points that warrant further investigation.

---

## Properties of Fitted Regression Line

The least squares regression line, estimated by $b_0$ and $b_1$, has several important mathematical properties:

1.  **The sum of the residuals is zero:** $\sum_{i=1}^{n} e_i = \sum_{i=1}^{n} (Y_i - \hat{Y}_i) = 0$. This is a direct consequence of the first normal equation.
2.  **The sum of the squared residuals is a minimum:** This is the objective criterion the least squares method satisfies. Any other line will result in a larger sum of squared residuals.
3.  **The regression line passes through the point of means ($\bar{X}, \bar{Y}$):**
    * Substituting $\bar{X}$ into the fitted regression equation:
        $\hat{Y}_{\bar{X}} = b_0 + b_1 \bar{X}$
    * Since $b_0 = \bar{Y} - b_1 \bar{X}$, then $\hat{Y}_{\bar{X}} = (\bar{Y} - b_1 \bar{X}) + b_1 \bar{X} = \bar{Y}$.
    * This implies that the "center of gravity" of the data always lies on the fitted line.
4.  **The sum of the observed values of $Y$ equals the sum of the fitted values:** $\sum_{i=1}^{n} Y_i = \sum_{i=1}^{n} \hat{Y}_i$. This property follows directly from $\sum e_i = 0$.
5.  **The sum of the cross-products of the independent variable values and the residuals is zero:** $\sum_{i=1}^{n} X_i e_i = 0$.
6.  **The sum of the cross-products of the fitted values and the residuals is zero:** $\sum_{i=1}^{n} \hat{Y}_i e_i = 0$. This implies that the residuals are uncorrelated with the fitted values.




##  Estimation of Error Term's Variance $\sigma^2$ 

### Point Estimator of $\sigma^2$

The parameter $\sigma^2$ represents the constant variance of the error terms $\epsilon_i$ (and thus of the responses $Y_i$) in the regression model. It quantifies the inherent variability around the true regression line. Since $\sigma^2$ is unknown, we need to estimate it from the sample data.

The point estimator for $\sigma^2$ is called the **Mean Squared Error (MSE)**, denoted as $s^2$. It is calculated as the sum of squared residuals (SSE) divided by its degrees of freedom.

The **Sum of Squared Errors (SSE)** is defined as:

$
SSE = \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^{n} e_i^2
$

Where $e_i$ are the residuals from the fitted least squares line.

The **Mean Squared Error (MSE)** is:
$s^2 = MSE = \frac{SSE}{n-2}$

The denominator, $n-2$, represents the **degrees of freedom** for SSE. In simple linear regression, we estimate two parameters ($\beta_0$ and $\beta_1$). Each parameter estimated "costs" one degree of freedom. Since we have $n$ observations and we've estimated 2 parameters, $n-2$ degrees of freedom remain for estimating the error variance.

The standard deviation of the error terms, $\sigma$, is estimated by $s = \sqrt{MSE}$, also known as the **root mean squared error** or **residual standard error**.

### Why is it Needed?

The estimation of $\sigma^2$ (or $s^2$) is crucial for several reasons in regression analysis:

1.  **Quantifying Model Fit (Goodness of Fit):** $s^2$ provides a measure of how much variability in the dependent variable $Y$ remains *unexplained* by the regression model. A smaller $s^2$ (relative to the overall variability of $Y$) indicates that the data points cluster more closely around the fitted regression line, suggesting a better fit of the model to the data.
2.  **Inference about Parameters:** $s^2$ is a critical component in calculating the standard errors of the estimated regression coefficients ($b_0$ and $b_1$). These standard errors are then used to construct confidence intervals for $\beta_0$ and $\beta_1$ and to perform hypothesis tests about their values (e.g., testing if $\beta_1 = 0$). Without an estimate of $\sigma^2$, we cannot assess the precision of our parameter estimates.
3.  **Prediction Intervals:** When predicting new individual observations of $Y$, the width of the prediction interval heavily depends on $s^2$. A larger $s^2$ leads to wider, less precise prediction intervals, reflecting greater uncertainty in individual predictions.
4.  **Model Comparison:** In more complex scenarios, $s^2$ is used in various statistical tests (like F-tests) to compare the fit of different regression models.

### What Constitutes "Good" (for a Point Estimator)?

A "good" point estimator generally possesses several desirable statistical properties:

1.  **Unbiasedness:** An estimator is unbiased if its expected value is equal to the true parameter it is estimating. For $s^2$, $E\{s^2\} = \sigma^2$. This means that, on average, if we were to take many samples and calculate $s^2$ for each, the average of these $s^2$ values would equal the true $\sigma^2$. The denominator $n-2$ ensures this unbiasedness. If we used $n$ instead of $n-2$, the estimator would be biased (underestimated).
2.  **Consistency:** As the sample size $n$ increases, a consistent estimator's value tends to get closer and closer to the true parameter value. $s^2$ is a consistent estimator of $\sigma^2$.
3.  **Efficiency (Minimum Variance):** An efficient estimator has the smallest possible variance among all unbiased estimators. This means that its values are clustered most tightly around the true parameter, providing the most precise estimate. Under certain assumptions (like normality of errors), $s^2$ is the minimum variance unbiased estimator (MVUE) for $\sigma^2$.
4.  **Sufficiency:** A sufficient estimator "captures" all the information about the parameter that is available in the sample.

In the context of the least squares estimator $s^2$, its unbiasedness and efficiency (under normality) are highly desirable properties that make it the standard choice.

---

##  Normal Error Regression Model

### Model

The **Normal Error Regression Model** is a specific version of the simple linear regression model that adds the crucial assumption that the error terms $\epsilon_i$ are normally distributed.

The formal statement of the Normal Error Regression Model is:

$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$

Where, in addition to the previous assumptions:
* $\epsilon_i \sim N(0, \sigma^2)$ (independent and identically distributed normal random variables with mean 0 and constant variance $\sigma^2$).

This assumption implies that the response variable $Y_i$ itself is also normally distributed for any given $X_i$:

$Y_i \sim N(\beta_0 + \beta_1 X_i, \sigma^2)$

The normal error assumption is essential for making valid statistical inferences, such as constructing confidence intervals and conducting hypothesis tests for the regression parameters.

### Estimation of Parameters by Method of Maximum Likelihood (MLE)

The **Method of Maximum Likelihood (MLE)** is a powerful and widely used technique for estimating parameters in statistical models. It's based on a different principle than the Method of Least Squares, but for the Normal Error Regression Model, it yields the *same* estimates for $\beta_0$ and $\beta_1$ as least squares.

#### Explaining MLE for Beginners:

Imagine you have a bag of coins, and you want to figure out if they are fair (50% heads) or biased (e.g., 70% heads). You pick a coin, flip it 10 times, and get 7 heads.

* **The Question:** Which "true probability of heads" (the parameter we're estimating) makes the observed outcome (7 heads out of 10) *most likely*?
* **Intuition:** If the coin was truly fair (parameter = 0.5), getting 7 heads is possible but maybe not the *most* likely. If the coin was truly 0.7 heads (parameter = 0.7), getting 7 heads is highly likely. If it was 0.2 heads, getting 7 heads is very unlikely.
* **MLE's Core Idea:** MLE seeks the value of the unknown parameter(s) that maximize the **likelihood** of observing the particular data that we actually collected. It asks: "Given my observed data, what parameter values make this data seem most probable?"

#### How MLE Works (Conceptually):

1.  **Likelihood Function:** For a given dataset, the likelihood function ($L$) is a mathematical expression that quantifies how "likely" your observed data is for different possible values of the parameters. It's essentially the joint probability (or probability density) of observing all your data points, expressed as a function of the unknown parameters.
2.  **Maximization:** The MLE approach then finds the specific parameter values that make this likelihood function as large as possible. This is done by using calculus: taking partial derivatives of the likelihood function (or, more commonly, its logarithm) with respect to each parameter, setting these derivatives to zero, and solving the resulting equations. The values that satisfy these equations are the maximum likelihood estimates.

#### MLE for the Normal Error Regression Model:

1.  **Probability Density Function (PDF) for a single observation:**
    Since $Y_i \sim N(\beta_0 + \beta_1 X_i, \sigma^2)$, the PDF for a single observation $Y_i$ is:
    $f(Y_i; \beta_0, \beta_1, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2\sigma^2}(Y_i - (\beta_0 + \beta_1 X_i))^2\right)$

2.  **Likelihood Function ($L$):**
    Assuming observations $Y_i$ are independent, the likelihood function for the entire sample $(Y_1, \dots, Y_n)$ is the product of the individual PDFs:
    $L(\beta_0, \beta_1, \sigma^2 | Y_1, \dots, Y_n) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2\sigma^2}(Y_i - (\beta_0 + \beta_1 X_i))^2\right)$

3.  **Log-Likelihood Function ($ln(L)$):**
    Maximizing $L$ is mathematically equivalent to maximizing its natural logarithm, $ln(L)$. This simplifies the calculations significantly by turning products into sums.
    $ln(L) = \sum_{i=1}^{n} \left[ -\frac{1}{2} \ln(2\pi) - \frac{1}{2} \ln(\sigma^2) - \frac{1}{2\sigma^2}(Y_i - (\beta_0 + \beta_1 X_i))^2 \right]$

4.  **Maximization Steps:**
    To find the MLEs for $\beta_0$, $\beta_1$, and $\sigma^2$, we take partial derivatives of $ln(L)$ with respect to each parameter and set them to zero:
    * $\frac{\partial ln(L)}{\partial \beta_0} = 0$
    * $\frac{\partial ln(L)}{\partial \beta_1} = 0$
    * $\frac{\partial ln(L)}{\partial \sigma^2} = 0$

    Solving these equations yields the MLEs.

#### Why is MLE Used and How it Works (The Connection to OLS):

The remarkable result for the Normal Error Regression Model is that:

* The **Maximum Likelihood Estimators** for $\beta_0$ and $\beta_1$ turn out to be *identical* to the **Ordinary Least Squares (OLS) estimators** $b_0$ and $b_1$.
    * This is because maximizing $ln(L)$ (specifically, the term involving $\beta_0$ and $\beta_1$) is equivalent to minimizing $\sum (Y_i - (\beta_0 + \beta_1 X_i))^2$, which is precisely the least squares criterion. The normality assumption makes the least squares method also the maximum likelihood method for $\beta_0$ and $\beta_1$.

* The **Maximum Likelihood Estimator** for $\sigma^2$ is $\hat{\sigma}^2_{MLE} = \frac{SSE}{n}$.
    * Notice this is different from the unbiased estimator $s^2 = \frac{SSE}{n-2}$. The MLE for $\sigma^2$ is biased (it tends to underestimate $\sigma^2$), but it is consistent. For practical purposes in small samples, the unbiased $s^2$ is generally preferred.

#### Advantages of MLE:

* **Asymptotic Properties:** Under broad conditions, MLEs have desirable asymptotic properties (as sample size $n \rightarrow \infty$):
    * **Consistency:** They converge to the true parameter values.
    * **Asymptotic Normality:** Their sampling distribution approaches a normal distribution, which is useful for inference.
    * **Asymptotic Efficiency:** They achieve the lowest possible variance among consistent estimators, meaning they are the most precise in large samples.
* **Generalizability:** MLE is a very general method that can be applied to a wide range of statistical models (e.g., logistic regression, generalized linear models) where OLS might not be applicable or optimal.
* **Provides a Framework for Inference:** The inverse of the matrix of second partial derivatives of the log-likelihood function (Hessian matrix) provides estimates of the variances and covariances of the MLEs, which are used to construct confidence intervals and hypothesis tests.

In essence, for the simple linear regression model with normally distributed errors, MLE provides a rigorous theoretical justification for why the simpler OLS method works so well for estimating the regression coefficients.
