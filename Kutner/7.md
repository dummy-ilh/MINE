Chapter 7, "Multiple Regression II," builds upon the foundation of Chapter 6 by delving into more advanced topics crucial for understanding and interpreting multiple regression models. This chapter focuses on assessing the unique contribution of individual or groups of predictor variables, understanding coefficient interpretation, and diagnosing a critical issue: multicollinearity.

---

# Chapter 7: Multiple Regression II

## 7.1 Extra Sums of Squares (Page 256)

### Basic Ideas (Page 256)
The concept of "extra sum of squares" is fundamental for understanding the marginal or unique contribution of predictor variables in a multiple regression model.
An **extra sum of squares** represents the **reduction in the error sum of squares (SSE)**, or equivalently, the **increase in the regression sum of squares (SSR)**, when one or more predictor variables are added to a regression model that already contains other predictor variables.
It quantifies the additional variability in $Y$ explained by the new predictor(s), *given* the predictors already present in the model. This makes it a powerful tool for assessing the incremental explanatory power.

### Definitions (Page 259)
The notation $SSR(X_1, X_2, \dots, X_p)$ typically denotes the SSR when all predictors $X_1, \dots, X_p$ are in the model.
* **$SSR(X_1)$:** The regression sum of squares when only $X_1$ is in the model.
* **$SSR(X_2|X_1)$:** The extra sum of squares associated with adding $X_2$ to a model that already contains $X_1$. This is the reduction in SSE when $X_2$ is added to the model with $X_1$.
* **$SSR(X_1|X_2)$:** The extra sum of squares associated with adding $X_1$ to a model that already contains $X_2$. This is generally *not* equal to $SSR(X_2|X_1)$ unless $X_1$ and $X_2$ are uncorrelated.
* **$SSR(X_2, X_3 | X_1)$:** The extra sum of squares associated with adding $X_2$ and $X_3$ to a model that already contains $X_1$.

### Decomposition of SSR into Extra Sums of Squares (Page 260)
The total SSR for a model with multiple predictors can be decomposed into extra sums of squares. The order of inclusion matters for the individual components.
For a model with $X_1$ and $X_2$:
$SSR(X_1, X_2) = SSR(X_1) + SSR(X_2|X_1)$
Alternatively:
$SSR(X_1, X_2) = SSR(X_2) + SSR(X_1|X_2)$

This demonstrates that the total SSR is the same regardless of the order, but the individual "extra" contributions ($SSR(X_2|X_1)$ vs. $SSR(X_1|X_2)$) depend on the order in which predictors are considered for their marginal contribution.

### ANOVA Table Containing Decomposition of SSR (Page 261)
The ANOVA table can be expanded to show these sequential sums of squares, clearly breaking down the total variability explained by the model according to the order in which variables were entered.
For example, for a model with $X_1, X_2, X_3$:

| Source       | DF       | SS                   | MS                |
| :----------- | :------- | :------------------- | :---------------- |
| Regression   | $p-1$    | $SSR$                | $MSR = SSR/(p-1)$ |
|   $X_1$      | 1        | $SSR(X_1)$           | $MSR(X_1)$        |
|   $X_2|X_1$  | 1        | $SSR(X_2|X_1)$       | $MSR(X_2|X_1)$    |
|   $X_3|X_1,X_2$ | 1        | $SSR(X_3|X_1,X_2)$   | $MSR(X_3|X_1,X_2)$|
| Error        | $n-p$    | $SSE$                | $MSE = SSE/(n-p)$ |
| Total        | $n-1$    | $SST$                |                   |

Where $SSR = SSR(X_1) + SSR(X_2|X_1) + SSR(X_3|X_1,X_2)$.
This table allows for sequential testing of predictors.

## 7.2 Uses of Extra Sums of Squares in Tests for Regression Coefficients (Page 263)

Extra sums of squares are the building blocks for flexible F-tests concerning the regression coefficients.

### Test whether a Single $\beta_k = 0$ (Page 263)
To test if a single coefficient $\beta_k$ is zero (i.e., if $X_k$ contributes significantly to the model *given all other predictors are already in the model*), we use an F-test:
$H_0: \beta_k = 0$
$H_a: \beta_k \neq 0$
The test statistic is:
$F^* = \frac{SSR(X_k | \text{all other } X\text{s})}{MSE}$
This $F^*$ statistic has 1 numerator degree of freedom and $n-p$ denominator degrees of freedom. This F-test is equivalent to the t-test for $\beta_k$ from Chapter 6, as $F^* = (t^*)^2$.

### Test whether Several $\beta_k = 0$ (Page 264)
This is a powerful general linear hypothesis test. It tests whether a *subset* of regression coefficients are simultaneously zero, given that other predictor variables are already in the model.
* **Full Model:** Includes all predictor variables relevant to the test.
* **Reduced Model:** A sub-model derived from the full model by setting the coefficients of the variables being tested to zero (i.e., removing those variables).

Let $SSR_{Full}$ be the regression sum of squares for the full model and $SSR_{Reduced}$ be the regression sum of squares for the reduced model.
The F-test statistic for testing if $q$ coefficients are simultaneously zero (e.g., $H_0: \beta_2 = \beta_3 = 0$) is:
$F^* = \frac{SSR_{Full} - SSR_{Reduced}}{q} \div \frac{SSE_{Full}}{n-p}$
Where $q$ is the number of coefficients being tested (the number of variables removed from the full model to get the reduced model).
The numerator, $SSR_{Full} - SSR_{Reduced}$, is precisely the extra sum of squares associated with the $q$ variables being added to the reduced model.
Under $H_0$, $F^*$ follows an F-distribution with $q$ numerator and $n-p$ (DF of $SSE_{Full}$) denominator degrees of freedom. This test is crucial for determining the collective contribution of a group of predictors.

## 7.3 Summary of Tests Concerning Regression Coefficients (Page 266)

This section consolidates the common hypothesis tests in multiple regression, highlighting how they relate to the concepts of full and reduced models, and extra sums of squares.

### Test whether All $\beta_k = 0$ (Page 266)
* **Hypotheses:** $H_0: \beta_1 = \beta_2 = \dots = \beta_{p-1} = 0$ (All predictor variables have no linear effect). $H_a$: At least one $\beta_k \neq 0$.
* **F-statistic:** $F^* = MSR / MSE$. This is the overall F-test from Chapter 6, where $MSR = SSR(X_1, \dots, X_{p-1}) / (p-1)$.

### Test whether a Single $\beta_k = 0$ (Page 267)
* **Hypotheses:** $H_0: \beta_k = 0$ (The $k$-th predictor has no effect, given others). $H_a: \beta_k \neq 0$.
* **Test-statistic:** $t^* = b_k / s\{b_k\}$ (from Chapter 6) or $F^* = (t^*)^2 = SSR(X_k | \text{other } X\text{s}) / MSE$.

### Test whether Some $\beta_k = 0$ (Page 267)
* **Hypotheses:** $H_0: \beta_{q} = \beta_{q+1} = \dots = \beta_{p-1} = 0$ (A specific subset of coefficients are zero). $H_a$: At least one of these is not zero.
* **Test-statistic:** The partial F-test statistic derived from comparing the full and reduced models (as described in 7.2), using the extra sum of squares for the group of predictors being tested. This is the most general and powerful test.

### Other Tests (Page 268)
The general linear hypothesis framework allows for even more complex tests, such as testing if $\beta_1 = \beta_2$ or if $\beta_1 + \beta_2 = 1$. These involve setting up specific contrasts or constraints on the coefficients and using a similar F-test structure comparing models with and without these constraints.

## 7.4 Coefficients of Partial Determination (Page 268)

Partial determination coefficients quantify the proportion of variability in the dependent variable that is explained by one predictor (or a set of predictors) *after accounting for the effects of other predictors already in the model*.

### Two Predictor Variables (Page 269)
* **$r_{Y1.2}^2$:** The coefficient of partial determination between $Y$ and $X_1$, given $X_2$.
    It's interpreted as the proportion of the variation in $Y$ that is *not explained by $X_2$* that *is explained by $X_1$*.
    Formula: $r_{Y1.2}^2 = \frac{SSR(X_1|X_2)}{SSE(X_2)} = \frac{SSR(X_1|X_2)}{SST - SSR(X_2)}$.

### General Case (Page 269)
For a predictor $X_k$, the coefficient of partial determination $R_{Yk.\text{others}}^2$ is the proportion of the variation in $Y$ not explained by the "other" predictor variables that is explained by $X_k$.
Formula: $R_{Yk.\text{others}}^2 = \frac{SSR(X_k | \text{all other } X\text{s})}{SSE(\text{model excluding } X_k)}$.
This value is closely related to the $t$-statistic for $b_k$.

### Coefficients of Partial Correlation (Page 270)
The **coefficient of partial correlation** ($r_{Yk.\text{others}}$) is the square root of the coefficient of partial determination. It takes the sign of the corresponding regression coefficient $b_k$.
* It measures the strength and direction of the linear relationship between $Y$ and $X_k$, *after controlling for the linear effects of all other predictor variables*.
* This is distinct from the simple (marginal) correlation between $Y$ and $X_k$, which does not account for other predictors. Partial correlations are particularly useful in understanding the unique contribution of a predictor variable.

## 7.5 Standardized Multiple Regression Model (Page 271)

### Roundoff Errors in Normal Equations Calculations (Page 271)
When predictor variables have vastly different scales or magnitudes, the elements in the $\mathbf{X}^T \mathbf{X}$ matrix can become very large or very small. This can lead to numerical instability and increased roundoff errors during the computation of $(\mathbf{X}^T \mathbf{X})^{-1}$ by statistical software. Standardizing variables can mitigate this.

### Lack of Comparability in Regression Coefficients (Page 272)
In an unstandardized multiple regression model, the magnitude of the $b_k$ coefficients cannot be directly compared to assess the relative importance of predictor variables. This is because $b_k$ reflects the change in $Y$ for a one-unit change in $X_k$, and a 'one-unit change' has different meanings for variables measured on different scales (e.g., a one-unit change in 'age' vs. a one-unit change in 'income' ($).

### Correlation Transformation (Page 272)
To make coefficients comparable and improve numerical stability, variables are often subjected to a **correlation transformation** (also known as Z-score standardization).
For each variable (including $Y$ and all $X_k$s), the transformation is:
$Y_i^* = (Y_i - \bar{Y}) / s_Y$
$X_{ik}^* = (X_{ik} - \bar{X}_k) / s_k$
Where $s_Y$ and $s_k$ are the sample standard deviations of $Y$ and $X_k$, respectively.
The transformed variables $Y^*$ and $X^*$ have a mean of 0 and a standard deviation of 1.

### Standardized Regression Model (Page 273)
The regression model fitted using these standardized variables is the **standardized regression model**:
$Y_i^* = \beta_1^* X_{i1}^* + \beta_2^* X_{i2}^* + \dots + \beta_{p-1}^* X_{i,p-1}^* + \epsilon_i^*$
Note: The intercept $\beta_0^*$ is typically 0 in the standardized model (since means are 0).

### $\mathbf{X}'\mathbf{X}$ Matrix for Transformed Variables (Page 274)
When variables are correlation-transformed, the $\mathbf{X}^T \mathbf{X}$ matrix in the standardized model (ignoring the column of ones as $\beta_0^*$ is 0) becomes the **correlation matrix of the predictor variables**. This is numerically well-behaved.

### Estimated Standardized Regression Coefficients (Page 275)
The coefficients from the standardized model are denoted as $\mathbf{b}^*$ or $\beta_k^*$.
* **Interpretation:** A standardized regression coefficient $b_k^*$ indicates the change in the number of **standard deviations of $Y$** for a one-standard deviation increase in $X_k$, holding all other standardized predictors constant.
* **Comparability:** Because all variables are on the same standard deviation scale, the magnitudes of the $b_k^*$ coefficients *can* be compared to gauge the relative importance of predictor variables in influencing $Y$.
* **Conversion:** Standardized coefficients can be converted back to unstandardized coefficients and vice-versa.
    $b_k = b_k^* \cdot (s_Y / s_k)$
    $b_0 = \bar{Y} - \sum_{k=1}^{p-1} b_k \bar{X}_k$

## 7.6 Multicollinearity and Its Effects (Page 278)

**Multicollinearity** refers to the situation in multiple regression where two or more predictor variables are highly correlated with each other. It poses a significant challenge to the interpretation and stability of the regression model.

### Uncorrelated Predictor Variables (Page 279)
This is an ideal (but rare in observational studies) scenario. If predictor variables are perfectly uncorrelated (orthogonal), then:
* The regression coefficients $b_k$ are independent of which other predictors are in the model.
* The extra sum of squares $SSR(X_k | \text{other } X\text{s})$ would simply be $SSR(X_k)$.
* Coefficient estimates are highly stable and precise.
* Interpretation of individual coefficients is straightforward.
* This is often achieved in designed experiments (e.g., using orthogonal designs).

### Nature of Problem when Predictor Variables Are Perfectly Correlated (Page 281)
**Perfect Multicollinearity** occurs when one predictor variable is an exact linear combination of one or more other predictor variables.
* **Cause:** This happens if, for example, you include dummy variables for *all* categories of a categorical variable *and* an intercept in the model (the sum of dummies equals the intercept's column of ones). Or, if you include a variable and its exact duplicate.
* **Consequence:** The $\mathbf{X}^T \mathbf{X}$ matrix becomes **singular** (its determinant is zero), meaning its inverse $(\mathbf{X}^T \mathbf{X})^{-1}$ does **not exist**. Consequently, the least squares estimates $\mathbf{b} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$ cannot be uniquely determined. Statistical software will usually issue an error and drop one of the perfectly correlated variables.

### Effects of Multicollinearity (Page 283)
Even when multicollinearity is not perfect but **high** (i.e., predictors are highly correlated but not perfectly), it causes several practical problems:
1.  **Large Standard Errors for $b_k$:** The most prominent effect. High correlation among predictors makes it difficult for the model to isolate the unique contribution of each, leading to inflated variances of the coefficient estimates ($Var\{\mathbf{b}\} = \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}$). This results in large standard errors, wide confidence intervals, and consequently, individual t-tests that show individual predictors as not statistically significant, even if the overall model (F-test) is significant.
2.  **Unstable Coefficients:** The estimated coefficients $b_k$ become highly sensitive to small changes in the data or the inclusion/exclusion of other predictor variables. Adding or removing a few observations can drastically change the coefficient values.
3.  **Unexpected Signs or Magnitudes:** Coefficients may have signs opposite to what is expected based on theory or previous research, or their magnitudes may be unreasonably large or small.
4.  **Difficulty in Interpretation:** It becomes hard to interpret an individual $b_k$ as the change in $Y$ holding other predictors constant, because when predictors are highly correlated, it's difficult to vary one while keeping others truly constant.
5.  **High $R^2$ but Few Significant Individual Predictors:** The model might explain a large portion of the variance in $Y$ (high $R^2$), but individual t-tests for coefficients are often non-significant. This is because the predictors, as a group, explain a lot, but their individual contributions are indistinguishable due to their collinearity.

### Need for More Powerful Diagnostics for Multicollinearity (Page 289)
Simple pairwise correlation coefficients are insufficient to detect multicollinearity, as it can involve linear relationships among three or more variables simultaneously. This necessitates more sophisticated diagnostics:
* **Variance Inflation Factor (VIF):** A widely used metric that quantifies how much the variance of an estimated regression coefficient is inflated due to collinearity with other predictors. VIF values greater than 5 or 10 typically indicate problematic multicollinearity.
* **Tolerance:** The reciprocal of VIF (1/VIF). Values close to 0 indicate high multicollinearity.
* **Eigenvalues of $\mathbf{X}^T \mathbf{X}$ and Condition Index:** These provide a more holistic view of the correlation structure among predictors and can identify specific patterns of multicollinearity. (Often discussed in detail in Chapter 8).

Understanding multicollinearity is crucial because it directly impacts the reliability and interpretability of the multiple regression model, potentially leading to incorrect conclusions about the effects of individual predictors.
