Chapter 10, "Diagnostics for Model Adequacy," is a critical follow-up to the model-building process. While Chapter 9 focuses on selecting the "best" subset of predictors, Chapter 10 provides advanced tools to scrutinize the chosen model for problems related to individual observations (outliers, high-leverage points, influential points) and the interrelationships among predictor variables (multicollinearity). These diagnostics help ensure the reliability and validity of the regression results.

---

# Chapter 10: Diagnostics for Model Adequacy

## 10.1 Model Adequacy for a Predictor Variable - Added-Variable Plots (Page 384)

**Added-variable plots**, also known as **partial regression plots** or **partial residual plots**, are graphical tools used to assess the nature of the relationship between the dependent variable ($Y$) and a specific predictor variable ($X_k$), **after accounting for the effects of all other predictor variables already in the model**.

* **How it's constructed:** For a given predictor $X_k$:
    1.  Regress $Y$ on *all other predictor variables* (excluding $X_k$) and obtain the residuals, denoted as $e(Y|X_{\text{others}})$. These residuals represent the variation in $Y$ not explained by the other predictors.
    2.  Regress $X_k$ on *all other predictor variables* (excluding $X_k$) and obtain the residuals, denoted as $e(X_k|X_{\text{others}})$. These residuals represent the variation in $X_k$ not explained by the other predictors.
    3.  Plot $e(Y|X_{\text{others}})$ on the vertical axis against $e(X_k|X_{\text{others}})$ on the horizontal axis.

* **Interpretation and Uses:**
    * **Detecting Linear Relationship:** The slope of the points in the added-variable plot for $X_k$ will be approximately equal to the estimated regression coefficient $b_k$ in the full multiple regression model. If a clear linear pattern is visible, it suggests $X_k$ has a significant partial relationship with $Y$.
    * **Assessing Curvilinearity:** If the plot shows a curvilinear pattern, it suggests that $X_k$ might require a polynomial term or a transformation to capture its true relationship with $Y$.
    * **Identifying Influential Observations:** Outliers or high-leverage points that are particularly influential for the coefficient of $X_k$ will often stand out distinctly in this plot.
    * **Detecting Multicollinearity:** If $X_k$ is highly correlated with other predictors, $e(X_k|X_{\text{others}})$ will have very little variation (the points will be highly compressed horizontally), making it difficult to see its unique relationship with $Y$.

## 10.2 Identifying Outlying Y Observations - Studentized Deleted Residuals (Page 390)

An **outlying observation** (or outlier) is one that deviates substantially from the general pattern of the data. Outliers in $Y$ (response outliers) have unusual response values given their predictor values.

### Outlying Cases (Page 390)
An observation is considered an outlier if its $Y$ value is very far from what the regression model predicts for its $X$ values.

### Residuals and Semistudentized Residuals (Page 392)
* **Residuals ($e_i = Y_i - \hat{Y}_i$):** Simple differences between observed and fitted values. Their magnitude depends on the units of $Y$, making comparison difficult. Their variance is not constant, particularly for high-leverage observations.
* **Semistudentized Residuals ($e_i / \sqrt{MSE}$):** Standardized by the error standard deviation ($s = \sqrt{MSE}$). They have a mean of 0 and approximate variance of 1. However, their variance is still not exactly constant because $\sqrt{MSE}$ does not account for the varying precision of $\hat{Y}_i$ at different $X$ values.

### Hat Matrix (Page 392)
The **Hat Matrix ($\mathbf{H}$)** is defined as $\mathbf{H} = \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T$.
* It is a projection matrix that maps the observed response vector $\mathbf{Y}$ to the vector of fitted values: $\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}$.
* The diagonal elements of the hat matrix, $h_{ii}$, are called **leverage values**. They measure the "leverage" or "potential influence" of the $i$-th observation's predictor values ($\mathbf{X}_i$) on its own fitted value $\hat{Y}_i$. A high $h_{ii}$ indicates that the $i$-th observation has unusual predictor values, placing it far from the center of the predictor space.

### Studentized Residuals (Page 394)
* $e_i^* = \frac{e_i}{\sqrt{MSE(1-h_{ii})}}$
* These are residuals standardized by their *true* (estimated) standard deviation. The denominator $\sqrt{MSE(1-h_{ii})}$ explicitly accounts for the fact that the variance of residuals is smaller for observations with higher leverage.
* Studentized residuals have constant variance for all observations and follow a $t$-distribution approximately (when $n-p$ is large). They are better for outlier detection than raw or semistudentized residuals.

### Deleted Residuals (Page 395)
* $d_i = Y_i - \hat{Y}_{i(i)}$
* The deleted residual for the $i$-th observation is the difference between its observed value $Y_i$ and its predicted value $\hat{Y}_{i(i)}$ when the regression model is fitted using **all observations *except* the $i$-th observation**.
* This directly measures how well the model predicts the $i$-th observation when that observation itself has no influence on the model's parameters.

### Studentized Deleted Residuals (External Studentized Residuals or Jackknife Residuals) (Page 396)
* $t_i = \frac{d_i}{\sqrt{MSE_{(i)}(1-h_{ii})}}$
* This is the most effective statistic for identifying outlying $Y$ observations. It standardizes the deleted residual using $MSE_{(i)}$, which is the mean square error calculated *without* the $i$-th observation. This means $MSE_{(i)}$ is not influenced by the $i$-th observation itself.
* The $t_i$ statistic follows a $t$-distribution with $n-p-1$ degrees of freedom when the error terms are normally distributed.
* **Identification:** Large absolute values of $t_i$ (e.g., $|t_i| > 2$ or 3, or comparing to critical $t$-values) indicate potential outliers in the $Y$ direction.

## 10.3 Identifying Outlying X Observations - Hat Matrix Leverage Values (Page 398)

This section focuses on identifying outliers in the predictor variable space, which are also known as **high-leverage points**.

### Use of Hat Matrix for Identifying Outlying X Observations (Page 398)
* The diagonal elements $h_{ii}$ of the hat matrix are the key. A high $h_{ii}$ value means that the $i$-th observation's predictor values ($\mathbf{X}_i$) are far from the center of the observed predictor values. Such points have the potential to exert a strong influence on the regression line, "pulling" it towards them.
* **Guidelines:** A common rule of thumb for identifying high-leverage points is if $h_{ii} > 2p/n$ or $h_{ii} > 3p/n$, where $p$ is the number of parameters in the model and $n$ is the number of observations. However, even if an observation has high leverage, it may not be influential if its $Y$ value is consistent with the trend.

### Use of Hat Matrix to Identify Hidden Extrapolation (Page 400)
High leverage values are also a strong indicator of **hidden extrapolation** (discussed in Chapter 6). Even if an observation's individual $X$ values are within the range of the training data, its *combination* of $X$ values (its position in the multidimensional predictor space) might be far from the cluster of other observations. This makes predictions for such points unreliable.

## 10.4 Identifying Influential Cases - DFFITS, Cook's Distance, and DFBETAS Measures (Page 400)

**Influential cases** are observations that, if removed from the dataset, would significantly change the estimated regression coefficients, fitted values, or other aspects of the model. Influential points often combine high leverage with a large residual.

### Influence on Single Fitted Value - DFFITS (Page 401)
* $DFFITS_i = \frac{\hat{Y}_i - \hat{Y}_{i(i)}}{\sqrt{MSE_{(i)} h_{ii}}}$
* Measures the standardized change in the $i$-th fitted value ($\hat{Y}_i$) that results from deleting the $i$-th observation from the dataset.
* **Interpretation:** Large absolute values of $DFFITS_i$ suggest that the $i$-th observation has a substantial impact on its own fitted value.
* **Guidelines:** $|DFFITS_i| > 1$ for small to medium datasets; for large datasets, $|DFFITS_i| > 2\sqrt{p/n}$ is often used.

### Influence on All Fitted Values - Cook's Distance (Page 402)
* $D_i = \frac{\sum_{j=1}^n (\hat{Y}_j - \hat{Y}_{j(i)})^2}{p \cdot MSE}$
* Cook's Distance measures the overall influence of the $i$-th observation on *all* fitted values simultaneously. It essentially combines information about the observation's residual size and its leverage.
* **Interpretation:** A large Cook's Distance indicates that removing the $i$-th observation would lead to a substantial change in the fitted values for *all* observations.
* **Guidelines:** A common rule of thumb is $D_i > 1$ as a threshold for concern. Some suggest comparing $D_i$ to $F(0.5, p, n-p)$ (the 50th percentile of an F-distribution).

### Influence on the Regression Coefficients - DFBETAS (Page 404)
* $DFBETAS_{k(i)} = \frac{b_k - b_{k(i)}}{\sqrt{MSE_{(i)} (\mathbf{X}^T \mathbf{X})^{-1}_{kk}}}$
* DFBETAS measures the standardized change in the $k$-th regression coefficient ($b_k$) that results from deleting the $i$-th observation. There will be one DFBETAS value for *each* coefficient for each observation.
* **Interpretation:** Large absolute values of $DFBETAS_{k(i)}$ indicate that the $i$-th observation has a strong influence on the estimate of the $k$-th regression coefficient.
* **Guidelines:** $|DFBETAS_{k(i)}| > 1$ for small to medium datasets; for large datasets, $|DFBETAS_{k(i)}| > 2/\sqrt{n}$ is often used.

### Influence on Inferences (Page 405)
Influential observations can significantly distort the regression results, leading to:
* Biased (or unrepresentative) coefficient estimates.
* Inflated or deflated standard errors.
* Incorrect p-values and confidence intervals.
* Misleading conclusions about the relationships between variables.

### Some Final Comments (Page 406)
Identifying outliers and influential points is the first step. The next crucial step is to investigate *why* these observations are unusual. Possible reasons include:
* **Data Errors:** Incorrectly recorded data.
* **True Extreme Values:** The observation is genuinely unusual but valid.
* **Missing Variables:** The model is misspecified, and an important variable that explains the extreme observation is missing.
* **Structural Breaks:** The observation belongs to a different population or regime.

Based on the investigation, one might: correct the data, remove the observation (with justification), transform variables, or use more robust regression methods.

## 10.5 Multicollinearity Diagnostics - Variance Inflation Factor (Page 406)

**Multicollinearity** refers to the presence of high correlations among predictor variables in a multiple regression model.

### Informal Diagnostics (Page 407)
* **Pairwise Correlation Matrix:** Examining the correlation coefficients between all pairs of predictor variables. High correlations (e.g., $|r| > 0.8$) suggest potential multicollinearity.
* **Issue:** Pairwise correlations only detect collinearity between two variables; they cannot detect multicollinearity involving three or more variables simultaneously.

### Variance Inflation Factor (VIF) (Page 408)
* $VIF_k = \frac{1}{1 - R_k^2}$
    Where $R_k^2$ is the coefficient of determination obtained when the $k$-th predictor variable ($X_k$) is regressed on all *other* predictor variables in the model.
* **Interpretation:** $VIF_k$ quantifies how much the variance of the estimated regression coefficient $b_k$ is inflated due to its linear relationship with the other predictor variables in the model.
    * If $VIF_k = 1$, there is no multicollinearity for $X_k$ (it's orthogonal to other predictors).
    * If $VIF_k > 1$, there is some multicollinearity.
* **Guidelines:**
    * A VIF value of **5 or greater** is commonly considered a cause for concern.
    * A VIF value of **10 or greater** is often seen as indicating serious multicollinearity, requiring action.
* **Tolerance:** The reciprocal of VIF, $Tolerance_k = 1/VIF_k = 1 - R_k^2$. Tolerance values close to 0 indicate high multicollinearity.

High VIF values imply imprecise (large standard errors) and unstable coefficient estimates, making it difficult to interpret the unique contribution of individual predictor variables. Addressing multicollinearity often involves removing one of the correlated predictors, combining them, or using specialized techniques like principal components regression.

Chapter 10, "Diagnostics for Model Adequacy," is a critical follow-up to the model-building process. While Chapter 9 focuses on selecting the "best" subset of predictors, Chapter 10 provides advanced tools to scrutinize the chosen model for problems related to individual observations (outliers, high-leverage points, influential points) and the interrelationships among predictor variables (multicollinearity). These diagnostics help ensure the reliability and validity of the regression results.

---

# Chapter 10: Diagnostics for Model Adequacy

## 10.1 Model Adequacy for a Predictor Variable - Added-Variable Plots (Page 384)

**Added-variable plots**, also known as **partial regression plots** or **partial residual plots**, are graphical tools used to assess the nature of the relationship between the dependent variable ($Y$) and a specific predictor variable ($X_k$), **after accounting for the effects of all other predictor variables already in the model**.

* **How it's constructed:** For a given predictor $X_k$:
    1.  Regress $Y$ on *all other predictor variables* (excluding $X_k$) and obtain the residuals, denoted as $e(Y|X_{\text{others}})$. These residuals represent the variation in $Y$ not explained by the other predictors.
    2.  Regress $X_k$ on *all other predictor variables* (excluding $X_k$) and obtain the residuals, denoted as $e(X_k|X_{\text{others}})$. These residuals represent the variation in $X_k$ not explained by the other predictors.
    3.  Plot $e(Y|X_{\text{others}})$ on the vertical axis against $e(X_k|X_{\text{others}})$ on the horizontal axis.

* **Interpretation and Uses:**
    * **Detecting Linear Relationship:** The slope of the points in the added-variable plot for $X_k$ will be approximately equal to the estimated regression coefficient $b_k$ in the full multiple regression model. If a clear linear pattern is visible, it suggests $X_k$ has a significant partial relationship with $Y$.
    * **Assessing Curvilinearity:** If the plot shows a curvilinear pattern, it suggests that $X_k$ might require a polynomial term or a transformation to capture its true relationship with $Y$.
    * **Identifying Influential Observations:** Outliers or high-leverage points that are particularly influential for the coefficient of $X_k$ will often stand out distinctly in this plot.
    * **Detecting Multicollinearity:** If $X_k$ is highly correlated with other predictors, $e(X_k|X_{\text{others}})$ will have very little variation (the points will be highly compressed horizontally), making it difficult to see its unique relationship with $Y$.

## 10.2 Identifying Outlying Y Observations - Studentized Deleted Residuals (Page 390)

An **outlying observation** (or outlier) is one that deviates substantially from the general pattern of the data. Outliers in $Y$ (response outliers) have unusual response values given their predictor values.

### Outlying Cases (Page 390)
An observation is considered an outlier if its $Y$ value is very far from what the regression model predicts for its $X$ values.

### Residuals and Semistudentized Residuals (Page 392)
* **Residuals ($e_i = Y_i - \hat{Y}_i$):** Simple differences between observed and fitted values. Their magnitude depends on the units of $Y$, making comparison difficult. Their variance is not constant, particularly for high-leverage observations.
* **Semistudentized Residuals ($e_i / \sqrt{MSE}$):** Standardized by the error standard deviation ($s = \sqrt{MSE}$). They have a mean of 0 and approximate variance of 1. However, their variance is still not exactly constant because $\sqrt{MSE}$ does not account for the varying precision of $\hat{Y}_i$ at different $X$ values.

### Hat Matrix (Page 392)
The **Hat Matrix ($\mathbf{H}$)** is defined as $\mathbf{H} = \mathbf{X}(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T$.
* It is a projection matrix that maps the observed response vector $\mathbf{Y}$ to the vector of fitted values: $\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}$.
* The diagonal elements of the hat matrix, $h_{ii}$, are called **leverage values**. They measure the "leverage" or "potential influence" of the $i$-th observation's predictor values ($\mathbf{X}_i$) on its own fitted value $\hat{Y}_i$. A high $h_{ii}$ indicates that the $i$-th observation has unusual predictor values, placing it far from the center of the predictor space.

### Studentized Residuals (Page 394)
* $e_i^* = \frac{e_i}{\sqrt{MSE(1-h_{ii})}}$
* These are residuals standardized by their *true* (estimated) standard deviation. The denominator $\sqrt{MSE(1-h_{ii})}$ explicitly accounts for the fact that the variance of residuals is smaller for observations with higher leverage.
* Studentized residuals have constant variance for all observations and follow a $t$-distribution approximately (when $n-p$ is large). They are better for outlier detection than raw or semistudentized residuals.

### Deleted Residuals (Page 395)
* $d_i = Y_i - \hat{Y}_{i(i)}$
* The deleted residual for the $i$-th observation is the difference between its observed value $Y_i$ and its predicted value $\hat{Y}_{i(i)}$ when the regression model is fitted using **all observations *except* the $i$-th observation**.
* This directly measures how well the model predicts the $i$-th observation when that observation itself has no influence on the model's parameters.

### Studentized Deleted Residuals (External Studentized Residuals or Jackknife Residuals) (Page 396)
* $t_i = \frac{d_i}{\sqrt{MSE_{(i)}(1-h_{ii})}}$
* This is the most effective statistic for identifying outlying $Y$ observations. It standardizes the deleted residual using $MSE_{(i)}$, which is the mean square error calculated *without* the $i$-th observation. This means $MSE_{(i)}$ is not influenced by the $i$-th observation itself.
* The $t_i$ statistic follows a $t$-distribution with $n-p-1$ degrees of freedom when the error terms are normally distributed.
* **Identification:** Large absolute values of $t_i$ (e.g., $|t_i| > 2$ or 3, or comparing to critical $t$-values) indicate potential outliers in the $Y$ direction.

## 10.3 Identifying Outlying X Observations - Hat Matrix Leverage Values (Page 398)

This section focuses on identifying outliers in the predictor variable space, which are also known as **high-leverage points**.

### Use of Hat Matrix for Identifying Outlying X Observations (Page 398)
* The diagonal elements $h_{ii}$ of the hat matrix are the key. A high $h_{ii}$ value means that the $i$-th observation's predictor values ($\mathbf{X}_i$) are far from the center of the observed predictor values. Such points have the potential to exert a strong influence on the regression line, "pulling" it towards them.
* **Guidelines:** A common rule of thumb for identifying high-leverage points is if $h_{ii} > 2p/n$ or $h_{ii} > 3p/n$, where $p$ is the number of parameters in the model and $n$ is the number of observations. However, even if an observation has high leverage, it may not be influential if its $Y$ value is consistent with the trend.

### Use of Hat Matrix to Identify Hidden Extrapolation (Page 400)
High leverage values are also a strong indicator of **hidden extrapolation** (discussed in Chapter 6). Even if an observation's individual $X$ values are within the range of the training data, its *combination* of $X$ values (its position in the multidimensional predictor space) might be far from the cluster of other observations. This makes predictions for such points unreliable.

## 10.4 Identifying Influential Cases - DFFITS, Cook's Distance, and DFBETAS Measures (Page 400)

**Influential cases** are observations that, if removed from the dataset, would significantly change the estimated regression coefficients, fitted values, or other aspects of the model. Influential points often combine high leverage with a large residual.

### Influence on Single Fitted Value - DFFITS (Page 401)
* $DFFITS_i = \frac{\hat{Y}_i - \hat{Y}_{i(i)}}{\sqrt{MSE_{(i)} h_{ii}}}$
* Measures the standardized change in the $i$-th fitted value ($\hat{Y}_i$) that results from deleting the $i$-th observation from the dataset.
* **Interpretation:** Large absolute values of $DFFITS_i$ suggest that the $i$-th observation has a substantial impact on its own fitted value.
* **Guidelines:** $|DFFITS_i| > 1$ for small to medium datasets; for large datasets, $|DFFITS_i| > 2\sqrt{p/n}$ is often used.

### Influence on All Fitted Values - Cook's Distance (Page 402)
* $D_i = \frac{\sum_{j=1}^n (\hat{Y}_j - \hat{Y}_{j(i)})^2}{p \cdot MSE}$
* Cook's Distance measures the overall influence of the $i$-th observation on *all* fitted values simultaneously. It essentially combines information about the observation's residual size and its leverage.
* **Interpretation:** A large Cook's Distance indicates that removing the $i$-th observation would lead to a substantial change in the fitted values for *all* observations.
* **Guidelines:** A common rule of thumb is $D_i > 1$ as a threshold for concern. Some suggest comparing $D_i$ to $F(0.5, p, n-p)$ (the 50th percentile of an F-distribution).

### Influence on the Regression Coefficients - DFBETAS (Page 404)
* $DFBETAS_{k(i)} = \frac{b_k - b_{k(i)}}{\sqrt{MSE_{(i)} (\mathbf{X}^T \mathbf{X})^{-1}_{kk}}}$
* DFBETAS measures the standardized change in the $k$-th regression coefficient ($b_k$) that results from deleting the $i$-th observation. There will be one DFBETAS value for *each* coefficient for each observation.
* **Interpretation:** Large absolute values of $DFBETAS_{k(i)}$ indicate that the $i$-th observation has a strong influence on the estimate of the $k$-th regression coefficient.
* **Guidelines:** $|DFBETAS_{k(i)}| > 1$ for small to medium datasets; for large datasets, $|DFBETAS_{k(i)}| > 2/\sqrt{n}$ is often used.

### Influence on Inferences (Page 405)
Influential observations can significantly distort the regression results, leading to:
* Biased (or unrepresentative) coefficient estimates.
* Inflated or deflated standard errors.
* Incorrect p-values and confidence intervals.
* Misleading conclusions about the relationships between variables.

### Some Final Comments (Page 406)
Identifying outliers and influential points is the first step. The next crucial step is to investigate *why* these observations are unusual. Possible reasons include:
* **Data Errors:** Incorrectly recorded data.
* **True Extreme Values:** The observation is genuinely unusual but valid.
* **Missing Variables:** The model is misspecified, and an important variable that explains the extreme observation is missing.
* **Structural Breaks:** The observation belongs to a different population or regime.

Based on the investigation, one might: correct the data, remove the observation (with justification), transform variables, or use more robust regression methods.

## 10.5 Multicollinearity Diagnostics - Variance Inflation Factor (Page 406)

**Multicollinearity** refers to the presence of high correlations among predictor variables in a multiple regression model.

### Informal Diagnostics (Page 407)
* **Pairwise Correlation Matrix:** Examining the correlation coefficients between all pairs of predictor variables. High correlations (e.g., $|r| > 0.8$) suggest potential multicollinearity.
* **Issue:** Pairwise correlations only detect collinearity between two variables; they cannot detect multicollinearity involving three or more variables simultaneously.

### Variance Inflation Factor (VIF) (Page 408)
* $VIF_k = \frac{1}{1 - R_k^2}$
    Where $R_k^2$ is the coefficient of determination obtained when the $k$-th predictor variable ($X_k$) is regressed on all *other* predictor variables in the model.
* **Interpretation:** $VIF_k$ quantifies how much the variance of the estimated regression coefficient $b_k$ is inflated due to its linear relationship with the other predictor variables in the model.
    * If $VIF_k = 1$, there is no multicollinearity for $X_k$ (it's orthogonal to other predictors).
    * If $VIF_k > 1$, there is some multicollinearity.
* **Guidelines:**
    * A VIF value of **5 or greater** is commonly considered a cause for concern.
    * A VIF value of **10 or greater** is often seen as indicating serious multicollinearity, requiring action.
* **Tolerance:** The reciprocal of VIF, $Tolerance_k = 1/VIF_k = 1 - R_k^2$. Tolerance values close to 0 indicate high multicollinearity.

High VIF values imply imprecise (large standard errors) and unstable coefficient estimates, making it difficult to interpret the unique contribution of individual predictor variables. Addressing multicollinearity often involves removing one of the correlated predictors, combining them, or using specialized techniques like principal components regression.
