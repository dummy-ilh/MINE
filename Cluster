from transformers import DistilBertTokenizer, DistilBertModel
from sklearn.cluster import KMeans
import numpy as np

# Load pre-trained DistilBERT model and tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained('distilbert-base-uncased')

# Encode your text data using DistilBERT
encoded_texts = []
for text in text_data:
    input_ids = tokenizer.encode(text, add_special_tokens=True)
    with torch.no_grad():
        embeddings = model(torch.tensor([input_ids]))[0].numpy()
    encoded_texts.append(embeddings)

encoded_texts = np.array(encoded_texts)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=2, random_state=42)
clusters = kmeans.fit_predict(encoded_texts)

# Now you can assign labels to your test set based on distance
test_encoded_texts = [...]  # Encode your test set using the same method
test_distances = kmeans.transform(test_encoded_texts)
test_labels = np.argmin(test_distances, axis=1)
